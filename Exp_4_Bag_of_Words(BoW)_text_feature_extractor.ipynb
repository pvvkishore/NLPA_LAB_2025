{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63b41c5-50bf-44d4-9997-78783c46843b",
   "metadata": {},
   "source": [
    "# Code to extract text features using the Bag of Words (BoW) model from scratch using a carefully designed tiny dataset. Also compare the BoW with One Hot Encoding text featues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d6f4a4-7655-40ca-b11d-f88c13fe3cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love cats cats are cute',\n",
       " 'dogs are good good pets',\n",
       " 'I love dogs and cats',\n",
       " 'pets are cute and good good']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    \"I love cats cats are cute\",           # Doc 0: 6 words, \"cats\" appears twice\n",
    "    \"dogs are good good pets\",             # Doc 1: 5 words, \"good\" appears twice  \n",
    "    \"I love dogs and cats\",                # Doc 2: 5 words, mix of animals\n",
    "    \"pets are cute and good good\"          # Doc 3: 6 words, \"good\" appears twice\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a061c0-4e0c-4985-9f8e-4a766a5bd73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: 'I love cats cats are cute' (6 words)\n",
      "Document 1: 'dogs are good good pets' (5 words)\n",
      "Document 2: 'I love dogs and cats' (5 words)\n",
      "Document 3: 'pets are cute and good good' (6 words)\n",
      "\n",
      "Total documents: 4\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    word_count = len(doc.split())\n",
    "    print(f\"Document {i}: '{doc}' ({word_count} words)\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee836fc8-266d-4d0d-a70a-895efc298c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: TOKENIZATION\n",
      "======================================================================\n",
      "Document 0: ['i', 'love', 'cats', 'cats', 'are', 'cute']\n",
      "Document 1: ['dogs', 'are', 'good', 'good', 'pets']\n",
      "Document 2: ['i', 'love', 'dogs', 'and', 'cats']\n",
      "Document 3: ['pets', 'are', 'cute', 'and', 'good', 'good']\n",
      "\n",
      "Tokenized documents: [['i', 'love', 'cats', 'cats', 'are', 'cute'], ['dogs', 'are', 'good', 'good', 'pets'], ['i', 'love', 'dogs', 'and', 'cats'], ['pets', 'are', 'cute', 'and', 'good', 'good']]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization (split each document into words)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: TOKENIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tokenized_docs = []\n",
    "for i, doc in enumerate(documents):\n",
    "    # Convert to lowercase and split by spaces\n",
    "    tokens = doc.lower().split()\n",
    "    tokenized_docs.append(tokens)\n",
    "    print(f\"Document {i}: {tokens}\")\n",
    "\n",
    "print(f\"\\nTokenized documents: {tokenized_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc477aad-3b4f-4815-a07f-d6df4859b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: BUILD VOCABULARY\n",
      "======================================================================\n",
      "Collecting all words from all documents...\n",
      "Document 0 words: ['i', 'love', 'cats', 'cats', 'are', 'cute']\n",
      "Document 1 words: ['dogs', 'are', 'good', 'good', 'pets']\n",
      "Document 2 words: ['i', 'love', 'dogs', 'and', 'cats']\n",
      "Document 3 words: ['pets', 'are', 'cute', 'and', 'good', 'good']\n",
      "\n",
      "All words combined: ['i', 'love', 'cats', 'cats', 'are', 'cute', 'dogs', 'are', 'good', 'good', 'pets', 'i', 'love', 'dogs', 'and', 'cats', 'pets', 'are', 'cute', 'and', 'good', 'good']\n",
      "Total words (including duplicates): 22\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build vocabulary (collect all unique words)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: BUILD VOCABULARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Collecting all words from all documents...\")\n",
    "all_words = []\n",
    "for i, tokens in enumerate(tokenized_docs):\n",
    "    print(f\"Document {i} words: {tokens}\")\n",
    "    all_words.extend(tokens)  # Add all words to master list\n",
    "\n",
    "print(f\"\\nAll words combined: {all_words}\")\n",
    "print(f\"Total words (including duplicates): {len(all_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f36ff2-5e7c-410f-bc80-d165c367dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique words (vocabulary): ['and', 'are', 'cats', 'cute', 'dogs', 'good', 'i', 'love', 'pets']\n",
      "Vocabulary size: 9\n",
      "\n",
      "Word to Index mapping:\n",
      "  'and' -> position 0\n",
      "  'are' -> position 1\n",
      "  'cats' -> position 2\n",
      "  'cute' -> position 3\n",
      "  'dogs' -> position 4\n",
      "  'good' -> position 5\n",
      "  'i' -> position 6\n",
      "  'love' -> position 7\n",
      "  'pets' -> position 8\n"
     ]
    }
   ],
   "source": [
    "# Get unique words and sort them (creates our vocabulary)\n",
    "vocabulary = sorted(list(set(all_words)))\n",
    "print(f\"\\nUnique words (vocabulary): {vocabulary}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# Create word-to-index mapping for easy lookup\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocabulary):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "print(f\"\\nWord to Index mapping:\")\n",
    "for word, index in word_to_index.items():\n",
    "    print(f\"  '{word}' -> position {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6741f188-2733-450d-8c23-c0dd6b1a7c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: CREATE FREQUENCY VECTORS\n",
      "======================================================================\n",
      "\n",
      "Processing Document 0: ['i', 'love', 'cats', 'cats', 'are', 'cute']\n",
      "Word frequencies: {'i': 1, 'love': 1, 'cats': 2, 'are': 1, 'cute': 1}\n",
      "\n",
      "Processing Document 1: ['dogs', 'are', 'good', 'good', 'pets']\n",
      "Word frequencies: {'dogs': 1, 'are': 1, 'good': 2, 'pets': 1}\n",
      "\n",
      "Processing Document 2: ['i', 'love', 'dogs', 'and', 'cats']\n",
      "Word frequencies: {'i': 1, 'love': 1, 'dogs': 1, 'and': 1, 'cats': 1}\n",
      "\n",
      "Processing Document 3: ['pets', 'are', 'cute', 'and', 'good', 'good']\n",
      "Word frequencies: {'pets': 1, 'are': 1, 'cute': 1, 'and': 1, 'good': 2}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create frequency vectors for each document\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: CREATE FREQUENCY VECTORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bow_vectors = []\n",
    "for doc_id, tokens in enumerate(tokenized_docs):\n",
    "    print(f\"\\nProcessing Document {doc_id}: {tokens}\")\n",
    "    \n",
    "    # Count frequency of each word in this document\n",
    "    word_freq = {}\n",
    "    for word in tokens:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    \n",
    "    print(f\"Word frequencies: {word_freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7fe9a8-f01a-43f1-982e-6ced926f0d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Document 0: ['i', 'love', 'cats', 'cats', 'are', 'cute']\n",
      "Word frequencies: {'i': 1, 'love': 1, 'cats': 2, 'are': 1, 'cute': 1}\n",
      "BoW vector: [0, 1, 2, 1, 0, 0, 1, 1, 0]\n",
      "\n",
      "Processing Document 1: ['dogs', 'are', 'good', 'good', 'pets']\n",
      "Word frequencies: {'dogs': 1, 'are': 1, 'good': 2, 'pets': 1}\n",
      "BoW vector: [0, 1, 0, 0, 1, 2, 0, 0, 1]\n",
      "\n",
      "Processing Document 2: ['i', 'love', 'dogs', 'and', 'cats']\n",
      "Word frequencies: {'i': 1, 'love': 1, 'dogs': 1, 'and': 1, 'cats': 1}\n",
      "BoW vector: [1, 0, 1, 0, 1, 0, 1, 1, 0]\n",
      "\n",
      "Processing Document 3: ['pets', 'are', 'cute', 'and', 'good', 'good']\n",
      "Word frequencies: {'pets': 1, 'are': 1, 'cute': 1, 'and': 1, 'good': 2}\n",
      "BoW vector: [1, 1, 0, 1, 0, 2, 0, 0, 1]\n",
      "\n",
      "All BoW vectors: [[0, 1, 2, 1, 0, 0, 1, 1, 0], [0, 1, 0, 0, 1, 2, 0, 0, 1], [1, 0, 1, 0, 1, 0, 1, 1, 0], [1, 1, 0, 1, 0, 2, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "bow_vectors = []\n",
    "for doc_id, tokens in enumerate(tokenized_docs):\n",
    "    print(f\"\\nProcessing Document {doc_id}: {tokens}\")\n",
    "    \n",
    "    # Count frequency of each word in this document\n",
    "    word_freq = {}\n",
    "    for word in tokens:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    \n",
    "    print(f\"Word frequencies: {word_freq}\")\n",
    "    \n",
    "    # Create vector based on vocabulary order\n",
    "    vector = []\n",
    "    for vocab_word in vocabulary:\n",
    "        if vocab_word in word_freq:\n",
    "            count = word_freq[vocab_word]\n",
    "        else:\n",
    "            count = 0\n",
    "        vector.append(count)\n",
    "    \n",
    "    bow_vectors.append(vector)\n",
    "    print(f\"BoW vector: {vector}\")\n",
    "\n",
    "print(f\"\\nAll BoW vectors: {bow_vectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a93edc-574a-471a-b313-bf227b6b98c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: FINAL BAG OF WORDS MATRIX\n",
      "======================================================================\n",
      "\n",
      "VOCABULARY INDEX REFERENCE:\n",
      "Position 0: 'and'\n",
      "Position 1: 'are'\n",
      "Position 2: 'cats'\n",
      "Position 3: 'cute'\n",
      "Position 4: 'dogs'\n",
      "Position 5: 'good'\n",
      "Position 6: 'i'\n",
      "Position 7: 'love'\n",
      "Position 8: 'pets'\n",
      "\n",
      "BAG OF WORDS MATRIX:\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create the BoW matrix and display nicely\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: FINAL BAG OF WORDS MATRIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nVOCABULARY INDEX REFERENCE:\")\n",
    "for i, word in enumerate(vocabulary):\n",
    "    print(f\"Position {i}: '{word}'\")\n",
    "\n",
    "print(f\"\\nBAG OF WORDS MATRIX:\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1245fef2-806e-4c79-aceb-7f7a76bcfea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc#     and   are  cats  cute  dogs  good     i  love  pets\n",
      "------------------------------------------------------------\n",
      "Doc0       0     1     2     1     0     0     1     1     0\n",
      "Doc1       0     1     0     0     1     2     0     0     1\n",
      "Doc2       1     0     1     0     1     0     1     1     0\n",
      "Doc3       1     1     0     1     0     2     0     0     1\n"
     ]
    }
   ],
   "source": [
    "# Header row with vocabulary\n",
    "header = \"Doc#  \"\n",
    "for word in vocabulary:\n",
    "    header += f\"{word:>6}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Data rows\n",
    "for doc_id, vector in enumerate(bow_vectors):\n",
    "    row = f\"Doc{doc_id}  \"\n",
    "    for count in vector:\n",
    "        row += f\"{count:>6}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e9fbce2-e1b2-41db-a53f-131d1082f877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: REPETITIVE WORDS ANALYSIS\n",
      "======================================================================\n",
      "HOW BOW HANDLES REPETITIVE WORDS:\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Detailed analysis showing how repetitive words are handled\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: REPETITIVE WORDS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"HOW BOW HANDLES REPETITIVE WORDS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "repetitive_analysis = [\n",
    "    (\"cats\", \"Document 0 has 'cats' twice -> count = 2\"),\n",
    "    (\"good\", \"Documents 1&3 have 'good' twice each -> count = 2\"),\n",
    "    (\"are\", \"Appears once in docs 1,2,3 -> count = 1 each\"),\n",
    "    (\"love\", \"Appears once in docs 0,2 -> count = 1 each\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70f2bbc-0e29-4646-99a1-587242dd846e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: 'cats' (position 2)\n",
      "  Document 0 has 'cats' twice -> count = 2\n",
      "  Vector values across documents: Doc0=2 Doc1=0 Doc2=1 Doc3=0 \n",
      "\n",
      "Word: 'good' (position 5)\n",
      "  Documents 1&3 have 'good' twice each -> count = 2\n",
      "  Vector values across documents: Doc0=0 Doc1=2 Doc2=0 Doc3=2 \n",
      "\n",
      "Word: 'are' (position 1)\n",
      "  Appears once in docs 1,2,3 -> count = 1 each\n",
      "  Vector values across documents: Doc0=1 Doc1=1 Doc2=0 Doc3=1 \n",
      "\n",
      "Word: 'love' (position 7)\n",
      "  Appears once in docs 0,2 -> count = 1 each\n",
      "  Vector values across documents: Doc0=1 Doc1=0 Doc2=1 Doc3=0 \n"
     ]
    }
   ],
   "source": [
    "for word, explanation in repetitive_analysis:\n",
    "    word_idx = word_to_index[word]\n",
    "    print(f\"\\nWord: '{word}' (position {word_idx})\")\n",
    "    print(f\"  {explanation}\")\n",
    "    print(\"  Vector values across documents:\", end=\" \")\n",
    "    for doc_id, vector in enumerate(bow_vectors):\n",
    "        print(f\"Doc{doc_id}={vector[word_idx]}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd738af8-bb04-4cb4-9fc8-59a545569d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aeae664-a968-4666-a6f8-5f08a7eee772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SIMPLE NEURAL NETWORK FOR NEXT WORD PREDICTION USING BOW\n",
      "================================================================================\n",
      "DATASET:\n",
      "--------------------\n",
      "Document 0: 'I love cats cats are cute'\n",
      "Document 1: 'dogs are good good pets'\n",
      "Document 2: 'I love dogs and cats'\n",
      "Document 3: 'pets are cute and good good'\n",
      "\n",
      "================================================================================\n",
      "STEP 1: DATA PREPARATION FOR NEURAL NETWORK\n",
      "================================================================================\n",
      "Vocabulary: ['and', 'are', 'cats', 'cute', 'dogs', 'good', 'i', 'love', 'pets']\n",
      "Vocabulary size: 9\n",
      "Word to index mapping: {'and': 0, 'are': 1, 'cats': 2, 'cute': 3, 'dogs': 4, 'good': 5, 'i': 6, 'love': 7, 'pets': 8}\n",
      "\n",
      "CREATING TRAINING SEQUENCES:\n",
      "------------------------------\n",
      "\n",
      "Document 0: ['i', 'love', 'cats', 'cats', 'are', 'cute']\n",
      "  'i' -> 'love'\n",
      "  'love' -> 'cats'\n",
      "  'cats' -> 'cats'\n",
      "  'cats' -> 'are'\n",
      "  'are' -> 'cute'\n",
      "\n",
      "Document 1: ['dogs', 'are', 'good', 'good', 'pets']\n",
      "  'dogs' -> 'are'\n",
      "  'are' -> 'good'\n",
      "  'good' -> 'good'\n",
      "  'good' -> 'pets'\n",
      "\n",
      "Document 2: ['i', 'love', 'dogs', 'and', 'cats']\n",
      "  'i' -> 'love'\n",
      "  'love' -> 'dogs'\n",
      "  'dogs' -> 'and'\n",
      "  'and' -> 'cats'\n",
      "\n",
      "Document 3: ['pets', 'are', 'cute', 'and', 'good', 'good']\n",
      "  'pets' -> 'are'\n",
      "  'are' -> 'cute'\n",
      "  'cute' -> 'and'\n",
      "  'and' -> 'good'\n",
      "  'good' -> 'good'\n",
      "\n",
      "Total training sequences: 18\n",
      "All sequences: [('i', 'love'), ('love', 'cats'), ('cats', 'cats'), ('cats', 'are'), ('are', 'cute'), ('dogs', 'are'), ('are', 'good'), ('good', 'good'), ('good', 'pets'), ('i', 'love'), ('love', 'dogs'), ('dogs', 'and'), ('and', 'cats'), ('pets', 'are'), ('are', 'cute'), ('cute', 'and'), ('and', 'good'), ('good', 'good')]\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CONVERT TO BAG OF WORDS INPUT/OUTPUT\n",
      "================================================================================\n",
      "TRAINING DATA PREPARATION:\n",
      "------------------------------\n",
      "Sequence 1: 'i' -> 'love'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Output (target): [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Sequence 2: 'love' -> 'cats'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "  Output (target): [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 3: 'cats' -> 'cats'\n",
      "  Input (BoW):  [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "  Output (target): [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 4: 'cats' -> 'are'\n",
      "  Input (BoW):  [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "  Output (target): [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 5: 'are' -> 'cute'\n",
      "  Input (BoW):  [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Output (target): [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Sequence 6: 'dogs' -> 'are'\n",
      "  Input (BoW):  [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "  Output (target): [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 7: 'are' -> 'good'\n",
      "  Input (BoW):  [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Output (target): [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Sequence 8: 'good' -> 'good'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "  Output (target): [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Sequence 9: 'good' -> 'pets'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "  Output (target): [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Sequence 10: 'i' -> 'love'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Output (target): [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Sequence 11: 'love' -> 'dogs'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "  Output (target): [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "Sequence 12: 'dogs' -> 'and'\n",
      "  Input (BoW):  [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "  Output (target): [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 13: 'and' -> 'cats'\n",
      "  Input (BoW):  [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Output (target): [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 14: 'pets' -> 'are'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "  Output (target): [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 15: 'are' -> 'cute'\n",
      "  Input (BoW):  [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Output (target): [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Sequence 16: 'cute' -> 'and'\n",
      "  Input (BoW):  [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "  Output (target): [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 17: 'and' -> 'good'\n",
      "  Input (BoW):  [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Output (target): [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Sequence 18: 'good' -> 'good'\n",
      "  Input (BoW):  [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "  Output (target): [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "Training data shape:\n",
      "X_train: (18, 9) (sequences, vocabulary_size)\n",
      "y_train: (18, 9) (sequences, vocabulary_size)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: SIMPLE NEURAL NETWORK IMPLEMENTATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING THE NEURAL NETWORK\n",
      "================================================================================\n",
      "Neural Network Architecture:\n",
      "  Input layer: 9 neurons (vocabulary size)\n",
      "  Hidden layer: 5 neurons\n",
      "  Output layer: 9 neurons (vocabulary size)\n",
      "\n",
      "Training the model...\n",
      "Epoch 0, Loss: 2.1795\n",
      "Epoch 200, Loss: 1.7685\n",
      "Epoch 400, Loss: 0.9378\n",
      "Epoch 600, Loss: 0.6869\n",
      "Epoch 800, Loss: 0.6143\n",
      "\n",
      "Final loss: 0.5820\n",
      "\n",
      "================================================================================\n",
      "STEP 5: TESTING THE MODEL\n",
      "================================================================================\n",
      "NEXT WORD PREDICTIONS:\n",
      "------------------------------\n",
      "\n",
      "Input word: 'I'\n",
      "Predicted next word: 'love' (confidence: 0.963)\n",
      "Top 3 predictions:\n",
      "  1. 'love' (0.963)\n",
      "  2. 'and' (0.011)\n",
      "  3. 'cute' (0.007)\n",
      "\n",
      "Input word: 'love'\n",
      "Predicted next word: 'cats' (confidence: 0.500)\n",
      "Top 3 predictions:\n",
      "  1. 'cats' (0.500)\n",
      "  2. 'dogs' (0.440)\n",
      "  3. 'good' (0.026)\n",
      "\n",
      "Input word: 'cats'\n",
      "Predicted next word: 'are' (confidence: 0.501)\n",
      "Top 3 predictions:\n",
      "  1. 'are' (0.501)\n",
      "  2. 'cats' (0.462)\n",
      "  3. 'dogs' (0.013)\n",
      "\n",
      "Input word: 'are'\n",
      "Predicted next word: 'cute' (confidence: 0.638)\n",
      "Top 3 predictions:\n",
      "  1. 'cute' (0.638)\n",
      "  2. 'good' (0.316)\n",
      "  3. 'pets' (0.017)\n",
      "\n",
      "Input word: 'good'\n",
      "Predicted next word: 'good' (confidence: 0.702)\n",
      "Top 3 predictions:\n",
      "  1. 'good' (0.702)\n",
      "  2. 'pets' (0.269)\n",
      "  3. 'cats' (0.014)\n",
      "\n",
      "Input word: 'pets'\n",
      "Predicted next word: 'are' (confidence: 0.878)\n",
      "Top 3 predictions:\n",
      "  1. 'are' (0.878)\n",
      "  2. 'cats' (0.057)\n",
      "  3. 'and' (0.052)\n",
      "\n",
      "================================================================================\n",
      "STEP 6: SEMANTIC UNDERSTANDING FAILURES\n",
      "================================================================================\n",
      "WHY THE MODEL FAILS AT SEMANTIC UNDERSTANDING:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. CONTEXT INSENSITIVITY:\n",
      "-------------------------\n",
      "The model treats each word independently.\n",
      "It cannot understand that 'cats' and 'dogs' are both animals.\n",
      "It cannot understand that 'good' can describe both pets and qualities.\n",
      "\n",
      "Testing semantic relationships:\n",
      "After 'cats' -> 'are' (should understand it's an animal)\n",
      "After 'dogs' -> 'and' (should understand it's an animal)\n",
      "After 'pets' -> 'are' (should understand it's an animal)\n",
      "\n",
      "2. BAG OF WORDS LIMITATION:\n",
      "------------------------------\n",
      "BoW representation loses all word order and context:\n",
      "'cats are good' BoW: [0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "'good are cats' BoW: [0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "Identical representations: True\n",
      "\n",
      "3. NO UNDERSTANDING OF WORD RELATIONSHIPS:\n",
      "---------------------------------------------\n",
      "The model cannot understand:\n",
      "• 'cats' and 'pets' are related (cats are pets)\n",
      "• 'good' and 'cute' are both positive adjectives\n",
      "• 'I love cats' has different meaning than 'cats love I'\n",
      "\n",
      "Semantic understanding test:\n",
      "If the model understood semantics:\n",
      "• After 'I' should prefer 'love' (subject-verb pattern)\n",
      "• After 'cats' should prefer 'are' (noun-verb pattern)\n",
      "• After 'good' should prefer nouns like 'pets'\n",
      "\n",
      "What the model actually learned (frequency patterns):\n",
      "  After 'i': {'love': 2}\n",
      "  After 'love': {'cats': 1, 'dogs': 1}\n",
      "  After 'cats': {'cats': 1, 'are': 1}\n",
      "  After 'are': {'cute': 2, 'good': 1}\n",
      "  After 'dogs': {'are': 1, 'and': 1}\n",
      "  After 'good': {'good': 2, 'pets': 1}\n",
      "  After 'and': {'cats': 1, 'good': 1}\n",
      "  After 'pets': {'are': 1}\n",
      "  After 'cute': {'and': 1}\n",
      "\n",
      "4. STATISTICAL PATTERN VS SEMANTIC UNDERSTANDING:\n",
      "--------------------------------------------------\n",
      "The model learns statistical patterns from training data:\n",
      "• Which words appear after which words most frequently\n",
      "• But it doesn't understand WHY these patterns exist\n",
      "• It cannot generalize to new semantic relationships\n",
      "\n",
      "================================================================================\n",
      "STEP 7: WHAT SEMANTIC UNDERSTANDING WOULD LOOK LIKE\n",
      "================================================================================\n",
      "IDEAL SEMANTIC MODEL WOULD UNDERSTAND:\n",
      "---------------------------------------------\n",
      "✓ 'cats' and 'dogs' are both animals\n",
      "✓ 'good' and 'cute' are both positive descriptors\n",
      "✓ 'I love X' is a common pattern for expressing affection\n",
      "✓ Word order matters: 'I love cats' ≠ 'cats love I'\n",
      "✓ Context matters: 'good pets' vs 'pets good'\n",
      "\n",
      "OUR BOW MODEL LIMITATIONS:\n",
      "------------------------------\n",
      "❌ Cannot understand word relationships\n",
      "❌ Cannot understand grammar or syntax\n",
      "❌ Cannot understand context or meaning\n",
      "❌ Only learns surface-level statistical patterns\n",
      "❌ Treats all words as independent tokens\n",
      "\n",
      "WHY BOW FAILS FOR SEMANTIC UNDERSTANDING:\n",
      "---------------------------------------------\n",
      "1. No positional information (word order lost)\n",
      "2. No contextual information (surrounding words ignored)\n",
      "3. No semantic relationships (words treated independently)\n",
      "4. No understanding of grammar or syntax\n",
      "5. Only frequency-based patterns, not meaning-based\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "This demonstrates why modern NLP moved beyond BoW:\n",
      "• Word embeddings (Word2Vec, GloVe) capture semantic relationships\n",
      "• Contextual models (BERT, GPT) understand word order and context\n",
      "• Transformer architectures use attention to understand relationships\n",
      "• BoW is useful for simple tasks but fails at semantic understanding\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SIMPLE NEURAL NETWORK FOR NEXT WORD PREDICTION USING BOW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Same tiny dataset from previous example\n",
    "documents = [\n",
    "    \"I love cats cats are cute\",           # Doc 0: 6 words, \"cats\" appears twice\n",
    "    \"dogs are good good pets\",             # Doc 1: 5 words, \"good\" appears twice  \n",
    "    \"I love dogs and cats\",                # Doc 2: 5 words, mix of animals\n",
    "    \"pets are cute and good good\"          # Doc 3: 6 words, \"good\" appears twice\n",
    "]\n",
    "\n",
    "print(\"DATASET:\")\n",
    "print(\"-\" * 20)\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i}: '{doc}'\")\n",
    "\n",
    "# Step 1: Prepare data for neural network\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: DATA PREPARATION FOR NEURAL NETWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tokenize and create vocabulary\n",
    "all_tokens = []\n",
    "for doc in documents:\n",
    "    tokens = doc.lower().split()\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "vocabulary = sorted(list(set(all_tokens)))\n",
    "vocab_size = len(vocabulary)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Word to index mapping: {word_to_idx}\")\n",
    "\n",
    "# Create training sequences (context -> next word)\n",
    "print(f\"\\nCREATING TRAINING SEQUENCES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "sequences = []\n",
    "for doc_id, doc in enumerate(documents):\n",
    "    tokens = doc.lower().split()\n",
    "    print(f\"\\nDocument {doc_id}: {tokens}\")\n",
    "    \n",
    "    # Create sequences of length 2: [current_word] -> [next_word]\n",
    "    for i in range(len(tokens) - 1):\n",
    "        current_word = tokens[i]\n",
    "        next_word = tokens[i + 1]\n",
    "        sequences.append((current_word, next_word))\n",
    "        print(f\"  '{current_word}' -> '{next_word}'\")\n",
    "\n",
    "print(f\"\\nTotal training sequences: {len(sequences)}\")\n",
    "print(f\"All sequences: {sequences}\")\n",
    "\n",
    "# Step 2: Convert to BoW representation for neural network input\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: CONVERT TO BAG OF WORDS INPUT/OUTPUT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def word_to_bow_vector(word, vocabulary, word_to_idx):\n",
    "    \"\"\"Convert a single word to BoW vector (one-hot encoding)\"\"\"\n",
    "    vector = [0] * len(vocabulary)\n",
    "    if word in word_to_idx:\n",
    "        vector[word_to_idx[word]] = 1\n",
    "    return vector\n",
    "\n",
    "def word_to_output_vector(word, vocabulary, word_to_idx):\n",
    "    \"\"\"Convert target word to output vector (one-hot encoding)\"\"\"\n",
    "    vector = [0] * len(vocabulary)\n",
    "    if word in word_to_idx:\n",
    "        vector[word_to_idx[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Prepare training data\n",
    "X_train = []  # Input vectors (current word as BoW)\n",
    "y_train = []  # Output vectors (next word as one-hot)\n",
    "\n",
    "print(\"TRAINING DATA PREPARATION:\")\n",
    "print(\"-\" * 30)\n",
    "for i, (current_word, next_word) in enumerate(sequences):\n",
    "    input_vector = word_to_bow_vector(current_word, vocabulary, word_to_idx)\n",
    "    output_vector = word_to_output_vector(next_word, vocabulary, word_to_idx)\n",
    "    \n",
    "    X_train.append(input_vector)\n",
    "    y_train.append(output_vector)\n",
    "    \n",
    "    print(f\"Sequence {i+1}: '{current_word}' -> '{next_word}'\")\n",
    "    print(f\"  Input (BoW):  {input_vector}\")\n",
    "    print(f\"  Output (target): {output_vector}\")\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"\\nTraining data shape:\")\n",
    "print(f\"X_train: {X_train.shape} (sequences, vocabulary_size)\")\n",
    "print(f\"y_train: {y_train.shape} (sequences, vocabulary_size)\")\n",
    "\n",
    "# Step 3: Simple Neural Network Implementation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: SIMPLE NEURAL NETWORK IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class SimpleNextWordPredictor:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        print(f\"Neural Network Architecture:\")\n",
    "        print(f\"  Input layer: {input_size} neurons (vocabulary size)\")\n",
    "        print(f\"  Hidden layer: {hidden_size} neurons\")\n",
    "        print(f\"  Output layer: {output_size} neurons (vocabulary size)\")\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Backward pass\n",
    "        dz2 = output - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.a1 * (1 - self.a1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.1):\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Calculate loss (cross-entropy)\n",
    "            loss = -np.mean(np.sum(y * np.log(output + 1e-15), axis=1))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
    "            \n",
    "            # Update weights\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            if epoch % 200 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, word, vocabulary, word_to_idx, idx_to_word):\n",
    "        input_vector = word_to_bow_vector(word, vocabulary, word_to_idx)\n",
    "        input_vector = np.array([input_vector])\n",
    "        \n",
    "        output = self.forward(input_vector)\n",
    "        predicted_idx = np.argmax(output[0])\n",
    "        predicted_word = idx_to_word[predicted_idx]\n",
    "        confidence = output[0][predicted_idx]\n",
    "        \n",
    "        return predicted_word, confidence, output[0]\n",
    "\n",
    "# Step 4: Train the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: TRAINING THE NEURAL NETWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create and train the model\n",
    "model = SimpleNextWordPredictor(input_size=vocab_size, hidden_size=5, output_size=vocab_size)\n",
    "\n",
    "print(\"\\nTraining the model...\")\n",
    "losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.5)\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# Step 5: Test the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: TESTING THE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"NEXT WORD PREDICTIONS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "test_words = ['I', 'love', 'cats', 'are', 'good', 'pets']\n",
    "for word in test_words:\n",
    "    if word.lower() in word_to_idx:\n",
    "        predicted_word, confidence, full_output = model.predict(\n",
    "            word.lower(), vocabulary, word_to_idx, idx_to_word\n",
    "        )\n",
    "        print(f\"\\nInput word: '{word}'\")\n",
    "        print(f\"Predicted next word: '{predicted_word}' (confidence: {confidence:.3f})\")\n",
    "        \n",
    "        # Show top 3 predictions\n",
    "        top_indices = np.argsort(full_output)[-3:][::-1]\n",
    "        print(\"Top 3 predictions:\")\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            word_pred = idx_to_word[idx]\n",
    "            prob = full_output[idx]\n",
    "            print(f\"  {i+1}. '{word_pred}' ({prob:.3f})\")\n",
    "\n",
    "# Step 6: Demonstrate semantic understanding failures\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: SEMANTIC UNDERSTANDING FAILURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"WHY THE MODEL FAILS AT SEMANTIC UNDERSTANDING:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n1. CONTEXT INSENSITIVITY:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"The model treats each word independently.\")\n",
    "print(\"It cannot understand that 'cats' and 'dogs' are both animals.\")\n",
    "print(\"It cannot understand that 'good' can describe both pets and qualities.\")\n",
    "\n",
    "# Test semantic relationships\n",
    "print(f\"\\nTesting semantic relationships:\")\n",
    "animal_words = ['cats', 'dogs', 'pets']\n",
    "for word in animal_words:\n",
    "    if word in word_to_idx:\n",
    "        pred_word, conf, _ = model.predict(word, vocabulary, word_to_idx, idx_to_word)\n",
    "        print(f\"After '{word}' -> '{pred_word}' (should understand it's an animal)\")\n",
    "\n",
    "print(\"\\n2. BAG OF WORDS LIMITATION:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"BoW representation loses all word order and context:\")\n",
    "\n",
    "# Show how BoW treats semantically different sentences the same\n",
    "sentence1 = \"cats are good\"\n",
    "sentence2 = \"good are cats\"  \n",
    "tokens1 = sentence1.split()\n",
    "tokens2 = sentence2.split()\n",
    "\n",
    "bow1 = [0] * vocab_size\n",
    "bow2 = [0] * vocab_size\n",
    "\n",
    "for token in tokens1:\n",
    "    if token in word_to_idx:\n",
    "        bow1[word_to_idx[token]] = 1\n",
    "\n",
    "for token in tokens2:\n",
    "    if token in word_to_idx:\n",
    "        bow2[word_to_idx[token]] = 1\n",
    "\n",
    "print(f\"'{sentence1}' BoW: {bow1}\")\n",
    "print(f\"'{sentence2}' BoW: {bow2}\")\n",
    "print(f\"Identical representations: {bow1 == bow2}\")\n",
    "\n",
    "print(\"\\n3. NO UNDERSTANDING OF WORD RELATIONSHIPS:\")\n",
    "print(\"-\" * 45)\n",
    "print(\"The model cannot understand:\")\n",
    "print(\"• 'cats' and 'pets' are related (cats are pets)\")\n",
    "print(\"• 'good' and 'cute' are both positive adjectives\")\n",
    "print(\"• 'I love cats' has different meaning than 'cats love I'\")\n",
    "\n",
    "# Test actual semantic understanding\n",
    "print(f\"\\nSemantic understanding test:\")\n",
    "print(\"If the model understood semantics:\")\n",
    "print(\"• After 'I' should prefer 'love' (subject-verb pattern)\")\n",
    "print(\"• After 'cats' should prefer 'are' (noun-verb pattern)\")\n",
    "print(\"• After 'good' should prefer nouns like 'pets'\")\n",
    "\n",
    "# Show what the model actually learned\n",
    "print(f\"\\nWhat the model actually learned (frequency patterns):\")\n",
    "word_next_count = {}\n",
    "for current, next_word in sequences:\n",
    "    if current not in word_next_count:\n",
    "        word_next_count[current] = {}\n",
    "    if next_word not in word_next_count[current]:\n",
    "        word_next_count[current][next_word] = 0\n",
    "    word_next_count[current][next_word] += 1\n",
    "\n",
    "for word, next_words in word_next_count.items():\n",
    "    print(f\"  After '{word}': {next_words}\")\n",
    "\n",
    "print(\"\\n4. STATISTICAL PATTERN VS SEMANTIC UNDERSTANDING:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"The model learns statistical patterns from training data:\")\n",
    "print(\"• Which words appear after which words most frequently\")\n",
    "print(\"• But it doesn't understand WHY these patterns exist\")\n",
    "print(\"• It cannot generalize to new semantic relationships\")\n",
    "\n",
    "# Step 7: Compare with ideal semantic understanding\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: WHAT SEMANTIC UNDERSTANDING WOULD LOOK LIKE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"IDEAL SEMANTIC MODEL WOULD UNDERSTAND:\")\n",
    "print(\"-\" * 45)\n",
    "print(\"✓ 'cats' and 'dogs' are both animals\")\n",
    "print(\"✓ 'good' and 'cute' are both positive descriptors\")\n",
    "print(\"✓ 'I love X' is a common pattern for expressing affection\")\n",
    "print(\"✓ Word order matters: 'I love cats' ≠ 'cats love I'\")\n",
    "print(\"✓ Context matters: 'good pets' vs 'pets good'\")\n",
    "\n",
    "print(f\"\\nOUR BOW MODEL LIMITATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"❌ Cannot understand word relationships\")\n",
    "print(\"❌ Cannot understand grammar or syntax\")\n",
    "print(\"❌ Cannot understand context or meaning\")\n",
    "print(\"❌ Only learns surface-level statistical patterns\")\n",
    "print(\"❌ Treats all words as independent tokens\")\n",
    "\n",
    "print(f\"\\nWHY BOW FAILS FOR SEMANTIC UNDERSTANDING:\")\n",
    "print(\"-\" * 45)\n",
    "print(\"1. No positional information (word order lost)\")\n",
    "print(\"2. No contextual information (surrounding words ignored)\")\n",
    "print(\"3. No semantic relationships (words treated independently)\")\n",
    "print(\"4. No understanding of grammar or syntax\")\n",
    "print(\"5. Only frequency-based patterns, not meaning-based\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "print(\"This demonstrates why modern NLP moved beyond BoW:\")\n",
    "print(\"• Word embeddings (Word2Vec, GloVe) capture semantic relationships\")\n",
    "print(\"• Contextual models (BERT, GPT) understand word order and context\")\n",
    "print(\"• Transformer architectures use attention to understand relationships\")\n",
    "print(\"• BoW is useful for simple tasks but fails at semantic understanding\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83302583-91db-4ca0-b109-cc360d11a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BAG OF WORDS vs ONE-HOT ENCODING COMPARISON\n",
      "================================================================================\n",
      "\n",
      "INPUT DOCUMENTS:\n",
      "------------------------------\n",
      "D1: 'This movie star had dinner with my friend.'\n",
      "D2: 'I watch this movie on star movies.'\n",
      "D3: 'In this movie the opening was focused on the stars in the sky.'\n",
      "\n",
      "================================================================================\n",
      "PART 1: BAG OF WORDS (BOW) IMPLEMENTATION\n",
      "================================================================================\n",
      "\n",
      "STEP 1: PREPROCESSING AND TOKENIZATION\n",
      "--------------------------------------------------\n",
      "D1: ['this', 'movie', 'star', 'had', 'dinner', 'with', 'my', 'friend']\n",
      "D2: ['i', 'watch', 'this', 'movie', 'on', 'star', 'movies']\n",
      "D3: ['in', 'this', 'movie', 'the', 'opening', 'was', 'focused', 'on', 'the', 'stars', 'in', 'the', 'sky']\n",
      "\n",
      "All tokens combined: ['this', 'movie', 'star', 'had', 'dinner', 'with', 'my', 'friend', 'i', 'watch', 'this', 'movie', 'on', 'star', 'movies', 'in', 'this', 'movie', 'the', 'opening', 'was', 'focused', 'on', 'the', 'stars', 'in', 'the', 'sky']\n",
      "Total tokens: 28\n",
      "\n",
      "STEP 2: CREATE GLOBAL VOCABULARY\n",
      "----------------------------------------\n",
      "Vocabulary: ['dinner', 'focused', 'friend', 'had', 'i', 'in', 'movie', 'movies', 'my', 'on', 'opening', 'sky', 'star', 'stars', 'the', 'this', 'was', 'watch', 'with']\n",
      "Vocabulary size: 19\n",
      "\n",
      "Word to Index mapping:\n",
      "  'dinner' -> 0\n",
      "  'focused' -> 1\n",
      "  'friend' -> 2\n",
      "  'had' -> 3\n",
      "  'i' -> 4\n",
      "  'in' -> 5\n",
      "  'movie' -> 6\n",
      "  'movies' -> 7\n",
      "  'my' -> 8\n",
      "  'on' -> 9\n",
      "  'opening' -> 10\n",
      "  'sky' -> 11\n",
      "  'star' -> 12\n",
      "  'stars' -> 13\n",
      "  'the' -> 14\n",
      "  'this' -> 15\n",
      "  'was' -> 16\n",
      "  'watch' -> 17\n",
      "  'with' -> 18\n",
      "\n",
      "STEP 3: CREATE BOW VECTORS\n",
      "-----------------------------------\n",
      "\n",
      "D1 word frequencies:\n",
      "  'dinner': 1\n",
      "  'friend': 1\n",
      "  'had': 1\n",
      "  'movie': 1\n",
      "  'my': 1\n",
      "  'star': 1\n",
      "  'this': 1\n",
      "  'with': 1\n",
      "D1 BoW vector: [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "\n",
      "D2 word frequencies:\n",
      "  'i': 1\n",
      "  'movie': 1\n",
      "  'movies': 1\n",
      "  'on': 1\n",
      "  'star': 1\n",
      "  'this': 1\n",
      "  'watch': 1\n",
      "D2 BoW vector: [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "\n",
      "D3 word frequencies:\n",
      "  'focused': 1\n",
      "  'in': 2\n",
      "  'movie': 1\n",
      "  'on': 1\n",
      "  'opening': 1\n",
      "  'sky': 1\n",
      "  'stars': 1\n",
      "  'the': 3\n",
      "  'this': 1\n",
      "  'was': 1\n",
      "D3 BoW vector: [0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 0, 1, 3, 1, 1, 0, 0]\n",
      "\n",
      "BoW Matrix shape: (3, 19)\n",
      "BoW Matrix:\n",
      "Docs\\Words  dinner focused  friend     had       i      in   movie  movies      my      on opening     sky    star   stars     the    this     was   watch    with\n",
      "       D1       1       0       1       1       0       0       1       0       1       0       0       0       1       0       0       1       0       0       1\n",
      "       D2       0       0       0       0       1       0       1       1       0       1       0       0       1       0       0       1       0       1       0\n",
      "       D3       0       1       0       0       0       2       1       0       0       1       1       1       0       1       3       1       1       0       0\n",
      "\n",
      "================================================================================\n",
      "PART 2: ONE-HOT ENCODING IMPLEMENTATION\n",
      "================================================================================\n",
      "\n",
      "STEP 1: ONE-HOT ENCODING CONCEPT\n",
      "----------------------------------------\n",
      "One-Hot Encoding creates binary vectors where:\n",
      "- Each dimension represents a unique word\n",
      "- Value = 1 if word is present, 0 if absent\n",
      "- No frequency information, only presence/absence\n",
      "\n",
      "STEP 2: CREATE ONE-HOT VECTORS\n",
      "--------------------------------------\n",
      "\n",
      "D1 unique words: ['dinner', 'friend', 'had', 'movie', 'my', 'star', 'this', 'with']\n",
      "D1 One-Hot vector: [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "\n",
      "D2 unique words: ['i', 'movie', 'movies', 'on', 'star', 'this', 'watch']\n",
      "D2 One-Hot vector: [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "\n",
      "D3 unique words: ['focused', 'in', 'movie', 'on', 'opening', 'sky', 'stars', 'the', 'this', 'was']\n",
      "D3 One-Hot vector: [0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "One-Hot Matrix shape: (3, 19)\n",
      "One-Hot Matrix:\n",
      "Docs\\Words  dinner focused  friend     had       i      in   movie  movies      my      on opening     sky    star   stars     the    this     was   watch    with\n",
      "       D1       1       0       1       1       0       0       1       0       1       0       0       0       1       0       0       1       0       0       1\n",
      "       D2       0       0       0       0       1       0       1       1       0       1       0       0       1       0       0       1       0       1       0\n",
      "       D3       0       1       0       0       0       1       1       0       0       1       1       1       0       1       1       1       1       0       0\n",
      "\n",
      "================================================================================\n",
      "PART 3: DETAILED COMPARISON\n",
      "================================================================================\n",
      "\n",
      "SIDE-BY-SIDE COMPARISON:\n",
      "------------------------------\n",
      "\n",
      "D1:\n",
      "  Original: 'This movie star had dinner with my friend.'\n",
      "  BoW:      [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "  One-Hot:  [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "\n",
      "D2:\n",
      "  Original: 'I watch this movie on star movies.'\n",
      "  BoW:      [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "  One-Hot:  [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "\n",
      "D3:\n",
      "  Original: 'In this movie the opening was focused on the stars in the sky.'\n",
      "  BoW:      [0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 0, 1, 3, 1, 1, 0, 0]\n",
      "  One-Hot:  [0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "================================================================================\n",
      "PART 4: PERFORMANCE METRICS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. DIMENSIONALITY ANALYSIS:\n",
      "------------------------------\n",
      "Vector dimensions: 19\n",
      "Both BoW and One-Hot have same dimensionality\n",
      "\n",
      "2. SPARSITY ANALYSIS:\n",
      "-------------------------\n",
      "D1:\n",
      "  BoW non-zero elements: 8/19 (42.1% dense)\n",
      "  One-Hot non-zero: 8/19 (42.1% dense)\n",
      "  BoW sparsity: 57.9%\n",
      "  One-Hot sparsity: 57.9%\n",
      "D2:\n",
      "  BoW non-zero elements: 7/19 (36.8% dense)\n",
      "  One-Hot non-zero: 7/19 (36.8% dense)\n",
      "  BoW sparsity: 63.2%\n",
      "  One-Hot sparsity: 63.2%\n",
      "D3:\n",
      "  BoW non-zero elements: 10/19 (52.6% dense)\n",
      "  One-Hot non-zero: 10/19 (52.6% dense)\n",
      "  BoW sparsity: 47.4%\n",
      "  One-Hot sparsity: 47.4%\n",
      "\n",
      "Average Sparsity:\n",
      "  BoW: 56.1%\n",
      "  One-Hot: 56.1%\n",
      "\n",
      "3. INFORMATION CONTENT:\n",
      "----------------------------\n",
      "BoW advantages:\n",
      "  ✓ Captures word frequency information\n",
      "  ✓ Can distinguish between documents with same words but different frequencies\n",
      "  ✓ Better for tasks where word importance varies with frequency\n",
      "\n",
      "One-Hot advantages:\n",
      "  ✓ Binary representation (simpler)\n",
      "  ✓ Less sensitive to word frequency variations\n",
      "  ✓ Better for tasks where presence matters more than frequency\n",
      "\n",
      "4. DOCUMENT SIMILARITY ANALYSIS:\n",
      "-----------------------------------\n",
      "Cosine Similarities:\n",
      "  D1 vs D2:\n",
      "    BoW similarity: 0.4009\n",
      "    One-Hot similarity: 0.4009\n",
      "    Difference: 0.0000\n",
      "  D1 vs D3:\n",
      "    BoW similarity: 0.1543\n",
      "    One-Hot similarity: 0.2236\n",
      "    Difference: 0.0693\n",
      "  D2 vs D3:\n",
      "    BoW similarity: 0.2474\n",
      "    One-Hot similarity: 0.3586\n",
      "    Difference: 0.1111\n",
      "\n",
      "5. COMPUTATIONAL COMPLEXITY:\n",
      "--------------------------------\n",
      "Memory Usage:\n",
      "  BoW matrix: 228 bytes\n",
      "  One-Hot matrix: 228 bytes\n",
      "\n",
      "Computational Complexity:\n",
      "  BoW: O(V) per document (V = vocabulary size)\n",
      "  One-Hot: O(V) per document\n",
      "  Both have same time complexity for creation\n",
      "\n",
      "6. USE CASE RECOMMENDATIONS:\n",
      "--------------------------------\n",
      "Use BoW when:\n",
      "  • Word frequency matters (e.g., document classification)\n",
      "  • Need to distinguish repetition importance\n",
      "  • Working with longer documents\n",
      "  • Term frequency is a meaningful signal\n",
      "\n",
      "Use One-Hot when:\n",
      "  • Only word presence/absence matters\n",
      "  • Want to reduce noise from frequency variations\n",
      "  • Memory efficiency is crucial (for sparse documents)\n",
      "  • Working with short documents or keywords\n",
      "\n",
      "================================================================================\n",
      "PART 5: PRACTICAL EXAMPLE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "WHY DIFFERENT SIMILARITIES?\n",
      "------------------------------\n",
      "Let's analyze D1 vs D2 similarity:\n",
      "D1: 'This movie star had dinner with my friend.'\n",
      "D2: 'I watch this movie on star movies.'\n",
      "\n",
      "Shared words: {'this', 'star', 'movie'}\n",
      "D1 BoW vector: [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "D2 BoW vector: [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "\n",
      "Manual calculation:\n",
      "BoW cosine similarity: 0.4009\n",
      "One-Hot cosine similarity: 0.4009\n",
      "\n",
      "Key Insight:\n",
      "BoW considers word frequency, so 'movie' appearing once in each\n",
      "document contributes differently than if it appeared multiple times.\n",
      "One-Hot only cares about presence, making it more binary in nature.\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: BOW vs ONE-HOT ENCODING\n",
      "================================================================================\n",
      "\n",
      "Choose based on your specific use case:\n",
      "• BoW for frequency-sensitive tasks\n",
      "• One-Hot for presence-based tasks\n",
      "• Both lose semantic meaning and word order\n",
      "• Consider advanced methods (TF-IDF, word embeddings) for better performance\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Input documents\n",
    "documents = {\n",
    "    'D1': 'This movie star had dinner with my friend.',\n",
    "    'D2': 'I watch this movie on star movies.',\n",
    "    'D3': 'In this movie the opening was focused on the stars in the sky.'\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BAG OF WORDS vs ONE-HOT ENCODING COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nINPUT DOCUMENTS:\")\n",
    "print(\"-\" * 30)\n",
    "for doc_id, text in documents.items():\n",
    "    print(f\"{doc_id}: '{text}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1: BAG OF WORDS (BOW) IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Preprocessing and Tokenization\n",
    "print(\"\\nSTEP 1: PREPROCESSING AND TOKENIZATION\")\n",
    "print(\"-\" * 50)\n",
    "processed_docs = {}\n",
    "all_tokens = []\n",
    "\n",
    "for doc_id, text in documents.items():\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokens = clean_text.split()\n",
    "    processed_docs[doc_id] = tokens\n",
    "    all_tokens.extend(tokens)\n",
    "    print(f\"{doc_id}: {tokens}\")\n",
    "\n",
    "print(f\"\\nAll tokens combined: {all_tokens}\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "\n",
    "# Step 2: Create Global Vocabulary\n",
    "print(\"\\nSTEP 2: CREATE GLOBAL VOCABULARY\")\n",
    "print(\"-\" * 40)\n",
    "vocabulary = sorted(set(all_tokens))\n",
    "vocab_size = len(vocabulary)\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create word to index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "print(f\"\\nWord to Index mapping:\")\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"  '{word}' -> {idx}\")\n",
    "\n",
    "# Step 3: Create BoW Vectors\n",
    "print(\"\\nSTEP 3: CREATE BOW VECTORS\")\n",
    "print(\"-\" * 35)\n",
    "bow_vectors = {}\n",
    "bow_matrix = []\n",
    "\n",
    "for doc_id, tokens in processed_docs.items():\n",
    "    word_counts = Counter(tokens)\n",
    "    bow_vector = [word_counts.get(word, 0) for word in vocabulary]\n",
    "    bow_vectors[doc_id] = bow_vector\n",
    "    bow_matrix.append(bow_vector)\n",
    "    \n",
    "    print(f\"\\n{doc_id} word frequencies:\")\n",
    "    for word in vocabulary:\n",
    "        count = word_counts.get(word, 0)\n",
    "        if count > 0:\n",
    "            print(f\"  '{word}': {count}\")\n",
    "    print(f\"{doc_id} BoW vector: {bow_vector}\")\n",
    "\n",
    "bow_matrix = np.array(bow_matrix)\n",
    "print(f\"\\nBoW Matrix shape: {bow_matrix.shape}\")\n",
    "print(\"BoW Matrix:\")\n",
    "print(\"Docs\\\\Words\", end=\"\")\n",
    "for word in vocabulary:\n",
    "    print(f\"{word:>8}\", end=\"\")\n",
    "print()\n",
    "for i, doc_id in enumerate(['D1', 'D2', 'D3']):\n",
    "    print(f\"{doc_id:>9}\", end=\"\")\n",
    "    for val in bow_matrix[i]:\n",
    "        print(f\"{val:>8}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: ONE-HOT ENCODING IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSTEP 1: ONE-HOT ENCODING CONCEPT\")\n",
    "print(\"-\" * 40)\n",
    "print(\"One-Hot Encoding creates binary vectors where:\")\n",
    "print(\"- Each dimension represents a unique word\")\n",
    "print(\"- Value = 1 if word is present, 0 if absent\")\n",
    "print(\"- No frequency information, only presence/absence\")\n",
    "\n",
    "print(\"\\nSTEP 2: CREATE ONE-HOT VECTORS\")\n",
    "print(\"-\" * 38)\n",
    "onehot_vectors = {}\n",
    "onehot_matrix = []\n",
    "\n",
    "for doc_id, tokens in processed_docs.items():\n",
    "    unique_tokens = set(tokens)\n",
    "    onehot_vector = [1 if word in unique_tokens else 0 for word in vocabulary]\n",
    "    onehot_vectors[doc_id] = onehot_vector\n",
    "    onehot_matrix.append(onehot_vector)\n",
    "    \n",
    "    print(f\"\\n{doc_id} unique words: {sorted(unique_tokens)}\")\n",
    "    print(f\"{doc_id} One-Hot vector: {onehot_vector}\")\n",
    "\n",
    "onehot_matrix = np.array(onehot_matrix)\n",
    "print(f\"\\nOne-Hot Matrix shape: {onehot_matrix.shape}\")\n",
    "print(\"One-Hot Matrix:\")\n",
    "print(\"Docs\\\\Words\", end=\"\")\n",
    "for word in vocabulary:\n",
    "    print(f\"{word:>8}\", end=\"\")\n",
    "print()\n",
    "for i, doc_id in enumerate(['D1', 'D2', 'D3']):\n",
    "    print(f\"{doc_id:>9}\", end=\"\")\n",
    "    for val in onehot_matrix[i]:\n",
    "        print(f\"{val:>8}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: DETAILED COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSIDE-BY-SIDE COMPARISON:\")\n",
    "print(\"-\" * 30)\n",
    "for doc_id in ['D1', 'D2', 'D3']:\n",
    "    print(f\"\\n{doc_id}:\")\n",
    "    print(f\"  Original: '{documents[doc_id]}'\")\n",
    "    print(f\"  BoW:      {bow_vectors[doc_id]}\")\n",
    "    print(f\"  One-Hot:  {onehot_vectors[doc_id]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: PERFORMANCE METRICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_metrics():\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Vector Dimensionality\n",
    "    print(\"\\n1. DIMENSIONALITY ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Vector dimensions: {vocab_size}\")\n",
    "    print(\"Both BoW and One-Hot have same dimensionality\")\n",
    "    \n",
    "    # 2. Sparsity Analysis\n",
    "    print(\"\\n2. SPARSITY ANALYSIS:\")\n",
    "    print(\"-\" * 25)\n",
    "    bow_sparsity = []\n",
    "    onehot_sparsity = []\n",
    "    \n",
    "    for i, doc_id in enumerate(['D1', 'D2', 'D3']):\n",
    "        bow_nonzero = np.count_nonzero(bow_matrix[i])\n",
    "        onehot_nonzero = np.count_nonzero(onehot_matrix[i])\n",
    "        bow_sparse = (vocab_size - bow_nonzero) / vocab_size * 100\n",
    "        onehot_sparse = (vocab_size - onehot_nonzero) / vocab_size * 100\n",
    "        \n",
    "        bow_sparsity.append(bow_sparse)\n",
    "        onehot_sparsity.append(onehot_sparse)\n",
    "        \n",
    "        print(f\"{doc_id}:\")\n",
    "        print(f\"  BoW non-zero elements: {bow_nonzero}/{vocab_size} ({100-bow_sparse:.1f}% dense)\")\n",
    "        print(f\"  One-Hot non-zero: {onehot_nonzero}/{vocab_size} ({100-onehot_sparse:.1f}% dense)\")\n",
    "        print(f\"  BoW sparsity: {bow_sparse:.1f}%\")\n",
    "        print(f\"  One-Hot sparsity: {onehot_sparse:.1f}%\")\n",
    "    \n",
    "    avg_bow_sparsity = np.mean(bow_sparsity)\n",
    "    avg_onehot_sparsity = np.mean(onehot_sparsity)\n",
    "    print(f\"\\nAverage Sparsity:\")\n",
    "    print(f\"  BoW: {avg_bow_sparsity:.1f}%\")\n",
    "    print(f\"  One-Hot: {avg_onehot_sparsity:.1f}%\")\n",
    "    \n",
    "    # 3. Information Content\n",
    "    print(\"\\n3. INFORMATION CONTENT:\")\n",
    "    print(\"-\" * 28)\n",
    "    print(\"BoW advantages:\")\n",
    "    print(\"  ✓ Captures word frequency information\")\n",
    "    print(\"  ✓ Can distinguish between documents with same words but different frequencies\")\n",
    "    print(\"  ✓ Better for tasks where word importance varies with frequency\")\n",
    "    \n",
    "    print(\"\\nOne-Hot advantages:\")\n",
    "    print(\"  ✓ Binary representation (simpler)\")\n",
    "    print(\"  ✓ Less sensitive to word frequency variations\")\n",
    "    print(\"  ✓ Better for tasks where presence matters more than frequency\")\n",
    "    \n",
    "    # 4. Document Similarity Analysis\n",
    "    print(\"\\n4. DOCUMENT SIMILARITY ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Cosine similarity for BoW\n",
    "    bow_similarities = cosine_similarity(bow_matrix)\n",
    "    onehot_similarities = cosine_similarity(onehot_matrix)\n",
    "    \n",
    "    doc_pairs = [('D1', 'D2'), ('D1', 'D3'), ('D2', 'D3')]\n",
    "    indices = [(0, 1), (0, 2), (1, 2)]\n",
    "    \n",
    "    print(\"Cosine Similarities:\")\n",
    "    for (doc1, doc2), (i, j) in zip(doc_pairs, indices):\n",
    "        bow_sim = bow_similarities[i][j]\n",
    "        onehot_sim = onehot_similarities[i][j]\n",
    "        print(f\"  {doc1} vs {doc2}:\")\n",
    "        print(f\"    BoW similarity: {bow_sim:.4f}\")\n",
    "        print(f\"    One-Hot similarity: {onehot_sim:.4f}\")\n",
    "        print(f\"    Difference: {abs(bow_sim - onehot_sim):.4f}\")\n",
    "    \n",
    "    # 5. Memory and Computational Complexity\n",
    "    print(\"\\n5. COMPUTATIONAL COMPLEXITY:\")\n",
    "    print(\"-\" * 32)\n",
    "    print(\"Memory Usage:\")\n",
    "    bow_memory = bow_matrix.nbytes\n",
    "    onehot_memory = onehot_matrix.nbytes\n",
    "    print(f\"  BoW matrix: {bow_memory} bytes\")\n",
    "    print(f\"  One-Hot matrix: {onehot_memory} bytes\")\n",
    "    \n",
    "    print(\"\\nComputational Complexity:\")\n",
    "    print(\"  BoW: O(V) per document (V = vocabulary size)\")\n",
    "    print(\"  One-Hot: O(V) per document\")\n",
    "    print(\"  Both have same time complexity for creation\")\n",
    "    \n",
    "    # 6. Use Case Analysis\n",
    "    print(\"\\n6. USE CASE RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 32)\n",
    "    print(\"Use BoW when:\")\n",
    "    print(\"  • Word frequency matters (e.g., document classification)\")\n",
    "    print(\"  • Need to distinguish repetition importance\")\n",
    "    print(\"  • Working with longer documents\")\n",
    "    print(\"  • Term frequency is a meaningful signal\")\n",
    "    \n",
    "    print(\"\\nUse One-Hot when:\")\n",
    "    print(\"  • Only word presence/absence matters\")\n",
    "    print(\"  • Want to reduce noise from frequency variations\")\n",
    "    print(\"  • Memory efficiency is crucial (for sparse documents)\")\n",
    "    print(\"  • Working with short documents or keywords\")\n",
    "\n",
    "calculate_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: PRACTICAL EXAMPLE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nWHY DIFFERENT SIMILARITIES?\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Let's analyze D1 vs D2 similarity:\")\n",
    "print(f\"D1: '{documents['D1']}'\")\n",
    "print(f\"D2: '{documents['D2']}'\")\n",
    "\n",
    "print(f\"\\nShared words: {set(processed_docs['D1']) & set(processed_docs['D2'])}\")\n",
    "print(f\"D1 BoW vector: {bow_vectors['D1']}\")\n",
    "print(f\"D2 BoW vector: {bow_vectors['D2']}\")\n",
    "\n",
    "# Calculate manual cosine similarity\n",
    "def manual_cosine(vec1, vec2):\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    norm2 = math.sqrt(sum(b * b for b in vec2))\n",
    "    return dot_product / (norm1 * norm2) if norm1 * norm2 != 0 else 0\n",
    "\n",
    "bow_sim_manual = manual_cosine(bow_vectors['D1'], bow_vectors['D2'])\n",
    "onehot_sim_manual = manual_cosine(onehot_vectors['D1'], onehot_vectors['D2'])\n",
    "\n",
    "print(f\"\\nManual calculation:\")\n",
    "print(f\"BoW cosine similarity: {bow_sim_manual:.4f}\")\n",
    "print(f\"One-Hot cosine similarity: {onehot_sim_manual:.4f}\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"BoW considers word frequency, so 'movie' appearing once in each\")\n",
    "print(\"document contributes differently than if it appeared multiple times.\")\n",
    "print(\"One-Hot only cares about presence, making it more binary in nature.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: BOW vs ONE-HOT ENCODING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nChoose based on your specific use case:\")\n",
    "print(\"• BoW for frequency-sensitive tasks\")\n",
    "print(\"• One-Hot for presence-based tasks\")\n",
    "print(\"• Both lose semantic meaning and word order\")\n",
    "print(\"• Consider advanced methods (TF-IDF, word embeddings) for better performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a52ef5-4b1c-4615-ae05-868e55d58ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
