{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68ce664",
   "metadata": {},
   "source": [
    "# Positional Encoding vs No Positional Encoding\n",
    "This notebook reproduces the experiment comparing token embeddings **with** and **without** sinusoidal Positional Encoding (PE). It generates cosine similarity heatmaps, a PCA projection, and summary metrics.\n",
    "\n",
    "Sentence: *\"The quick brown fox jumps over the lazy dog\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ad254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and helper functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Positional encoding (sinusoidal)\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            div = pos / (10000 ** (i / d_model))\n",
    "            pe[pos, i] = math.sin(div)\n",
    "            if i+1 < d_model:\n",
    "                pe[pos, i+1] = math.cos(div)\n",
    "    return pe\n",
    "\n",
    "def cosine_similarity_matrix(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    Xn = X / (norms + 1e-9)\n",
    "    return Xn @ Xn.T\n",
    "\n",
    "def pairwise_position_diff_matrix(n):\n",
    "    idxs = np.arange(n)\n",
    "    return np.abs(idxs[:,None] - idxs[None,:])\n",
    "\n",
    "def compute_metrics(sim_matrix, pos_diff):\n",
    "    n = sim_matrix.shape[0]\n",
    "    mask = ~np.eye(n, dtype=bool)\n",
    "    sims = sim_matrix[mask]\n",
    "    diffs = pos_diff[mask]\n",
    "    avg_sim = sims.mean()\n",
    "    std_sim = sims.std()\n",
    "    corr = np.corrcoef(diffs.flatten(), sims.flatten())[0,1]\n",
    "    return avg_sim, std_sim, corr\n",
    "\n",
    "print('helpers ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters and embeddings\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = sentence.lower().split()\n",
    "vocab = sorted(set(tokens))\n",
    "token_to_idx = {t:i for i,t in enumerate(vocab)}\n",
    "indices = [token_to_idx[t] for t in tokens]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "d_model = 64\n",
    "n_tokens = len(tokens)\n",
    "\n",
    "# Random token embeddings (simulated learned embeddings)\n",
    "embedding_matrix = np.random.randn(vocab_size, d_model) * 0.5\n",
    "embeddings = np.array([embedding_matrix[idx] for idx in indices])\n",
    "\n",
    "# Positional encoding and combined embeddings\n",
    "pe = positional_encoding(n_tokens, d_model)\n",
    "emb_no_pe = embeddings.copy()\n",
    "emb_with_pe = embeddings + pe\n",
    "\n",
    "print('embeddings created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe38ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrices and metrics\n",
    "sim_no_pe = cosine_similarity_matrix(emb_no_pe)\n",
    "sim_with_pe = cosine_similarity_matrix(emb_with_pe)\n",
    "pos_diff = pairwise_position_diff_matrix(n_tokens)\n",
    "\n",
    "avg_no, std_no, corr_no = compute_metrics(sim_no_pe, pos_diff)\n",
    "avg_pe, std_pe, corr_pe = compute_metrics(sim_with_pe, pos_diff)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Setting':['No Positional Encoding','With Positional Encoding'],\n",
    "    'Avg Cosine Similarity':[avg_no, avg_pe],\n",
    "    'Std Cosine Similarity':[std_no, std_pe],\n",
    "    'Corr(position_diff, similarity)':[corr_no, corr_pe]\n",
    "})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31dee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: similarity heatmap (No PE)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title('Cosine similarity (No Positional Encoding)')\n",
    "plt.imshow(sim_no_pe, interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(n_tokens), tokens, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(n_tokens), tokens)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14614e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: similarity heatmap (With PE)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title('Cosine similarity (With Positional Encoding)')\n",
    "plt.imshow(sim_with_pe, interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(n_tokens), tokens, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(n_tokens), tokens)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: PCA 2D projection comparing embeddings\n",
    "pca = PCA(n_components=2)\n",
    "all_emb = np.vstack([emb_no_pe, emb_with_pe])\n",
    "pca2 = pca.fit_transform(all_emb)\n",
    "proj_no = pca2[:n_tokens]\n",
    "proj_pe = pca2[n_tokens:]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('PCA 2D projection: tokens (No PE) and tokens (With PE)')\n",
    "plt.scatter(proj_no[:,0], proj_no[:,1], marker='o', label='No PE')\n",
    "for i,t in enumerate(tokens):\n",
    "    plt.text(proj_no[i,0], proj_no[i,1], f' {t}', fontsize=9)\n",
    "plt.scatter(proj_pe[:,0], proj_pe[:,1], marker='x', label='With PE')\n",
    "for i,t in enumerate(tokens):\n",
    "    plt.text(proj_pe[i,0], proj_pe[i,1], f' {t}', fontsize=9)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e90b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Bar chart comparing metrics\n",
    "labels = ['Avg Cosine','Std Cosine','PosDiff-Corr']\n",
    "no_vals = [avg_no, std_no, corr_no]\n",
    "pe_vals = [avg_pe, std_pe, corr_pe]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Comparison of metrics: No PE vs With PE')\n",
    "plt.bar(x - width/2, no_vals, width, label='No PE')\n",
    "plt.bar(x + width/2, pe_vals, width, label='With PE')\n",
    "plt.xticks(x, labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and interpretation\n",
    "print('Summary metrics:')\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print('\\nInterpretation:')\n",
    "print('- A negative correlation between position difference and similarity indicates that tokens farther apart tend to be less similar; with PE we expect a stronger negative correlation.')\n",
    "print('- Compare heatmaps and PCA to visually inspect how PE changes embedding geometry.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf27c5c",
   "metadata": {},
   "source": [
    "## Next steps (optional)\n",
    "- Replace random embeddings with pre-trained word embeddings (e.g., GloVe) for more realistic behavior.\n",
    "- Use trainable positional embeddings instead of sinusoidal PE and compare.\n",
    "- Run a small downstream task (e.g., distance-based classifier) to measure empirical performance change.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
