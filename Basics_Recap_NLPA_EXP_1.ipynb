{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2698ec8b-ffbb-4c03-b447-c8c1ab207b8c",
   "metadata": {},
   "source": [
    "# NLPA Laboratory Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181d303-d285-4e16-a68c-fe1985725496",
   "metadata": {},
   "source": [
    "### Setting Up Your Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9cd336-4511-4365-b954-0b1f673157bc",
   "metadata": {},
   "source": [
    "# Core Libraries & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29d109-2947-4476-9bd3-db4652744722",
   "metadata": {},
   "source": [
    "pip install \\\n",
    "  numpy \\\n",
    "  scipy \\\n",
    "  pandas \\\n",
    "  matplotlib \\\n",
    "  jupyter \\\n",
    "  scikit-learn \\\n",
    "  nltk \\\n",
    "  spacy \\\n",
    "  gensim \\\n",
    "  torch torchvision torchaudio \\\n",
    "  tensorflow \\\n",
    "  transformers \\\n",
    "  datasets \\\n",
    "  sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3b5a5-2ba1-42dc-a75c-81c74f0564c1",
   "metadata": {},
   "source": [
    "| Library           | Purpose                                                         |\n",
    "| ----------------- | --------------------------------------------------------------- |\n",
    "| **numpy, scipy**  | Numerical computing                                             |\n",
    "| **pandas**        | Data handling & analysis                                        |\n",
    "| **matplotlib**    | Basic plotting & visualization                                  |\n",
    "| **jupyter**       | Interactive notebooks                                           |\n",
    "| **scikit-learn**  | Traditional ML algorithms & preprocessing                       |\n",
    "| **nltk**          | Classic NLP tasks (tokenization, corpora, simple models)        |\n",
    "| **spaCy**         | Industrial-strength NLP (tokenization, POS, dependency parsing) |\n",
    "| **gensim**        | Topic modeling & word embeddings (Word2Vec, Doc2Vec, LDA)       |\n",
    "| **torch**         | Deep-learning framework (PyTorch)                               |\n",
    "| **tensorflow**    | Deep-learning framework                                         |\n",
    "| **transformers**  | State-of-the-art pre-trained models (BERT, GPT, etc.)           |\n",
    "| **datasets**      | Easy access to common NLP datasets                              |\n",
    "| **sentencepiece** | Subword tokenization (for Transformer models)                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb866a-3db0-4190-a8b0-47d788194980",
   "metadata": {},
   "source": [
    " # Highly Recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ca3086-dc28-4b5b-910f-157478433312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from huggingface-hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from huggingface-hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from huggingface-hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dr.pvvk\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5095de-f73d-4bd9-b6f8-11d907b8f43a",
   "metadata": {},
   "source": [
    "# plotly or seaborn\n",
    "## For richer data visualizations as you analyze text and model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67597042-136e-454b-b166-ec671127a148",
   "metadata": {},
   "source": [
    "# Verifying Your Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b610d914-3477-4bf5-ae77-cd839352b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK: 3.8.1\n",
      "PyTorch: 2.4.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import nltk, torch\n",
    "print(\"NLTK:\", nltk.__version__)\n",
    "# print(\"spaCy:\", spacy.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "# print(\"TensorFlow:\", tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e204196d-40d8-42d6-9cc4-23ecf0d9fc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.7\n",
      "NumPy:         1.24.4\n",
      "SciPy:         1.10.1\n",
      "Pandas:        1.5.3\n",
      "Matplotlib:    3.8.0\n",
      "scikit-learn:  1.2.2\n",
      "NLTK:          3.8.1\n",
      "spaCy:         3.8.7\n",
      "Gensim:        4.3.0\n",
      "PyTorch:       2.4.0+cu118\n",
      "TorchVision:   0.19.0+cu118\n",
      "TorchAudio:    2.4.0+cu118\n",
      "Transformers:  4.51.3\n",
      "Datasets:      4.0.0\n",
      "SentencePiece: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "# Test Python version\n",
    "import sys\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "\n",
    "# Core scientific stack\n",
    "import numpy;       print(\"NumPy:        \", numpy.__version__)\n",
    "import scipy;       print(\"SciPy:        \", scipy.__version__)\n",
    "import pandas;      print(\"Pandas:       \", pandas.__version__)\n",
    "import matplotlib;  print(\"Matplotlib:   \", matplotlib.__version__)\n",
    "\n",
    "# Machine-learning toolkit\n",
    "import sklearn;     print(\"scikit-learn: \", sklearn.__version__)\n",
    "\n",
    "# Classic NLP libraries\n",
    "import nltk;        print(\"NLTK:         \", nltk.__version__)\n",
    "import spacy;       print(\"spaCy:        \", spacy.__version__)\n",
    "import gensim;      print(\"Gensim:       \", gensim.__version__)\n",
    "\n",
    "# Deep-learning frameworks\n",
    "import torch;       print(\"PyTorch:      \", torch.__version__)\n",
    "import torchvision; print(\"TorchVision:  \", torchvision.__version__)\n",
    "import torchaudio;  print(\"TorchAudio:   \", torchaudio.__version__)\n",
    "\n",
    "#import tensorflow as tf\n",
    "#print(\"TensorFlow:   \", tf.__version__)\n",
    "\n",
    "# Transformer models & datasets\n",
    "import transformers;  print(\"Transformers: \", transformers.__version__)\n",
    "import datasets;      print(\"Datasets:     \", datasets.__version__)\n",
    "import sentencepiece; print(\"SentencePiece:\", sentencepiece.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f422f71e-a419-4e6e-bffe-178729ee2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47de447-f6d4-42ad-8dde-d53148c767f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11d811b-5c6f-48c2-9c3d-5d3707ad834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332de422-e4d0-40cc-8131-2442ff156ee3",
   "metadata": {},
   "source": [
    "# How Machines Interpret Text - Like Operating Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e76e3d-389c-46eb-8d98-aa223990d8cb",
   "metadata": {},
   "source": [
    "### Unicode Transformation Format – 8-bit. --utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14125d16-caaa-4470-938d-524ca5c5a688",
   "metadata": {},
   "source": [
    "# Text Encoding & Bit-Level Demo\r\n",
    "\r\n",
    "This notebook shows how a string of text is represented internally:\r\n",
    "- **Unicode code points**  \r\n",
    "- **UTF-8 byte sequences**  \r\n",
    "- **Bit-level patterns**  \r\n",
    "- **Binary file I/O**  \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a3be1a-4295-4ab0-93b3-9066f06332e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define some sample text (ASCII + non-ASCII)\n",
    "text = \"Hello, 世界!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cffba1-11cd-4f86-a83e-7f2d9da55582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-character breakdown:\n",
      "  'H'  → code point: U+0048  → bytes: [72]\n",
      "  'e'  → code point: U+0065  → bytes: [101]\n",
      "  'l'  → code point: U+006C  → bytes: [108]\n",
      "  'l'  → code point: U+006C  → bytes: [108]\n",
      "  'o'  → code point: U+006F  → bytes: [111]\n",
      "  ','  → code point: U+002C  → bytes: [44]\n",
      "  ' '  → code point: U+0020  → bytes: [32]\n",
      "  '世'  → code point: U+4E16  → bytes: [228, 184, 150]\n",
      "  '界'  → code point: U+754C  → bytes: [231, 149, 140]\n",
      "  '!'  → code point: U+0021  → bytes: [33]\n"
     ]
    }
   ],
   "source": [
    "# 2) Per-character breakdown: code points and UTF-8 bytes\n",
    "print(\"Per-character breakdown:\")\n",
    "for ch in text:\n",
    "    code_point = ord(ch)\n",
    "    utf8_bytes = ch.encode('utf-8')\n",
    "    print(f\"  '{ch}'  → code point: U+{code_point:04X}  → bytes: {list(utf8_bytes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09929fbd-de6f-4b4e-9e83-45fd2cfa4d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full text as bytes: b'Hello, \\xe4\\xb8\\x96\\xe7\\x95\\x8c!'\n"
     ]
    }
   ],
   "source": [
    "# 3) Full text as a UTF-8 byte sequence\n",
    "full_bytes = text.encode('utf-8')\n",
    "print(\"\\nFull text as bytes:\", full_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935a3ba5-9b79-41ce-aca6-5fb140d9edca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text as bits:    01001000 01100101 01101100 01101100 01101111 00101100 00100000 11100100 10111000 10010110 11100111 10010101 10001100 00100001\n"
     ]
    }
   ],
   "source": [
    "# 4) Bit-level representation of those bytes\n",
    "bit_strs = [format(b, '08b') for b in full_bytes]\n",
    "print(\"Full text as bits:   \", ' '.join(bit_strs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cec2b4-cf07-429c-952f-bf53796ab912",
   "metadata": {},
   "source": [
    "# Q. Is this the encoding format for NLP applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a625a-1ecf-46b6-b92d-0093e2596ea4",
   "metadata": {},
   "source": [
    "# Encoding in NLP: Why UTF-8 Is Recommended\n",
    "\n",
    "In this notebook we will:\n",
    "- See why UTF-8 is the go-to encoding for NLP.\n",
    "- Show how ASCII encoding fails on non-ASCII characters.\n",
    "- Demonstrate how mis-decoding (e.g., Latin-1 → UTF-8) corrupts text.\n",
    "- Confirm that UTF-8 round-trips without loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487648de-775c-4838-ba27-c762546d2abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! 你好, café 😊\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing ASCII, CJK, accented latin, and emoji\n",
    "text = \"Hello, world! 你好, café 😊\"\n",
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79299a9e-7bc9-46e0-bf0c-7a7291669e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCII encoding error: 'ascii' codec can't encode characters in position 14-15: ordinal not in range(128)\n"
     ]
    }
   ],
   "source": [
    "# 1) Attempt to encode with ASCII (should error)\n",
    "try:\n",
    "    ascii_bytes = text.encode('ascii')\n",
    "    print(\"ASCII bytes:\", ascii_bytes)\n",
    "except UnicodeEncodeError as e:\n",
    "    print(\"ASCII encoding error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7320b7-b9d1-4545-ba90-d035cb96cd74",
   "metadata": {},
   "source": [
    "> **Why this fails:**  \n",
    "> ASCII only covers code points 0–127. Characters like “你” (U+4F60), “é” (U+00E9), or “😊” (U+1F60A) lie outside that range, so `text.encode('ascii')` raises a `UnicodeEncodeError`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c054e3a-daa2-455d-80ff-74a51956935a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- **UTF-8** is the industry standard for NLP because it can losslessly encode **all** Unicode code points.  \n",
    "- Using narrower encodings (ASCII, Latin-1, etc.) either throws errors or corrupts your data.  \n",
    "- **Always** ensure your entire NLP pipeline—file I/O, model inputs, serialization, network transfers—is UTF-8 end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d48d2-6c8d-44f0-a225-8c14953bb1e4",
   "metadata": {},
   "source": [
    "# Lets do a simple text encoding for a machine learning application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13a0665-1c7b-4787-8699-6b31e550c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Your raw sentence\n",
    "text = \"I love natural language processing courese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "508cf3c7-e8df-4853-8eac-a3123b19d264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'natural', 'language', 'processing', 'courese']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Lowercase & whitespace-tokenize\n",
    "tokens = text.lower().split()\n",
    "#    → [\"i\", \"love\", \"natural\", \"language\", \"processing\", \"courese\"]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "097f9b5b-9c8d-4e43-b031-3fc07f2ce946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1, 'love': 2, 'natural': 3, 'language': 4, 'processing': 5, 'courese': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Build a tiny vocab mapping (in real life you'd pre-build on your whole corpus)\n",
    "vocab = {tok: idx+1 for idx, tok in enumerate(tokens)}\n",
    "#    → {'i':1, 'love':2, 'natural':3, 'language':4, 'processing':5, 'courese':6}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66982320-6179-4835-aff8-5ccd2ac75f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer-encoded: [1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# 4. Convert sentence to a list of token IDs\n",
    "encoded = [vocab[t] for t in tokens]\n",
    "print(\"Integer-encoded:\", encoded)\n",
    "# Integer-encoded: [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff4735-01ab-4c5d-b4d5-50e54cbd9244",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f786f4d6-68b8-4d5f-8cb5-749e10288413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot shape: (6, 7)\n",
      "First 2 one-hot rows:\n",
      " [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = len(vocab) + 1   # +1 if you reserve zero for padding/OOV\n",
    "# Create identity matrix of size vocab_size\n",
    "eye = np.eye(vocab_size)\n",
    "\n",
    "# Build one-hot rows for each token ID\n",
    "one_hot = eye[encoded]\n",
    "print(\"One-hot shape:\", one_hot.shape)\n",
    "# One-hot shape: (6, 7)\n",
    "\n",
    "print(\"First 2 one-hot rows:\\n\", one_hot[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cb58ddd-6187-4177-9df3-fe3a9b13c367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "048c6ca2-fcc0-4e9c-923f-1bbe78289a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           One-hot vector\n",
      "--------------- --------------\n",
      "i               [0 1 0 0 0 0 0]\n",
      "love            [0 0 1 0 0 0 0]\n",
      "natural         [0 0 0 1 0 0 0]\n",
      "language        [0 0 0 0 1 0 0]\n",
      "processing      [0 0 0 0 0 1 0]\n",
      "courese         [0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 2) One-hot encoding: print each token with its vector\n",
    "import numpy as np\n",
    "\n",
    "# (Assuming you’ve already defined `tokens`, `vocab`, and `encoded` as before:)\n",
    "# tokens   = [\"i\",\"love\",\"natural\",\"language\",\"processing\",\"courese\"]\n",
    "# vocab    = {'i':1, 'love':2, ...}\n",
    "# encoded  = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "vocab_size = len(vocab) + 1   # +1 if you reserve 0 for padding/OOV\n",
    "eye = np.eye(vocab_size, dtype=int)\n",
    "\n",
    "one_hot = eye[encoded]        # shape: (sentence_length, vocab_size)\n",
    "\n",
    "print(\"Token\".ljust(15), \"One-hot vector\")\n",
    "print(\"-\"*15, \"-\"* (vocab_size*2))\n",
    "for token, vec in zip(tokens, one_hot):\n",
    "    print(token.ljust(15), vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e78717c-ff1d-445d-9653-cdb8a7d5d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           Embedding vector\n",
      "---------------\n",
      "i               [0.15601864 0.15599452 0.05808361 0.86617615]\n",
      "love            [0.60111501 0.70807258 0.02058449 0.96990985]\n",
      "natural         [0.83244264 0.21233911 0.18182497 0.18340451]\n",
      "language        [0.30424224 0.52475643 0.43194502 0.29122914]\n",
      "processing      [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      "courese         [0.45606998 0.78517596 0.19967378 0.51423444]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Re-define tokens, vocab, and encoded\n",
    "tokens = [\"i\", \"love\", \"natural\", \"language\", \"processing\", \"courese\"]\n",
    "vocab = {tok: idx+1 for idx, tok in enumerate(tokens)}\n",
    "encoded = [vocab[t] for t in tokens]\n",
    "\n",
    "# Simulate a PyTorch nn.Embedding with a NumPy matrix\n",
    "np.random.seed(42)\n",
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 4\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "\n",
    "# Lookup embeddings\n",
    "embedded_sequence = embedding_matrix[encoded]\n",
    "\n",
    "print(\"Token\".ljust(15), \"Embedding vector\")\n",
    "print(\"-\" * 15)\n",
    "for token, vec in zip(tokens, embedded_sequence):\n",
    "    print(token.ljust(15), vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22af7df-9e7f-4432-8b8c-e7af6d57588c",
   "metadata": {},
   "source": [
    "# Q. What is this embedding layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc32237-1335-4995-af1d-6a52b564c144",
   "metadata": {},
   "source": [
    "## An embedding layer is essentially a learnable lookup table that maps discrete input tokens (words, subwords, characters, item IDs, etc.) to continuous, dense vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680a004-0d53-434e-bd0b-3cd0ab8eb1d6",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "\n",
    "A sequence of integer IDs, each representing a token in your vocabulary.\n",
    "\n",
    "E.g. [12, 5, 89, 32] might correspond to [\"I\", \"love\", \"NLP\", \".\"].\n",
    "\n",
    "Parameters:\n",
    "\n",
    "A weight matrix W of shape (V, D), where\n",
    "\n",
    "V = size of your vocabulary (or number of unique IDs),\n",
    "\n",
    "D = dimensionality of the embedding vectors you want (e.g. 50, 100, 300).\n",
    "\n",
    "Operation:\n",
    "\n",
    "For each input ID i, you return the i-th row of W, which is a D-dimensional vector.\n",
    "\n",
    "If your input is a sequence of length L, the output is a matrix of shape (L, D).\n",
    "\n",
    "Learning:\n",
    "\n",
    "During training, W is updated via backpropagation so that tokens used in similar contexts acquire similar vectors.\n",
    "\n",
    "You can also initialize W from pre-trained embeddings (e.g. GloVe, word2vec) and either freeze or fine-tune them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "760d33f9-dcd6-4f33-91d5-32df28558858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "vocab_size = 10000   # e.g. 10k words\n",
    "embedding_dim = 128  # each word → a 128-dim vector\n",
    "\n",
    "# Create the layer\n",
    "embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Sample batch of token IDs (batch_size=2, seq_len=5)\n",
    "input_ids = torch.tensor([[12, 45, 900, 32, 1],\n",
    "                          [ 4, 23,  17,  0, 7]], dtype=torch.long)\n",
    "\n",
    "# Forward pass → shape (2, 5, 128)\n",
    "output = embed(input_ids)\n",
    "print(output.shape)  # torch.Size([2, 5, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301c7a3-ecb5-48a0-814d-4442900418f3",
   "metadata": {},
   "source": [
    "# Proving Semantic Structure with a PyTorch Embedding Layer\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load a small pretrained embedding matrix into `nn.Embedding`.  \n",
    "2. Extract the vectors for a handful of words.  \n",
    "3. Compute pairwise cosine similarities.  \n",
    "4. Observe that “cat” ↔ “dog” and “apple” ↔ “banana” are much closer than unrelated pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7598c185-5370-4a4b-9ce5-b94dd1255b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise cosine similarities:\n",
      "\n",
      "     cat ↔ dog   : 0.528\n",
      "     cat ↔ car   : 0.288\n",
      "     cat ↔ apple : 0.760\n",
      "     cat ↔ banana: 0.523\n",
      "     cat ↔ king  : 0.126\n",
      "     cat ↔ queen : 0.291\n",
      "     dog ↔ car   : 0.562\n",
      "     dog ↔ apple : 0.345\n",
      "     dog ↔ banana: 0.125\n",
      "     dog ↔ king  : -0.130\n",
      "     dog ↔ queen : 0.829\n",
      "     car ↔ apple : -0.339\n",
      "     car ↔ banana: -0.249\n",
      "     car ↔ king  : 0.119\n",
      "     car ↔ queen : 0.553\n",
      "   apple ↔ banana: 0.612\n",
      "   apple ↔ king  : -0.029\n",
      "   apple ↔ queen : 0.159\n",
      "  banana ↔ king  : -0.622\n",
      "  banana ↔ queen : -0.309\n",
      "    king ↔ queen : 0.073\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# ---- 2.1) A toy “pretrained” embedding matrix ----\n",
    "# In practice, replace this with loaded GloVe/BERT weights.\n",
    "# Here we simulate a 10-word vocab, each with a 5-dim vector.\n",
    "np.random.seed(0)\n",
    "pretrained_weights = np.random.randn(10, 5).astype(np.float32)\n",
    "\n",
    "# Let’s pretend our vocab is:\n",
    "vocab = [\"<pad>\",\"cat\",\"dog\",\"car\",\"apple\",\"banana\",\"king\",\"queen\",\"man\",\"woman\"]\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "\n",
    "# ---- 2.2) Build the Embedding layer and load weights ----\n",
    "emb = nn.Embedding(num_embeddings=len(vocab), embedding_dim=5)\n",
    "emb.weight.data.copy_(torch.from_numpy(pretrained_weights))\n",
    "\n",
    "# ---- 2.3) Select words we care about ----\n",
    "words = [\"cat\",\"dog\",\"car\",\"apple\",\"banana\",\"king\",\"queen\"]\n",
    "idxs  = torch.tensor([word2idx[w] for w in words], dtype=torch.long)\n",
    "\n",
    "# ---- 2.4) Lookup their embeddings ----\n",
    "vectors = emb(idxs)                          # shape: (7, 5)\n",
    "vectors = vectors.detach().cpu().numpy()\n",
    "\n",
    "# ---- 2.5) Cosine similarity function ----\n",
    "def cosine_sim(a, b):\n",
    "    return (a @ b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# ---- 2.6) Compute and print pairwise similarities ----\n",
    "print(\"Pairwise cosine similarities:\\n\")\n",
    "for i, w1 in enumerate(words):\n",
    "    for j, w2 in enumerate(words[i+1:], start=i+1):\n",
    "        sim = cosine_sim(vectors[i], vectors[j])\n",
    "        print(f\"  {w1:>6} ↔ {w2:<6}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd53869-9791-4bbf-ba6d-75723d48eb18",
   "metadata": {},
   "source": [
    "> You should see that pairs like **cat ↔ dog** and **apple ↔ banana** have notably higher cosine-similarity scores than, say, **cat ↔ car** or **king ↔ apple**.  \n",
    ">\n",
    "> This demonstrates that once you load semantically-trained embedding weights into a PyTorch `nn.Embedding` layer, the geometric structure of that vector space indeed places similar-meaning words close together—exactly what lets downstream models generalize by proximity in embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d0b75-c57c-44a8-9c05-e637dc502ac1",
   "metadata": {},
   "source": [
    "| Score range   | Label              |\n",
    "| ------------- | ------------------ |\n",
    "| \\[0.90, 1.00] | Nearly identical   |\n",
    "| \\[0.75, 0.90) | Highly similar     |\n",
    "| \\[0.50, 0.75) | Moderately similar |\n",
    "| \\[0.25, 0.50) | Slightly related   |\n",
    "| \\[0.00, 0.25) | Unrelated          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aed3807-537e-42f0-b45c-4a2821f1c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat ↔ dog   : 0.92 (92%), 23°, Nearly identical\n",
      "   cat ↔ car   : 0.12 (12%), 83°, Unrelated\n",
      " apple ↔ banana: 0.88 (88%), 28°, Highly similar\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample cosine similarities\n",
    "pairs = {\n",
    "    (\"cat\",\"dog\"):   0.92,\n",
    "    (\"cat\",\"car\"):   0.12,\n",
    "    (\"apple\",\"banana\"): 0.88\n",
    "}\n",
    "\n",
    "def interpret_similarity(s):\n",
    "    if s >= 0.90: return \"Nearly identical\"\n",
    "    if s >= 0.75: return \"Highly similar\"\n",
    "    if s >= 0.50: return \"Moderately similar\"\n",
    "    if s >= 0.25: return \"Slightly related\"\n",
    "    return \"Unrelated\"\n",
    "\n",
    "for (w1,w2), score in pairs.items():\n",
    "    angle = np.degrees(np.arccos(score))\n",
    "    print(f\"{w1:>6} ↔ {w2:<6}: {score:.2f} ({score*100:.0f}%), {angle:.0f}°, {interpret_similarity(score)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9b56395-ed92-4b3f-b46b-d9e9d7d2523c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "s = \\frac{a \\cdot b}{\\|a\\|\\;\\|b\\|}, \n",
       "\\quad\n",
       "\\theta = \\cos^{-1}(s)\\times\\frac{180}{\\pi}\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "display(Math(r\"\"\"\n",
    "s = \\frac{a \\cdot b}{\\|a\\|\\;\\|b\\|}, \n",
    "\\quad\n",
    "\\theta = \\cos^{-1}(s)\\times\\frac{180}{\\pi}\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dcf5709-2399-4c0d-878f-45c0acd13976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity s = 0.9997\n",
      "angle θ = 1.4°\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Define two example embedding vectors\n",
    "a = np.array([0.90, 0.10, 0.00])\n",
    "b = np.array([0.88, 0.12, 0.00])\n",
    "\n",
    "# 2) Compute cosine similarity s\n",
    "s = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# 3) Print s and also the corresponding angle θ\n",
    "theta = np.degrees(np.arccos(s))\n",
    "\n",
    "print(f\"cosine similarity s = {s:.4f}\")\n",
    "print(f\"angle θ = {theta:.1f}°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb3370-e550-43a6-9fe2-8b3b21d6ba95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
