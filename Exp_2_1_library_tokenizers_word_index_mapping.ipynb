{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f893b9",
   "metadata": {},
   "source": [
    "# Tokenization & Word↔Index Mapping with NLTK, Gensim, spaCy, and PyTorch/torchtext\n",
    "\n",
    "This notebook shows **built-in tokenization** features and **word↔index** mappings across popular libraries.\n",
    "\n",
    "Libraries covered:\n",
    "- **NLTK** (tokenizers)\n",
    "- **Gensim** (`simple_preprocess`, `Dictionary`)\n",
    "- **spaCy** (tokenizer + `StringStore`)\n",
    "- **PyTorch/torchtext** (`get_tokenizer`, `build_vocab_from_iterator`) + quick `nn.Embedding` demo\n",
    "\n",
    "> If a library is missing, the cell will **gracefully explain** how to install it and, when possible, show a small fallback demo so the flow isn’t broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d487e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 example texts.\n",
      "1. I love NLP, esp. tokenization!\n",
      "2. Byte-Pair Encoding is cool; so is WordPiece.\n",
      "3. Let's build vocabularies and map tokens ↔ ids.\n"
     ]
    }
   ],
   "source": [
    "# Example texts we will reuse across libraries\n",
    "texts = [\n",
    "    \"I love NLP, esp. tokenization!\",\n",
    "    \"Byte-Pair Encoding is cool; so is WordPiece.\",\n",
    "    \"Let's build vocabularies and map tokens ↔ ids.\"\n",
    "]\n",
    "print('Loaded', len(texts), 'example texts.')\n",
    "for i, t in enumerate(texts, 1):\n",
    "    print(f'{i}.', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a22ea6",
   "metadata": {},
   "source": [
    "## 1) NLTK — Tokenize and Map Words ↔ Indexes\n",
    "We try `nltk.word_tokenize` (requires the **punkt** model). If it’s not available, we fall back to `wordpunct_tokenize` (no download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4d725a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer method: nltk.word_tokenize (punkt)\n",
      "NLTK tokens 1: ['I', 'love', 'NLP', ',', 'esp', '.', 'tokenization', '!']\n",
      "NLTK tokens 2: ['Byte-Pair', 'Encoding', 'is', 'cool', ';', 'so', 'is', 'WordPiece', '.']\n",
      "NLTK tokens 3: ['Let', \"'s\", 'build', 'vocabularies', 'and', 'map', 'tokens', '↔', 'ids', '.']\n",
      "\n",
      "NLTK stoi sample (first 10): [('!', 1), (\"'s\", 2), (',', 3), ('.', 4), (';', 5), ('Byte-Pair', 6), ('Encoding', 7), ('I', 8), ('Let', 9), ('NLP', 10)]\n",
      "Doc0 -> ids: [8, 18, 10, 3, 15, 4, 21, 1]\n",
      "ids -> Doc0: ['I', 'love', 'NLP', ',', 'esp', '.', 'tokenization', '!']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "try:\n",
    "    import nltk\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        # Ensure punkt is present; otherwise a LookupError is thrown\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        tok_fn = word_tokenize\n",
    "        method = \"nltk.word_tokenize (punkt)\"\n",
    "    except LookupError:\n",
    "        from nltk.tokenize import wordpunct_tokenize as tok_fn\n",
    "        method = \"nltk.wordpunct_tokenize (no model download)\"\n",
    "    print('Tokenizer method:', method)\n",
    "    nl_tokens = [tok_fn(t) for t in texts]\n",
    "    for i, toks in enumerate(nl_tokens, 1):\n",
    "        print(f'NLTK tokens {i}:', toks)\n",
    "\n",
    "    # Build a simple word-index mapping (contiguous indices)\n",
    "    vocab = sorted(set(itertools.chain.from_iterable(nl_tokens)))\n",
    "    stoi = {w:i for i,w in enumerate(vocab, start=1)}  # reserve 0 for <pad>\n",
    "    itos = {i:w for w,i in stoi.items()}\n",
    "    print('\\nNLTK stoi sample (first 10):', list(itertools.islice(stoi.items(), 10)))\n",
    "\n",
    "    # Convert first document to ids and back\n",
    "    ids_doc0 = [stoi[w] for w in nl_tokens[0]]\n",
    "    back_doc0 = [itos[i] for i in ids_doc0]\n",
    "    print('Doc0 -> ids:', ids_doc0)\n",
    "    print('ids -> Doc0:', back_doc0)\n",
    "except Exception as e:\n",
    "    print('NLTK not available or failed to tokenize:', type(e).__name__, str(e))\n",
    "    print('Try: pip install nltk  (and optionally: nltk.download(\"punkt\"))')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83fa1e",
   "metadata": {},
   "source": [
    "## 2) Gensim — `simple_preprocess` + `Dictionary` (token ↔ id)\n",
    "`gensim.utils.simple_preprocess` lowercases & strips punctuation. `gensim.corpora.Dictionary` builds token↔id mapping and supports `doc2bow`, `doc2idx`, and reverse lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3536b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim tokens:\n",
      " 1: ['i', 'love', 'nlp', 'esp', 'tokenization']\n",
      " 2: ['byte', 'pair', 'encoding', 'is', 'cool', 'so', 'is', 'wordpiece']\n",
      " 3: ['let', 's', 'build', 'vocabularies', 'and', 'map', 'tokens', 'ids']\n",
      "\n",
      "Dictionary size: 20\n",
      "token2id (first 10): [('esp', 0), ('i', 1), ('love', 2), ('nlp', 3), ('tokenization', 4), ('byte', 5), ('cool', 6), ('encoding', 7), ('is', 8), ('pair', 9)]\n",
      "Doc0 doc2bow: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "Doc0 doc2idx: [1, 2, 3, 0, 4]\n",
      "id->token (from doc2idx): ['i', 'love', 'nlp', 'esp', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from gensim.utils import simple_preprocess\n",
    "    from gensim.corpora import Dictionary\n",
    "\n",
    "    gs_tokens = [simple_preprocess(t, deacc=True, min_len=1) for t in texts]\n",
    "    print('Gensim tokens:')\n",
    "    for i, toks in enumerate(gs_tokens, 1):\n",
    "        print(f' {i}:', toks)\n",
    "\n",
    "    # Build dictionary (token<->id)\n",
    "    dictionary = Dictionary(gs_tokens)\n",
    "    print('\\nDictionary size:', len(dictionary))\n",
    "    print('token2id (first 10):', list(itertools.islice(dictionary.token2id.items(), 10)))\n",
    "\n",
    "    # doc2bow (list of (token_id, count)) and doc2idx (ids sequence)\n",
    "    bow0 = dictionary.doc2bow(gs_tokens[0])\n",
    "    idx0 = dictionary.doc2idx(gs_tokens[0], unknown_word_index=-1)\n",
    "    print('Doc0 doc2bow:', bow0)\n",
    "    print('Doc0 doc2idx:', idx0)\n",
    "\n",
    "    # Reverse lookup id->token\n",
    "    ids_only = [idx for idx in idx0 if idx >= 0]\n",
    "    rev0 = [dictionary[i] for i in ids_only]\n",
    "    print('id->token (from doc2idx):', rev0)\n",
    "except Exception as e:\n",
    "    print('Gensim not available:', type(e).__name__, str(e))\n",
    "    print('Try: pip install gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c4f2d",
   "metadata": {},
   "source": [
    "## 3) spaCy — Tokenizer + `StringStore` (token ↔ hash id)\n",
    "We use `spacy.blank(\"en\")` so no model download is needed. `nlp.vocab.strings` maps **string ↔ integer key**. Note these are **hash-based ids** (not contiguous). For NN embeddings, build your own contiguous mapping if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ede847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy tokens 1: ['I', 'love', 'NLP', ',', 'esp', '.', 'tokenization', '!']\n",
      "spaCy tokens 2: ['Byte', '-', 'Pair', 'Encoding', 'is', 'cool', ';', 'so', 'is', 'WordPiece', '.']\n",
      "spaCy tokens 3: ['Let', \"'s\", 'build', 'vocabularies', 'and', 'map', 'tokens', '↔', 'ids', '.']\n",
      "\n",
      "spaCy StringStore ids (doc0): [4690420944186131903, 3702023516439754181, 15832915187156881108, 2593208677638477497, 9888022622711118288, 12646065887601541794, 15418258291467594259, 17494803046312582752]\n",
      "Back to tokens (doc0): ['I', 'love', 'NLP', ',', 'esp', '.', 'tokenization', '!']\n",
      "\n",
      "Contiguous stoi (first 10): [('!', 1), (\"'s\", 2), (',', 3), ('-', 4), ('.', 5), (';', 6), ('Byte', 7), ('Encoding', 8), ('I', 9), ('Let', 10)]\n",
      "Doc0 -> ids: [9, 20, 11, 3, 17, 5, 23, 1]\n",
      "ids -> Doc0: ['I', 'love', 'NLP', ',', 'esp', '.', 'tokenization', '!']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.blank('en')  # tokenizer only, no model download\n",
    "    spacy_tokens = [[tok.text for tok in nlp(t)] for t in texts]\n",
    "    for i, toks in enumerate(spacy_tokens, 1):\n",
    "        print(f'spaCy tokens {i}:', toks)\n",
    "\n",
    "    # Built-in StringStore mapping (string<->id)\n",
    "    stringstore = nlp.vocab.strings\n",
    "    spacy_ids = [[stringstore[tok] for tok in toks] for toks in spacy_tokens]\n",
    "    spacy_back = [[stringstore[id_] for id_ in ids] for ids in spacy_ids]\n",
    "    print('\\nspaCy StringStore ids (doc0):', spacy_ids[0])\n",
    "    print('Back to tokens (doc0):', spacy_back[0])\n",
    "\n",
    "    # Optional: contiguous mapping for embeddings\n",
    "    import itertools\n",
    "    vocab = sorted(set(itertools.chain.from_iterable(spacy_tokens)))\n",
    "    stoi = {w:i for i,w in enumerate(vocab, start=1)}\n",
    "    itos = {i:w for w,i in stoi.items()}\n",
    "    ids_doc0 = [stoi[w] for w in spacy_tokens[0]]\n",
    "    back_doc0 = [itos[i] for i in ids_doc0]\n",
    "    print('\\nContiguous stoi (first 10):', list(itertools.islice(stoi.items(), 10)))\n",
    "    print('Doc0 -> ids:', ids_doc0)\n",
    "    print('ids -> Doc0:', back_doc0)\n",
    "except Exception as e:\n",
    "    print('spaCy not available:', type(e).__name__, str(e))\n",
    "    print('Try: pip install spacy  (no model needed for tokenizer-only demo)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952ac92",
   "metadata": {},
   "source": [
    "## 4) PyTorch / torchtext — Tokenizer + Vocabulary + Embedding\n",
    "We try `torchtext.data.utils.get_tokenizer(\"basic_english\")` and `torchtext.vocab.build_vocab_from_iterator`. If `torchtext` is missing, we fall back to a simple regex tokenizer and build a vocabulary manually to show the mapping and an `nn.Embedding` lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36f08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchtext not available; showing a minimal PyTorch fallback.\n",
      "Error: ModuleNotFoundError No module named 'torchtext'\n",
      "fallback tokens 1: ['i', 'love', 'nlp', ',', 'esp', '.', 'tokenization', '!']\n",
      "fallback tokens 2: ['byte', '-', 'pair', 'encoding', 'is', 'cool', ';', 'so', 'is', 'wordpiece', '.']\n",
      "fallback tokens 3: ['let', \"'\", 's', 'build', 'vocabularies', 'and', 'map', 'tokens', '↔', 'ids', '.']\n",
      "Doc0 -> ids: [14, 18, 20, 4, 13, 6, 24, 2]\n",
      "ids -> Doc0: ['i', 'love', 'nlp', ',', 'esp', '.', 'tokenization', '!']\n",
      "Embedding output shape (batch, seq, dim): (1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "try:\n",
    "    import torch\n",
    "    from torchtext.data.utils import get_tokenizer\n",
    "    from torchtext.vocab import build_vocab_from_iterator\n",
    "    import torch.nn as nn\n",
    "\n",
    "    print('Using torchtext basic_english tokenizer')\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    tt_tokens = [tokenizer(t) for t in texts]\n",
    "    for i, toks in enumerate(tt_tokens, 1):\n",
    "        print(f'torchtext tokens {i}:', toks)\n",
    "\n",
    "    vocab = build_vocab_from_iterator(tt_tokens, specials=['<unk>', '<pad>'])\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    print('\\nVocab size:', len(vocab))\n",
    "    print('Sample token->id:', {tok: vocab[tok] for tok in tt_tokens[0]})\n",
    "\n",
    "    # Map doc0 to ids and back\n",
    "    ids0 = [vocab[t] for t in tt_tokens[0]]\n",
    "    back0 = [vocab.lookup_token(i) for i in ids0]\n",
    "    print('Doc0 -> ids:', ids0)\n",
    "    print('ids -> Doc0:', back0)\n",
    "\n",
    "    # Embedding example\n",
    "    emb = nn.Embedding(num_embeddings=len(vocab), embedding_dim=8)\n",
    "    x = torch.tensor(ids0).unsqueeze(0)  # shape (1, seq_len)\n",
    "    E = emb(x)\n",
    "    print('Embedding output shape (batch, seq, dim):', tuple(E.shape))\n",
    "except Exception as e:\n",
    "    print('torchtext not available; showing a minimal PyTorch fallback.')\n",
    "    print('Error:', type(e).__name__, str(e))\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    # very simple regex tokenizer\n",
    "    tt_tokens = [re.findall(r\"\\w+|[^\\w\\s]\", t.lower()) for t in texts]\n",
    "    for i, toks in enumerate(tt_tokens, 1):\n",
    "        print(f'fallback tokens {i}:', toks)\n",
    "    # build contiguous vocab (reserve 0:<pad>, 1:<unk>)\n",
    "    vocab_set = sorted(set(itertools.chain.from_iterable(tt_tokens)))\n",
    "    stoi = {w:i+2 for i,w in enumerate(vocab_set)}\n",
    "    stoi['<pad>'] = 0; stoi['<unk>'] = 1\n",
    "    itos = {i:w for w,i in stoi.items()}\n",
    "    ids0 = [stoi.get(t, 1) for t in tt_tokens[0]]\n",
    "    back0 = [itos[i] for i in ids0]\n",
    "    print('Doc0 -> ids:', ids0)\n",
    "    print('ids -> Doc0:', back0)\n",
    "    # Embedding\n",
    "    emb = nn.Embedding(num_embeddings=len(stoi), embedding_dim=8)\n",
    "    x = torch.tensor(ids0).unsqueeze(0)\n",
    "    E = emb(x)\n",
    "    print('Embedding output shape (batch, seq, dim):', tuple(E.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162d739",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- **NLTK**: `word_tokenize` (needs punkt) or `wordpunct_tokenize` — then build your own `stoi/itos`.\n",
    "- **Gensim**: `simple_preprocess` + `Dictionary` gives **token↔id** out of the box, plus `doc2idx`, `doc2bow`.\n",
    "- **spaCy**: tokenizer + `StringStore` (string↔hash), or build your own contiguous ids for embeddings.\n",
    "- **PyTorch/torchtext**: `get_tokenizer('basic_english')` + `build_vocab_from_iterator`, then `vocab()` and `lookup_token()`; **easy to plug into `nn.Embedding`**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
