{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a36497e-b692-45d7-9dc3-887afbe9e454",
   "metadata": {},
   "source": [
    "# Exp_8: To build, train, test and evaluate a simple neural network on curated vector dataset. Build the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6fdf5-7604-46d4-bbd2-d40f831bcb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62fe33-175a-4bec-8dad-e73b34381d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data and targets\n",
    "input_data = [-1, 0, 1, 2, 3]\n",
    "target_data = [1, 2, 3, 4, 5]  # Fixed to match input length\n",
    "type(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5666b3e-7813-4e4c-9f6c-45cb22ed9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(input_data, dtype=torch.float32).view(-1, 1)  # Reshape to column vector\n",
    "y = torch.tensor(target_data, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc83f3-6c9a-4183-8896-8096203f231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657bb931-d0a6-4c6b-a852-e1a1c9bd7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d79d04-830a-4c54-b3bf-9e2f4c28ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input (X): {X.flatten().tolist()}\")\n",
    "print(f\"Target (y): {y.flatten().tolist()}\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f98a1-46f7-49dd-b5f2-6267d46471bd",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77767a-9d45-4b0c-9270-2c13960b72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network with multiple layers.\n",
    "    Architecture: Input -> Hidden1 -> Hidden2 -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=1, hidden1_size=8, hidden2_size=4, output_size=1):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.layer1 = nn.Linear(input_size, hidden1_size)    # Input to first hidden layer\n",
    "        self.layer2 = nn.Linear(hidden1_size, hidden2_size)  # First to second hidden layer\n",
    "        self.layer3 = nn.Linear(hidden2_size, output_size)   # Second hidden to output layer\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()      # ReLU activation for hidden layers\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation (optional)\n",
    "        \n",
    "        # Store intermediate outputs for visualization\n",
    "        self.layer_outputs = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîç Forward Pass Details:\")\n",
    "        print(f\"Input: {x.flatten().tolist()}\")\n",
    "        \n",
    "        # Layer 1: Input -> Hidden1\n",
    "        z1 = self.layer1(x)\n",
    "        print(f\"After Linear Layer 1 (before activation): {z1.detach().numpy().round(3)}\")\n",
    "        \n",
    "        a1 = self.relu(z1)\n",
    "        print(f\"After ReLU Activation 1: {a1.detach().numpy().round(3)}\")\n",
    "        self.layer_outputs['hidden1'] = a1.detach()\n",
    "        \n",
    "        # Layer 2: Hidden1 -> Hidden2\n",
    "        z2 = self.layer2(a1)\n",
    "        print(f\"After Linear Layer 2 (before activation): {z2.detach().numpy().round(3)}\")\n",
    "        \n",
    "        a2 = self.relu(z2)\n",
    "        print(f\"After ReLU Activation 2: {a2.detach().numpy().round(3)}\")\n",
    "        self.layer_outputs['hidden2'] = a2.detach()\n",
    "        \n",
    "        # Layer 3: Hidden2 -> Output\n",
    "        output = self.layer3(a2)\n",
    "        print(f\"Final Output: {output.detach().numpy().round(3)}\")\n",
    "        self.layer_outputs['output'] = output.detach()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff0547-2c79-4e21-9588-10dc0f82d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "HYPERPARAMETERS = {\n",
    "    'learning_rate': 0.01,\n",
    "    'epochs': 100,\n",
    "    'hidden1_size': 8,\n",
    "    'hidden2_size': 4,\n",
    "    'weight_init': 'xavier_uniform'\n",
    "}\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in HYPERPARAMETERS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5de143-bb0d-4051-80e9-e7fc8e6b403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "model = MyNeuralNetwork(\n",
    "    input_size=1, \n",
    "    hidden1_size=HYPERPARAMETERS['hidden1_size'],\n",
    "    hidden2_size=HYPERPARAMETERS['hidden2_size'],\n",
    "    output_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb074894-eb80-447d-9c76-b88e75bb34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights using Xavier uniform initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656b95c-ff44-46f2-ae01-63472b550ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display initial weights\n",
    "print(\"\\nInitial Weights:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.numpy().round(4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ac050-83b2-49ef-b11b-a86295714f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "print(\"Loss Function: Mean Squared Error (MSE)\")\n",
    "print(\"MSE = (1/n) * Œ£(predicted - actual)¬≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c6c09-e7f8-4b2d-8c14-7b5daae1213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=HYPERPARAMETERS['learning_rate'])\n",
    "print(f\"Optimizer: SGD with learning rate = {HYPERPARAMETERS['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8ce0a-562d-4036-ab85-8a9b2ab11af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for tracking training metrics\n",
    "loss_history = []\n",
    "predictions_history = []\n",
    "weight_history = {name: [] for name, _ in model.named_parameters()}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Each epoch includes: Forward Pass -> Loss Calculation -> Backpropagation -> Weight Update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820392b6-67db-4bdd-a889-3fe8b9377e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(HYPERPARAMETERS['epochs']):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(predictions, y)\n",
    "    loss_history.append(loss.item())\n",
    "    predictions_history.append(predictions.detach().numpy().copy())\n",
    "    \n",
    "    # Store weights for visualization\n",
    "    for name, param in model.named_parameters():\n",
    "        weight_history[name].append(param.data.clone())\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if epoch % 100 == 0 or epoch < 5:\n",
    "        print(f\"\\nEpoch {epoch}:\")\n",
    "        print(f\"  Loss: {loss.item():.6f}\")\n",
    "        print(f\"  Predictions: {predictions.detach().numpy().flatten().round(3)}\")\n",
    "        print(f\"  Targets: {y.numpy().flatten()}\")\n",
    "        \n",
    "        if epoch < 5:  # Show gradients for first few epochs\n",
    "            print(\"  Gradients:\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"    {name}: {param.grad.data.numpy().round(6)}\")\n",
    "\n",
    "print(f\"\\nTraining completed! Final loss: {loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b795071-9518-4a5a-bbbd-f392589654a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions\n",
    "final_predictions = model(X).detach().numpy().flatten()\n",
    "targets_np = y.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cadeb2-b141-4382-a159-e19c5857a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "mse = mean_squared_error(targets_np, final_predictions)\n",
    "mae = mean_absolute_error(targets_np, final_predictions)\n",
    "r2 = r2_score(targets_np, final_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  Mean Squared Error (MSE): {mse:.6f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"  R¬≤ Score: {r2:.6f}\")\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Input -> Prediction (Target)\")\n",
    "for i, (inp, pred, target) in enumerate(zip(input_data, final_predictions, targets_np)):\n",
    "    print(f\"  {inp} -> {pred:.3f} ({target})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4adb4-f37b-47dd-9960-d7fa280fc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Neural Network Learning Process Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Loss Curve\n",
    "axes[0, 0].plot(loss_history, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss Over Time')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss (MSE)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# 2. Predictions vs Targets\n",
    "axes[0, 1].scatter(targets_np, final_predictions, color='red', s=100, alpha=0.7, label='Predictions')\n",
    "axes[0, 1].plot([min(targets_np), max(targets_np)], [min(targets_np), max(targets_np)], 'k--', alpha=0.8, label='Perfect Prediction')\n",
    "axes[0, 1].set_title('Predictions vs Targets')\n",
    "axes[0, 1].set_xlabel('Target Values')\n",
    "axes[0, 1].set_ylabel('Predicted Values')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Input vs Output Mapping\n",
    "axes[0, 2].plot(input_data, targets_np, 'go-', label='Target', markersize=8, linewidth=2)\n",
    "axes[0, 2].plot(input_data, final_predictions, 'ro-', label='Predicted', markersize=8, linewidth=2)\n",
    "axes[0, 2].set_title('Input-Output Mapping')\n",
    "axes[0, 2].set_xlabel('Input')\n",
    "axes[0, 2].set_ylabel('Output')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Weight Evolution (Layer 1)\n",
    "layer1_weights = np.array([w[0].numpy() for w in weight_history['layer1.weight']])\n",
    "for i in range(min(4, layer1_weights.shape[1])):  # Show first 4 weights\n",
    "    axes[1, 0].plot(layer1_weights[:, i], label=f'Weight {i+1}', linewidth=2)\n",
    "axes[1, 0].set_title('Layer 1 Weight Evolution')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Weight Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Gradient Descent Visualization (simplified 2D representation)\n",
    "# Sample some points along the loss surface for visualization\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "loss_surface = []\n",
    "for x_val in x_range:\n",
    "    # Create temporary predictions using a simple linear relationship\n",
    "    temp_pred = x_val * np.array(input_data) + 2\n",
    "    temp_loss = np.mean((temp_pred - target_data)**2)\n",
    "    loss_surface.append(temp_loss)\n",
    "\n",
    "axes[1, 1].plot(x_range, loss_surface, 'b-', linewidth=2, alpha=0.7, label='Loss Surface')\n",
    "axes[1, 1].scatter([0], [loss_history[-1]], color='red', s=100, label='Final Position', zorder=5)\n",
    "axes[1, 1].set_title('Gradient Descent Concept')\n",
    "axes[1, 1].set_xlabel('Parameter Space (Simplified)')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Learning Rate Effect Visualization\n",
    "epochs_sample = np.arange(0, min(500, len(loss_history)))\n",
    "axes[1, 2].plot(epochs_sample, loss_history[:len(epochs_sample)], 'g-', linewidth=2)\n",
    "axes[1, 2].set_title(f'Learning Rate Effect (LR={HYPERPARAMETERS[\"learning_rate\"]})')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Loss')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd7473-3cef-4b98-8349-ab7bfa842c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9. NETWORK ACTIVATION VISUALIZATION\n",
    "# ==========================================\n",
    "print(\"\\nüîç STEP 9: NETWORK ACTIVATION ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Perform one final forward pass to get layer outputs\n",
    "_ = model(X)\n",
    "layer_outputs = model.layer_outputs\n",
    "\n",
    "# Visualize layer activations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Neural Network Layer Activations', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Hidden Layer 1 activations\n",
    "im1 = axes[0].imshow(layer_outputs['hidden1'].T, cmap='viridis', aspect='auto')\n",
    "axes[0].set_title('Hidden Layer 1 Activations')\n",
    "axes[0].set_xlabel('Input Samples')\n",
    "axes[0].set_ylabel('Neurons')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Hidden Layer 2 activations\n",
    "im2 = axes[1].imshow(layer_outputs['hidden2'].T, cmap='viridis', aspect='auto')\n",
    "axes[1].set_title('Hidden Layer 2 Activations')\n",
    "axes[1].set_xlabel('Input Samples')\n",
    "axes[1].set_ylabel('Neurons')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Output layer\n",
    "axes[2].plot(range(len(input_data)), layer_outputs['output'].numpy().flatten(), 'ro-', linewidth=2, markersize=8, label='Network Output')\n",
    "axes[2].plot(range(len(input_data)), targets_np, 'go-', linewidth=2, markersize=8, label='Target Output')\n",
    "axes[2].set_title('Output Layer')\n",
    "axes[2].set_xlabel('Input Samples')\n",
    "axes[2].set_ylabel('Output Value')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 10. KEY CONCEPTS EXPLANATION\n",
    "# ==========================================\n",
    "print(\"\\nüìö STEP 10: KEY CONCEPTS EXPLAINED\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ KEY NEURAL NETWORK CONCEPTS:\n",
    "\n",
    "1. FORWARD PASS:\n",
    "   - Data flows from input through hidden layers to output\n",
    "   - Each layer applies: output = activation(weights √ó input + bias)\n",
    "   - ReLU activation: max(0, x) - introduces non-linearity\n",
    "\n",
    "2. LOSS FUNCTION (MSE):\n",
    "   - Measures difference between predictions and targets\n",
    "   - MSE = (1/n) √ó Œ£(predicted - actual)¬≤\n",
    "   - Guides the learning process\n",
    "\n",
    "3. BACKPROPAGATION:\n",
    "   - Calculates gradients of loss with respect to each weight\n",
    "   - Uses chain rule to propagate errors backward\n",
    "   - Tells us how to adjust weights to reduce loss\n",
    "\n",
    "4. GRADIENT DESCENT:\n",
    "   - Updates weights in direction that reduces loss\n",
    "   - new_weight = old_weight - learning_rate √ó gradient\n",
    "   - Learning rate controls step size\n",
    "\n",
    "5. ACTIVATION FUNCTIONS:\n",
    "   - ReLU: Introduces non-linearity, allows learning complex patterns\n",
    "   - Without activation, network would be just linear transformations\n",
    "\n",
    "6. HYPERPARAMETERS:\n",
    "   - Learning rate: Too high = unstable, too low = slow learning\n",
    "   - Network size: More neurons = more capacity but risk of overfitting\n",
    "   - Epochs: Number of complete passes through data\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"The neural network has successfully learned to map inputs to targets!\")\n",
    "print(f\"Final loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"The network can now predict outputs for new inputs based on the learned pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0274c-94c9-46ca-8934-9059df8faa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
