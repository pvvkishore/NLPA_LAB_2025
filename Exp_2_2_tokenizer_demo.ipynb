{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d959e48c",
   "metadata": {},
   "source": [
    "# Tokenization Demo Notebook\n",
    "\n",
    "This notebook implements and compares several tokenizers to highlight their behavior and simple performance metrics.\n",
    "\n",
    "**Tokenizers covered**:\n",
    "- Word-level (regex-based)\n",
    "- Character-level\n",
    "- BPE (char-level with `</w>` marker)\n",
    "- WordPiece (toy greedy longest-match with `##` continuation)\n",
    "- SentencePiece-style **Unigram** (toy Viterbi segmentation with `‚ñÅ` boundaries)\n",
    "- **Byte-level BPE** (toy, UTF-8 bytes; fully reversible)\n",
    "\n",
    "> These are **educational toy implementations**, intended to show the ideas. For production use, prefer libraries such as **Hugging Face `tokenizers`**, **SentencePiece**, or **tiktoken**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e870db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready. pandas: 1.5.3 | numpy: 1.24.4\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re, time, math\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "print('Environment ready. pandas:', pd.__version__, '| numpy:', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5006d6",
   "metadata": {},
   "source": [
    "## Evaluation Sentences and Tiny Training Corpus\n",
    "We evaluate on a few sentences (including multilingual and emoji) and train toy subword models on a tiny in-memory corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79d5475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example & corpus loaded. Eval sentences: 5 | Train lines: 7\n"
     ]
    }
   ],
   "source": [
    "SENT_FIXED = \"I am thrilled to learn gen AI and build my own applications in @2025*\"\n",
    "sentences_eval = [\n",
    "    \"Hello world!\",\n",
    "    \"State-of-the-art models work.\",\n",
    "    SENT_FIXED,\n",
    "    \"I love üçï and Œª-calculus in 2025!\",\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå\"\n",
    "]\n",
    "\n",
    "corpus_train = [\n",
    "    \"We build generative AI systems in 2025.\",\n",
    "    \"I am thrilled to learn gen AI and build my own applications in @2025*\",\n",
    "    \"Tokenization balances vocabulary size and sequence length.\",\n",
    "    \"Byte-level BPE helps with emojis like üçï and code like print(x).\",\n",
    "    \"WordPiece and Unigram are popular subword methods.\",\n",
    "    \"LLaMA and GPT use different tokenizers for efficiency.\",\n",
    "    \"State-of-the-art models work well on multilingual data.\",\n",
    "]\n",
    "print('Example & corpus loaded. Eval sentences:', len(sentences_eval), '| Train lines:', len(corpus_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a70b2",
   "metadata": {},
   "source": [
    "## Metrics Helper\n",
    "We compute simple, comparable metrics across tokenizers: `chars`, `bytes`, `tokens`, `chars/token`, `bytes/token`, `OOV`, `detokenizes_exact`, `time_ms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12dec4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics helper ready.\n"
     ]
    }
   ],
   "source": [
    "def metrics_from_tokens(text: str, tokens: List[Any], detokenize_fn=None, oov_count:int=None):\n",
    "    text_chars = len(text)\n",
    "    text_bytes = len(text.encode('utf-8', errors='strict'))\n",
    "    n_tokens = len(tokens)\n",
    "    avg_chars_per_token = (text_chars / n_tokens) if n_tokens else np.nan\n",
    "    def token_bytes_len(tok):\n",
    "        if isinstance(tok, (bytes, bytearray)):\n",
    "            return len(tok)\n",
    "        if isinstance(tok, tuple) and all(isinstance(b, int) for b in tok):\n",
    "            return len(tok)\n",
    "        return len(str(tok).encode('utf-8', errors='ignore'))\n",
    "    bytes_per_token = np.mean([token_bytes_len(t) for t in tokens]) if tokens else np.nan\n",
    "    chars_per_token = avg_chars_per_token\n",
    "    oov = int(oov_count) if oov_count is not None else 0\n",
    "    recon_ok = None\n",
    "    if detokenize_fn is not None:\n",
    "        try:\n",
    "            recon = detokenize_fn(tokens)\n",
    "            recon_ok = (recon == text)\n",
    "        except Exception:\n",
    "            recon_ok = False\n",
    "    return {\n",
    "        \"chars\": text_chars,\n",
    "        \"bytes\": text_bytes,\n",
    "        \"tokens\": n_tokens,\n",
    "        \"chars/token\": round(chars_per_token, 3) if isinstance(chars_per_token, (int, float)) else np.nan,\n",
    "        \"bytes/token\": round(bytes_per_token, 3) if isinstance(bytes_per_token, (int, float)) else np.nan,\n",
    "        \"OOV\": oov,\n",
    "        \"detokenizes_exact\": recon_ok\n",
    "    }\n",
    "print('Metrics helper ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4323e9a",
   "metadata": {},
   "source": [
    "## 1) Word-level Tokenizer (Regex-based)\n",
    "Simple whitespace/punctuation splitting. Easy to read; suffers **OOV explosion** and large vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37a47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordTokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", flags=re.UNICODE)\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return self.pattern.findall(text)\n",
    "    def detokenize(self, tokens: List[str]) -> str:\n",
    "        out = []\n",
    "        prev_w = False\n",
    "        for t in tokens:\n",
    "            is_punct = re.match(r\"[^\\w\\s]\", t) is not None\n",
    "            is_word = not is_punct and not t.isspace()\n",
    "            if out and is_word and prev_w:\n",
    "                out.append(\" \")\n",
    "            elif out and is_word and (not prev_w):\n",
    "                out.append(\" \")\n",
    "            elif out and is_punct:\n",
    "                pass\n",
    "            out.append(t)\n",
    "            prev_w = is_word\n",
    "        return \"\".join(out)\n",
    "print('WordTokenizer ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef8137",
   "metadata": {},
   "source": [
    "## 2) Character-level Tokenizer\n",
    "Every character is a token. No OOV; very long sequences; weak per-token semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701ad4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharTokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "class CharTokenizer:\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "    def detokenize(self, tokens: List[str]) -> str:\n",
    "        return \"\".join(tokens)\n",
    "print('CharTokenizer ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6147c77",
   "metadata": {},
   "source": [
    "## 3) BPE (char-level) ‚Äì helpers and tokenizer\n",
    "Frequency-based **merging of most frequent adjacent pairs** until a target vocab size is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141ecfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE helpers & tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "def get_vocab_from_corpus(corpus: List[str]) -> List[List[str]]:\n",
    "    vocab = []\n",
    "    for line in corpus:\n",
    "        for word in re.findall(r\"\\S+\", line):\n",
    "            vocab.append(list(word) + [\"</w>\"])\n",
    "    return vocab\n",
    "\n",
    "def count_pairs(vocab_seqs: List[List[str]]) -> Counter:\n",
    "    pairs = Counter()\n",
    "    for symbols in vocab_seqs:\n",
    "        for a, b in zip(symbols, symbols[1:]):\n",
    "            pairs[(a, b)] += 1\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(vocab_seqs: List[List[str]], pair: Tuple[str, str]) -> List[List[str]]:\n",
    "    a, b = pair\n",
    "    merged = a + b\n",
    "    new_vocab = []\n",
    "    for symbols in vocab_seqs:\n",
    "        i = 0\n",
    "        new_symbols = []\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols)-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                new_symbols.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_vocab.append(new_symbols)\n",
    "    return new_vocab\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, merges: List[Tuple[str, str]]):\n",
    "        self.ranks = {tuple(m): i for i, m in enumerate(merges)}\n",
    "    @staticmethod\n",
    "    def train(corpus: List[str], num_merges: int = 200) -> \"BPETokenizer\":\n",
    "        vocab = get_vocab_from_corpus(corpus)\n",
    "        merges = []\n",
    "        for _ in range(num_merges):\n",
    "            pairs = count_pairs(vocab)\n",
    "            if not pairs: break\n",
    "            best = pairs.most_common(1)[0][0]\n",
    "            merges.append(best)\n",
    "            vocab = merge_pair(vocab, best)\n",
    "        return BPETokenizer(merges)\n",
    "    def bpe_word(self, word: str) -> List[str]:\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "        while True:\n",
    "            pairs = [((symbols[i], symbols[i+1]), i) for i in range(len(symbols)-1)]\n",
    "            ranked = [(self.ranks.get(p, 10**9), idx, p) for p, idx in pairs]\n",
    "            if not ranked: break\n",
    "            rank, idx, pair = min(ranked)\n",
    "            if rank == 10**9: break\n",
    "            a, b = pair\n",
    "            symbols = symbols[:idx] + [a+b] + symbols[idx+2:]\n",
    "        if symbols and symbols[-1] == \"</w>\":\n",
    "            symbols = symbols[:-1]\n",
    "        return symbols\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        words = re.findall(r\"\\S+\", text)\n",
    "        toks = []\n",
    "        for w in words:\n",
    "            toks.extend(self.bpe_word(w) + [\"<space>\"])\n",
    "        if toks and toks[-1] == \"<space>\":\n",
    "            toks = toks[:-1]\n",
    "        return toks\n",
    "    def detokenize(self, tokens: List[str]) -> str:\n",
    "        words = []\n",
    "        cur = []\n",
    "        for t in tokens:\n",
    "            if t == \"<space>\":\n",
    "                words.append(\"\".join(cur)); cur = []\n",
    "            else:\n",
    "                cur.append(t)\n",
    "        if cur: words.append(\"\".join(cur))\n",
    "        return \" \".join(words)\n",
    "print('BPE helpers & tokenizer ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c62b79",
   "metadata": {},
   "source": [
    "## 4) WordPiece (toy)\n",
    "Greedy longest-match over a learned subword vocabulary; uses `##` to mark continuation pieces. May emit `[UNK]` for unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706b9ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece (toy) ready.\n"
     ]
    }
   ],
   "source": [
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab: set, unk_token=\"[UNK]\"):\n",
    "        self.vocab = vocab\n",
    "        self.unk = unk_token\n",
    "    @staticmethod\n",
    "    def train(corpus: List[str], vocab_size: int = 300) -> \"WordPieceTokenizer\":\n",
    "        words = []\n",
    "        for line in corpus:\n",
    "            words.extend(re.findall(r\"\\S+\", line))\n",
    "        base = set(ch for w in words for ch in w)\n",
    "        vocab = set(list(base))\n",
    "        substr_freq = Counter()\n",
    "        for w in words:\n",
    "            L = len(w)\n",
    "            for i in range(L):\n",
    "                for j in range(i+1, min(L, i+6)+1):\n",
    "                    substr_freq[w[i:j]] += 1\n",
    "        for piece, _ in substr_freq.most_common():\n",
    "            if len(vocab) >= vocab_size: break\n",
    "            if piece not in vocab: vocab.add(piece)\n",
    "        for w in Counter(words).most_common(50):\n",
    "            if len(vocab) >= vocab_size: break\n",
    "            vocab.add(w[0])\n",
    "        return WordPieceTokenizer(vocab)\n",
    "    def tokenize_word(self, word: str) -> List[str]:\n",
    "        tokens, i = [], 0\n",
    "        while i < len(word):\n",
    "            match = None\n",
    "            for j in range(len(word), i, -1):\n",
    "                piece = word[i:j]\n",
    "                if piece in self.vocab:\n",
    "                    match = piece; break\n",
    "            if match is None:\n",
    "                if word[i] in self.vocab: match = word[i]\n",
    "                else: return [self.unk]\n",
    "            tokens.append(match if i==0 else (\"##\"+match))\n",
    "            i += len(match)\n",
    "        return tokens\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        toks = []\n",
    "        for w in re.findall(r\"\\S+\", text):\n",
    "            toks.extend(self.tokenize_word(w) + [\"<space>\"])\n",
    "        if toks and toks[-1] == \"<space>\": toks = toks[:-1]\n",
    "        return toks\n",
    "    def detokenize(self, tokens: List[str]) -> str:\n",
    "        words, cur = [], \"\"\n",
    "        for t in tokens:\n",
    "            if t == \"<space>\":\n",
    "                words.append(cur); cur = \"\"\n",
    "            elif t.startswith(\"##\"):\n",
    "                cur += t[2:]\n",
    "            elif t == self.unk:\n",
    "                cur += \"\"\n",
    "            else:\n",
    "                if cur: words.append(cur)\n",
    "                cur = t\n",
    "        if cur: words.append(cur)\n",
    "        return \" \".join(words)\n",
    "print('WordPiece (toy) ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f043a2",
   "metadata": {},
   "source": [
    "## 5) SentencePiece-style Unigram (toy)\n",
    "Probabilistic Unigram LM over candidate pieces; segmentation via Viterbi to maximize likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000a6de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram (toy) ready.\n"
     ]
    }
   ],
   "source": [
    "class UnigramTokenizer:\n",
    "    def __init__(self, pieces_probs: Dict[str, float]):\n",
    "        self.pieces_probs = pieces_probs\n",
    "        self.pieces = set(pieces_probs.keys())\n",
    "    @staticmethod\n",
    "    def train(corpus: List[str], vocab_size: int = 300) -> \"UnigramTokenizer\":\n",
    "        words = []\n",
    "        for line in corpus:\n",
    "            for w in re.findall(r\"\\S+\", line):\n",
    "                words.append(\"‚ñÅ\"+w)\n",
    "        cand = Counter()\n",
    "        for w in words:\n",
    "            L = len(w)\n",
    "            for i in range(L):\n",
    "                for j in range(i+1, min(L, i+8)+1):\n",
    "                    cand[w[i:j]] += 1\n",
    "        most = cand.most_common(vocab_size-1)\n",
    "        total = sum(cnt for _, cnt in most) + 1e-9\n",
    "        probs = {p: cnt/total for p, cnt in most}\n",
    "        for ch in set(\"\".join(words)):\n",
    "            if ch not in probs and len(probs) < vocab_size:\n",
    "                probs[ch] = 1.0 / (total * 10_000)\n",
    "        Z = sum(probs.values())\n",
    "        for k in list(probs.keys()):\n",
    "            probs[k] /= Z\n",
    "        return UnigramTokenizer(probs)\n",
    "    def viterbi_segment(self, w_with_bar: str) -> List[str]:\n",
    "        n = len(w_with_bar)\n",
    "        dp = [(-math.inf, -1)] * (n+1)\n",
    "        dp[0] = (0.0, -1)\n",
    "        for i in range(n):\n",
    "            if dp[i][0] == -math.inf: continue\n",
    "            for j in range(i+1, min(n, i+12)+1):\n",
    "                piece = w_with_bar[i:j]\n",
    "                if piece in self.pieces:\n",
    "                    logp = math.log(self.pieces_probs[piece] + 1e-12)\n",
    "                    if dp[i][0] + logp > dp[j][0]:\n",
    "                        dp[j] = (dp[i][0] + logp, i)\n",
    "        if dp[n][0] == -math.inf:\n",
    "            return list(w_with_bar)\n",
    "        out = []\n",
    "        cur = n\n",
    "        while cur > 0:\n",
    "            prev = dp[cur][1]\n",
    "            out.append(w_with_bar[prev:cur])\n",
    "            cur = prev\n",
    "        out.reverse()\n",
    "        return out\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        toks = []\n",
    "        for w in re.findall(r\"\\S+\", text):\n",
    "            toks.extend(self.viterbi_segment(\"‚ñÅ\"+w))\n",
    "        return toks\n",
    "    def detokenize(self, tokens: List[str]) -> str:\n",
    "        return \"\".join(tokens).replace(\"‚ñÅ\",\" \").lstrip()\n",
    "print('Unigram (toy) ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49dd9e",
   "metadata": {},
   "source": [
    "## 6) Byte-level BPE (toy)\n",
    "Operate directly on UTF-8 bytes (0‚Äì255). Guarantees reversibility and no OOV; merges are learned on bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb61747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-level BPE (toy) ready.\n"
     ]
    }
   ],
   "source": [
    "def bytes_from_text(text: str) -> Tuple[int, ...]:\n",
    "    return tuple(text.encode('utf-8', errors='strict'))\n",
    "def text_from_bytes(bseq: List[int]) -> str:\n",
    "    return bytes(bseq).decode('utf-8', errors='strict')\n",
    "def byte_pairs(seq: Tuple[Any, ...]) -> Counter:\n",
    "    return Counter(zip(seq, seq[1:]))\n",
    "def merge_byte_pair(seq: Tuple[Any, ...], pair: Tuple[Any, Any]):\n",
    "    merged, i = [], 0\n",
    "    a, b = pair\n",
    "    while i < len(seq):\n",
    "        if i < len(seq)-1 and seq[i] == a and seq[i+1] == b:\n",
    "            merged.append((a, b)); i += 2\n",
    "        else:\n",
    "            merged.append(seq[i]); i += 1\n",
    "    return tuple(merged)\n",
    "\n",
    "class ByteLevelBPETokenizer:\n",
    "    def __init__(self, merges: List[Tuple[Any, Any]]):\n",
    "        self.ranks = {m: i for i, m in enumerate(merges)}\n",
    "    @staticmethod\n",
    "    def train(corpus: List[str], num_merges: int = 300) -> \"ByteLevelBPETokenizer\":\n",
    "        data = [bytes_from_text(line) for line in corpus]\n",
    "        merges = []\n",
    "        for _ in range(num_merges):\n",
    "            total_pairs = Counter()\n",
    "            for seq in data:\n",
    "                total_pairs.update(byte_pairs(seq))\n",
    "            if not total_pairs: break\n",
    "            best = total_pairs.most_common(1)[0][0]\n",
    "            merges.append(best)\n",
    "            data = [merge_byte_pair(seq, best) for seq in data]\n",
    "        return ByteLevelBPETokenizer(merges)\n",
    "    def tokenize(self, text: str) -> List[Any]:\n",
    "        seq = tuple(bytes_from_text(text))\n",
    "        while True:\n",
    "            pairs = list(zip(seq, seq[1:]))\n",
    "            if not pairs: break\n",
    "            ranks = [(self.ranks.get(p, 10**9), idx, p) for idx, p in enumerate(pairs)]\n",
    "            rmin, idx, pair = min(ranks)\n",
    "            if rmin == 10**9: break\n",
    "            merged = []\n",
    "            i = 0\n",
    "            a, b = pair\n",
    "            while i < len(seq):\n",
    "                if i < len(seq)-1 and seq[i] == a and seq[i+1] == b:\n",
    "                    merged.append((a, b)); i += 2\n",
    "                else:\n",
    "                    merged.append(seq[i]); i += 1\n",
    "            seq = tuple(merged)\n",
    "        def flatten(sym):\n",
    "            if isinstance(sym, int): return (sym,)\n",
    "            out = []\n",
    "            for s in sym: out.extend(flatten(s))\n",
    "            return tuple(out)\n",
    "        tokens = [flatten(s) for s in seq]\n",
    "        return tokens\n",
    "    def detokenize(self, tokens: List[Tuple[int, ...]]) -> str:\n",
    "        flat = []\n",
    "        for tok in tokens: flat.extend(tok)\n",
    "        return text_from_bytes(flat)\n",
    "\n",
    "def pretty_byte_token(tok: Tuple[int, ...]) -> str:\n",
    "    try:\n",
    "        s = bytes(tok).decode('utf-8')\n",
    "        if all(32 <= b < 127 for b in tok):\n",
    "            return s\n",
    "        else:\n",
    "            return repr(s)\n",
    "    except Exception:\n",
    "        return \"[\" + \" \".join(f\"{b:02x}\" for b in tok) + \"]\"\n",
    "print('Byte-level BPE (toy) ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d28aa8",
   "metadata": {},
   "source": [
    "## Train Tokenizers (toy models)\n",
    "No training needed for word/char; train BPE/WordPiece/Unigram/Byte-BPE on the tiny corpus above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00eedb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokenizers instantiated.\n"
     ]
    }
   ],
   "source": [
    "word_tok = WordTokenizer()\n",
    "char_tok = CharTokenizer()\n",
    "bpe_tok = BPETokenizer.train(corpus_train, num_merges=200)\n",
    "wp_tok = WordPieceTokenizer.train(corpus_train, vocab_size=300)\n",
    "uni_tok = UnigramTokenizer.train(corpus_train, vocab_size=300)\n",
    "byte_bpe_tok = ByteLevelBPETokenizer.train(corpus_train, num_merges=300)\n",
    "print('All tokenizers instantiated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0bd96",
   "metadata": {},
   "source": [
    "## Evaluate and Compare\n",
    "Tokenize each evaluation sentence with each tokenizer and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7932b092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>text</th>\n",
       "      <th>chars</th>\n",
       "      <th>bytes</th>\n",
       "      <th>tokens</th>\n",
       "      <th>chars/token</th>\n",
       "      <th>bytes/token</th>\n",
       "      <th>OOV</th>\n",
       "      <th>detokenizes_exact</th>\n",
       "      <th>time_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.667</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.700</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>16</td>\n",
       "      <td>4.312</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>15</td>\n",
       "      <td>2.667</td>\n",
       "      <td>5.933</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Char</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Char</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Char</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Char</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.125</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Char</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>40</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.375</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>1.333</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>2.900</td>\n",
       "      <td>5.300</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>2.556</td>\n",
       "      <td>7.519</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>1.280</td>\n",
       "      <td>3.840</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>40</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.275</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000</td>\n",
       "      <td>5.667</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>1.526</td>\n",
       "      <td>3.632</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>29</td>\n",
       "      <td>2.379</td>\n",
       "      <td>5.207</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>2.133</td>\n",
       "      <td>4.667</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "      <td>3.077</td>\n",
       "      <td>5.923</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1.308</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.440</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>23</td>\n",
       "      <td>3.000</td>\n",
       "      <td>4.261</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>1.280</td>\n",
       "      <td>2.040</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>0.976</td>\n",
       "      <td>2.683</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>14.500</td>\n",
       "      <td>14.500</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>69.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>1.455</td>\n",
       "      <td>1.636</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>0.421</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tokenizer                                               text  \\\n",
       "0                   Word                                       Hello world!   \n",
       "1                   Word                      State-of-the-art models work.   \n",
       "2                   Word  I am thrilled to learn gen AI and build my own...   \n",
       "3                   Word                   I love üçï and Œª-calculus in 2025!   \n",
       "4                   Word           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "5                   Char                                       Hello world!   \n",
       "6                   Char                      State-of-the-art models work.   \n",
       "7                   Char  I am thrilled to learn gen AI and build my own...   \n",
       "8                   Char                   I love üçï and Œª-calculus in 2025!   \n",
       "9                   Char           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "10      BPE (char-level)                                       Hello world!   \n",
       "11      BPE (char-level)                      State-of-the-art models work.   \n",
       "12      BPE (char-level)  I am thrilled to learn gen AI and build my own...   \n",
       "13      BPE (char-level)                   I love üçï and Œª-calculus in 2025!   \n",
       "14      BPE (char-level)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "15       WordPiece (toy)                                       Hello world!   \n",
       "16       WordPiece (toy)                      State-of-the-art models work.   \n",
       "17       WordPiece (toy)  I am thrilled to learn gen AI and build my own...   \n",
       "18       WordPiece (toy)                   I love üçï and Œª-calculus in 2025!   \n",
       "19       WordPiece (toy)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "20         Unigram (toy)                                       Hello world!   \n",
       "21         Unigram (toy)                      State-of-the-art models work.   \n",
       "22         Unigram (toy)  I am thrilled to learn gen AI and build my own...   \n",
       "23         Unigram (toy)                   I love üçï and Œª-calculus in 2025!   \n",
       "24         Unigram (toy)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "25  Byte-level BPE (toy)                                       Hello world!   \n",
       "26  Byte-level BPE (toy)                      State-of-the-art models work.   \n",
       "27  Byte-level BPE (toy)  I am thrilled to learn gen AI and build my own...   \n",
       "28  Byte-level BPE (toy)                   I love üçï and Œª-calculus in 2025!   \n",
       "29  Byte-level BPE (toy)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "\n",
       "    chars  bytes  tokens  chars/token  bytes/token  OOV  detokenizes_exact  \\\n",
       "0      12     12       3        4.000        3.667    0               True   \n",
       "1      29     29      10        2.900        2.700    0              False   \n",
       "2      69     69      16        4.312        3.500    0              False   \n",
       "3      32     36      10        3.200        3.000    0              False   \n",
       "4      40     95      15        2.667        5.933    0              False   \n",
       "5      12     12      12        1.000        1.000    0               True   \n",
       "6      29     29      29        1.000        1.000    0               True   \n",
       "7      69     69      69        1.000        1.000    0               True   \n",
       "8      32     36      32        1.000        1.125    0               True   \n",
       "9      40     95      40        1.000        2.375    0               True   \n",
       "10     12     12       9        1.333        2.000    0               True   \n",
       "11     29     29      10        2.900        5.300    0              False   \n",
       "12     69     69      27        2.556        7.519    0              False   \n",
       "13     32     36      25        1.280        3.840    0              False   \n",
       "14     40     95      40        1.000        3.275    0               True   \n",
       "15     12     12       3        4.000        5.667    2              False   \n",
       "16     29     29      19        1.526        3.632    0               True   \n",
       "17     69     69      29        2.379        5.207    0               True   \n",
       "18     32     36      15        2.133        4.667    2              False   \n",
       "19     40     95      13        3.077        5.923    7              False   \n",
       "20     12     12      13        0.923        1.308    0               True   \n",
       "21     29     29      25        1.160        1.440    0               True   \n",
       "22     69     69      23        3.000        4.261    0               True   \n",
       "23     32     36      25        1.280        2.040    0               True   \n",
       "24     40     95      41        0.976        2.683    0               True   \n",
       "25     12     12      10        1.200        1.200    0               True   \n",
       "26     29     29       2       14.500       14.500    0               True   \n",
       "27     69     69       1       69.000       69.000    0               True   \n",
       "28     32     36      22        1.455        1.636    0               True   \n",
       "29     40     95      95        0.421        1.000    0               True   \n",
       "\n",
       "    time_ms  \n",
       "0      0.01  \n",
       "1      0.01  \n",
       "2      0.01  \n",
       "3      0.01  \n",
       "4      7.11  \n",
       "5      0.00  \n",
       "6      0.00  \n",
       "7      0.00  \n",
       "8      0.00  \n",
       "9      0.00  \n",
       "10     0.04  \n",
       "11     0.12  \n",
       "12     0.19  \n",
       "13     0.07  \n",
       "14     0.05  \n",
       "15     0.02  \n",
       "16     0.04  \n",
       "17     0.02  \n",
       "18     0.02  \n",
       "19     0.02  \n",
       "20     0.04  \n",
       "21     0.06  \n",
       "22     0.16  \n",
       "23     0.07  \n",
       "24     0.04  \n",
       "25     0.04  \n",
       "26     0.24  \n",
       "27     0.89  \n",
       "28     0.19  \n",
       "29     0.05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>text</th>\n",
       "      <th>chars</th>\n",
       "      <th>bytes</th>\n",
       "      <th>tokens</th>\n",
       "      <th>chars/token</th>\n",
       "      <th>bytes/token</th>\n",
       "      <th>OOV</th>\n",
       "      <th>detokenizes_exact</th>\n",
       "      <th>time_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.667</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.700</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>16</td>\n",
       "      <td>4.312</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>15</td>\n",
       "      <td>2.667</td>\n",
       "      <td>5.933</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Char</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Char</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Char</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Char</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.125</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Char</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>40</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.375</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>1.333</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>2.900</td>\n",
       "      <td>5.300</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>2.556</td>\n",
       "      <td>7.519</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>1.280</td>\n",
       "      <td>3.840</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BPE (char-level)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>40</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.275</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000</td>\n",
       "      <td>5.667</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>1.526</td>\n",
       "      <td>3.632</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>29</td>\n",
       "      <td>2.379</td>\n",
       "      <td>5.207</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>2.133</td>\n",
       "      <td>4.667</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WordPiece (toy)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "      <td>3.077</td>\n",
       "      <td>5.923</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1.308</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.440</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>23</td>\n",
       "      <td>3.000</td>\n",
       "      <td>4.261</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>1.280</td>\n",
       "      <td>2.040</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Unigram (toy)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>0.976</td>\n",
       "      <td>2.683</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>Hello world!</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>State-of-the-art models work.</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>14.500</td>\n",
       "      <td>14.500</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>I am thrilled to learn gen AI and build my own...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>69.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>I love üçï and Œª-calculus in 2025!</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>1.455</td>\n",
       "      <td>1.636</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Byte-level BPE (toy)</td>\n",
       "      <td>‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>0.421</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tokenizer                                               text  \\\n",
       "0                   Word                                       Hello world!   \n",
       "1                   Word                      State-of-the-art models work.   \n",
       "2                   Word  I am thrilled to learn gen AI and build my own...   \n",
       "3                   Word                   I love üçï and Œª-calculus in 2025!   \n",
       "4                   Word           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "5                   Char                                       Hello world!   \n",
       "6                   Char                      State-of-the-art models work.   \n",
       "7                   Char  I am thrilled to learn gen AI and build my own...   \n",
       "8                   Char                   I love üçï and Œª-calculus in 2025!   \n",
       "9                   Char           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "10      BPE (char-level)                                       Hello world!   \n",
       "11      BPE (char-level)                      State-of-the-art models work.   \n",
       "12      BPE (char-level)  I am thrilled to learn gen AI and build my own...   \n",
       "13      BPE (char-level)                   I love üçï and Œª-calculus in 2025!   \n",
       "14      BPE (char-level)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "15       WordPiece (toy)                                       Hello world!   \n",
       "16       WordPiece (toy)                      State-of-the-art models work.   \n",
       "17       WordPiece (toy)  I am thrilled to learn gen AI and build my own...   \n",
       "18       WordPiece (toy)                   I love üçï and Œª-calculus in 2025!   \n",
       "19       WordPiece (toy)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "20         Unigram (toy)                                       Hello world!   \n",
       "21         Unigram (toy)                      State-of-the-art models work.   \n",
       "22         Unigram (toy)  I am thrilled to learn gen AI and build my own...   \n",
       "23         Unigram (toy)                   I love üçï and Œª-calculus in 2025!   \n",
       "24         Unigram (toy)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "25  Byte-level BPE (toy)                                       Hello world!   \n",
       "26  Byte-level BPE (toy)                      State-of-the-art models work.   \n",
       "27  Byte-level BPE (toy)  I am thrilled to learn gen AI and build my own...   \n",
       "28  Byte-level BPE (toy)                   I love üçï and Œª-calculus in 2025!   \n",
       "29  Byte-level BPE (toy)           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‚Äî ŒöŒ±ŒªŒ∑ŒºŒ≠œÅŒ± Œ∫œåœÉŒºŒµ ‚Äî „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå   \n",
       "\n",
       "    chars  bytes  tokens  chars/token  bytes/token  OOV  detokenizes_exact  \\\n",
       "0      12     12       3        4.000        3.667    0               True   \n",
       "1      29     29      10        2.900        2.700    0              False   \n",
       "2      69     69      16        4.312        3.500    0              False   \n",
       "3      32     36      10        3.200        3.000    0              False   \n",
       "4      40     95      15        2.667        5.933    0              False   \n",
       "5      12     12      12        1.000        1.000    0               True   \n",
       "6      29     29      29        1.000        1.000    0               True   \n",
       "7      69     69      69        1.000        1.000    0               True   \n",
       "8      32     36      32        1.000        1.125    0               True   \n",
       "9      40     95      40        1.000        2.375    0               True   \n",
       "10     12     12       9        1.333        2.000    0               True   \n",
       "11     29     29      10        2.900        5.300    0              False   \n",
       "12     69     69      27        2.556        7.519    0              False   \n",
       "13     32     36      25        1.280        3.840    0              False   \n",
       "14     40     95      40        1.000        3.275    0               True   \n",
       "15     12     12       3        4.000        5.667    2              False   \n",
       "16     29     29      19        1.526        3.632    0               True   \n",
       "17     69     69      29        2.379        5.207    0               True   \n",
       "18     32     36      15        2.133        4.667    2              False   \n",
       "19     40     95      13        3.077        5.923    7              False   \n",
       "20     12     12      13        0.923        1.308    0               True   \n",
       "21     29     29      25        1.160        1.440    0               True   \n",
       "22     69     69      23        3.000        4.261    0               True   \n",
       "23     32     36      25        1.280        2.040    0               True   \n",
       "24     40     95      41        0.976        2.683    0               True   \n",
       "25     12     12      10        1.200        1.200    0               True   \n",
       "26     29     29       2       14.500       14.500    0               True   \n",
       "27     69     69       1       69.000       69.000    0               True   \n",
       "28     32     36      22        1.455        1.636    0               True   \n",
       "29     40     95      95        0.421        1.000    0               True   \n",
       "\n",
       "    time_ms  \n",
       "0      0.01  \n",
       "1      0.01  \n",
       "2      0.01  \n",
       "3      0.01  \n",
       "4      7.11  \n",
       "5      0.00  \n",
       "6      0.00  \n",
       "7      0.00  \n",
       "8      0.00  \n",
       "9      0.00  \n",
       "10     0.04  \n",
       "11     0.12  \n",
       "12     0.19  \n",
       "13     0.07  \n",
       "14     0.05  \n",
       "15     0.02  \n",
       "16     0.04  \n",
       "17     0.02  \n",
       "18     0.02  \n",
       "19     0.02  \n",
       "20     0.04  \n",
       "21     0.06  \n",
       "22     0.16  \n",
       "23     0.07  \n",
       "24     0.04  \n",
       "25     0.04  \n",
       "26     0.24  \n",
       "27     0.89  \n",
       "28     0.19  \n",
       "29     0.05  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_tokenizer(name: str, tok_obj, sentences: List[str], is_byte_level=False):\n",
    "    rows, samples = [], {}\n",
    "    for s in sentences:\n",
    "        t0 = time.perf_counter()\n",
    "        toks = tok_obj.tokenize(s)\n",
    "        dt = (time.perf_counter() - t0) * 1000.0\n",
    "        oov = sum(1 for t in toks if isinstance(t, str) and t == \"[UNK]\") if isinstance(tok_obj, WordPieceTokenizer) else 0\n",
    "        detok_fn = getattr(tok_obj, \"detokenize\", None)\n",
    "        m = metrics_from_tokens(s, toks, detokenize_fn=detok_fn, oov_count=oov)\n",
    "        m.update({\"tokenizer\": name, \"text\": s, \"time_ms\": round(dt, 2)})\n",
    "        rows.append(m)\n",
    "        samples[s] = [pretty_byte_token(t) for t in toks[:30]] if is_byte_level else toks[:30]\n",
    "    df = pd.DataFrame(rows)[[\"tokenizer\",\"text\",\"chars\",\"bytes\",\"tokens\",\"chars/token\",\"bytes/token\",\"OOV\",\"detokenizes_exact\",\"time_ms\"]]\n",
    "    return df, samples\n",
    "\n",
    "dfs, samples_all = [], {}\n",
    "df_w, samp_w = evaluate_tokenizer(\"Word\", word_tok, sentences_eval)\n",
    "dfs.append(df_w); samples_all[\"Word\"] = samp_w\n",
    "df_c, samp_c = evaluate_tokenizer(\"Char\", char_tok, sentences_eval)\n",
    "dfs.append(df_c); samples_all[\"Char\"] = samp_c\n",
    "df_bpe, samp_bpe = evaluate_tokenizer(\"BPE (char-level)\", bpe_tok, sentences_eval)\n",
    "dfs.append(df_bpe); samples_all[\"BPE\"] = samp_bpe\n",
    "df_wp, samp_wp = evaluate_tokenizer(\"WordPiece (toy)\", wp_tok, sentences_eval)\n",
    "dfs.append(df_wp); samples_all[\"WordPiece\"] = samp_wp\n",
    "df_uni, samp_uni = evaluate_tokenizer(\"Unigram (toy)\", uni_tok, sentences_eval)\n",
    "dfs.append(df_uni); samples_all[\"Unigram\"] = samp_uni\n",
    "df_bbpe, samp_bbpe = evaluate_tokenizer(\"Byte-level BPE (toy)\", byte_bpe_tok, sentences_eval, is_byte_level=True)\n",
    "dfs.append(df_bbpe); samples_all[\"Byte-BPE\"] = samp_bbpe\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "display(df_all)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815126c6",
   "metadata": {},
   "source": [
    "## Sample Tokenizations for the Fixed Sentence\n",
    "We print the first ~30 tokens from each tokenizer for:\n",
    "\n",
    "> `I am thrilled to learn gen AI and build my own applications in @2025*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7dbbfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am thrilled to learn gen AI and build my own applications in @2025* \n",
      "\n",
      "[Word] First ~30 tokens:\n",
      "['I', 'am', 'thrilled', 'to', 'learn', 'gen', 'AI', 'and', 'build', 'my', 'own', 'applications', 'in', '@', '2025', '*']\n",
      "\n",
      "[Char] First ~30 tokens:\n",
      "['I', ' ', 'a', 'm', ' ', 't', 'h', 'r', 'i', 'l', 'l', 'e', 'd', ' ', 't', 'o', ' ', 'l', 'e', 'a', 'r', 'n', ' ', 'g', 'e', 'n', ' ', 'A', 'I', ' ']\n",
      "\n",
      "[BPE] First ~30 tokens:\n",
      "['I</w>', '<space>', 'am</w>', '<space>', 'thrilled</w>', '<space>', 'to</w>', '<space>', 'learn</w>', '<space>', 'gen</w>', '<space>', 'AI</w>', '<space>', 'and</w>', '<space>', 'build</w>', '<space>', 'my</w>', '<space>', 'own</w>', '<space>', 'applications</w>', '<space>', 'in</w>', '<space>', '@2025*</w>']\n",
      "\n",
      "[WordPiece] First ~30 tokens:\n",
      "['I', '<space>', 'am', '<space>', 'thrill', '##ed', '<space>', 'to', '<space>', 'learn', '<space>', 'gen', '<space>', 'AI', '<space>', 'and', '<space>', 'build', '<space>', 'my', '<space>', 'own', '<space>', 'applic', '##ations', '<space>', 'in', '<space>', '@2025*']\n",
      "\n",
      "[Unigram] First ~30 tokens:\n",
      "['‚ñÅI', '‚ñÅam', '‚ñÅ', 'thrilled', '‚ñÅto', '‚ñÅlearn', '‚ñÅgen', '‚ñÅAI', '‚ñÅand', '‚ñÅbuild', '‚ñÅmy', '‚ñÅown', '‚ñÅapplic', 'ation', 's', '‚ñÅin', '‚ñÅ', '@', '2', '0', '2', '5', '*']\n",
      "\n",
      "[Byte-BPE] First ~30 tokens:\n",
      "['I am thrilled to learn gen AI and build my own applications in @2025*']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence:\", SENT_FIXED, \"\\n\")\n",
    "for name, samples in samples_all.items():\n",
    "    tok_list = samples[SENT_FIXED]\n",
    "    print(f\"[{name}] First ~30 tokens:\")\n",
    "    print(tok_list)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390f3bb",
   "metadata": {},
   "source": [
    "## Approximate Vocabulary Sizes\n",
    "Rough estimates for the toy implementations (Byte/char-based add ~256 base tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9918d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>approx_vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Char</td>\n",
       "      <td>1114112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPE</td>\n",
       "      <td>456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WordPiece</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unigram</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Byte-BPE</td>\n",
       "      <td>529.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenizer  approx_vocab_size\n",
       "0       Word                NaN\n",
       "1       Char          1114112.0\n",
       "2        BPE              456.0\n",
       "3  WordPiece              300.0\n",
       "4    Unigram              300.0\n",
       "5   Byte-BPE              529.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>approx_vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Char</td>\n",
       "      <td>1114112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPE</td>\n",
       "      <td>456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WordPiece</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unigram</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Byte-BPE</td>\n",
       "      <td>529.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenizer  approx_vocab_size\n",
       "0       Word                NaN\n",
       "1       Char          1114112.0\n",
       "2        BPE              456.0\n",
       "3  WordPiece              300.0\n",
       "4    Unigram              300.0\n",
       "5   Byte-BPE              529.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def est_vocab_size(tok_obj):\n",
    "    if isinstance(tok_obj, WordPieceTokenizer):\n",
    "        return len(tok_obj.vocab)\n",
    "    if isinstance(tok_obj, UnigramTokenizer):\n",
    "        return len(tok_obj.pieces)\n",
    "    if isinstance(tok_obj, BPETokenizer):\n",
    "        return len(tok_obj.ranks) + 256\n",
    "    if isinstance(tok_obj, ByteLevelBPETokenizer):\n",
    "        return len(tok_obj.ranks) + 256\n",
    "    if isinstance(tok_obj, WordTokenizer):\n",
    "        return np.nan\n",
    "    if isinstance(tok_obj, CharTokenizer):\n",
    "        return 1114112  # theoretical upper bound of Unicode scalar values\n",
    "    return np.nan\n",
    "\n",
    "vocab_rows = []\n",
    "for name, obj in [(\"Word\", word_tok), (\"Char\", char_tok), (\"BPE\", bpe_tok),\n",
    "                  (\"WordPiece\", wp_tok), (\"Unigram\", uni_tok), (\"Byte-BPE\", byte_bpe_tok)]:\n",
    "    vocab_rows.append({\"tokenizer\": name, \"approx_vocab_size\": est_vocab_size(obj)})\n",
    "df_vocab = pd.DataFrame(vocab_rows)\n",
    "display(df_vocab)\n",
    "df_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
