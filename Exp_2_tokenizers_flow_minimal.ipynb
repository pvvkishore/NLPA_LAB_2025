{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc4fe70",
   "metadata": {},
   "source": [
    "# Word, Character, and BPE Tokenizers — A Minimal, Flow‑Based Walkthrough\n",
    "\n",
    "Goal: **see each step** and keep code simple. We'll start with a tiny 5‑word corpus, then:\n",
    "1) **Word‑level** tokenization (trivial split)\n",
    "2) **Character‑level** tokenization (with `</w>` markers)\n",
    "3) **BPE training** step‑by‑step (count pairs → merge top pair), one cell per step\n",
    "4) **Use learned merges** to tokenize new words (greedy, minimal code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11173136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: ['low', 'lower', 'lowest', 'newest', 'widest']\n",
      "Test words: ['lowest', 'new', 'widen', 'lower', 'slow']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Tiny 5-word corpus\n",
    "corpus = [\"low\", \"lower\", \"lowest\", \"newest\", \"widest\"]\n",
    "print(\"Corpus:\", corpus)\n",
    "\n",
    "# We'll also keep a small list of test words for later\n",
    "test_words = [\"lowest\", \"new\", \"widen\", \"lower\", \"slow\"]\n",
    "print(\"Test words:\", test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0df79",
   "metadata": {},
   "source": [
    "## 1) Word‑level Tokenization (simple split)\n",
    "For our tiny corpus, this is just the list of words. For a sentence, we can do `sentence.split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f10194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: lowest new wid est\n",
      "Word tokens: ['lowest', 'new', 'wid', 'est']\n",
      "# of tokens: 4\n"
     ]
    }
   ],
   "source": [
    "sentence = \"lowest new wid est\"\n",
    "word_tokens = sentence.split()\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Word tokens:\", word_tokens)\n",
    "print(\"# of tokens:\", len(word_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f88611",
   "metadata": {},
   "source": [
    "## 2) Character‑level Tokenization (with `</w>` word end markers)\n",
    "We convert each word to a list of characters and append `</w>` so BPE knows word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62ea901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial char-level vocab with </w> markers:\n",
      "  ('l', 'o', 'w', '</w>') x 1\n",
      "  ('l', 'o', 'w', 'e', 'r', '</w>') x 1\n",
      "  ('l', 'o', 'w', 'e', 's', 't', '</w>') x 1\n",
      "  ('n', 'e', 'w', 'e', 's', 't', '</w>') x 1\n",
      "  ('w', 'i', 'd', 'e', 's', 't', '</w>') x 1\n",
      "\n",
      "As a table (for readability):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_symbols</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l o w &lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l o w e r &lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l o w e s t &lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n e w e s t &lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w i d e s t &lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_symbols  count\n",
       "0        l o w </w>      1\n",
       "1    l o w e r </w>      1\n",
       "2  l o w e s t </w>      1\n",
       "3  n e w e s t </w>      1\n",
       "4  w i d e s t </w>      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the initial symbol vocabulary as a dict: tuple(symbols) -> count\n",
    "vocab = {}\n",
    "for w in corpus:\n",
    "    syms = tuple(list(w) + [\"</w>\"])\n",
    "    vocab[syms] = vocab.get(syms, 0) + 1\n",
    "\n",
    "print(\"Initial char-level vocab with </w> markers:\")\n",
    "for syms, cnt in vocab.items():\n",
    "    print(\" \", syms, \"x\", cnt)\n",
    "\n",
    "print(\"\\nAs a table (for readability):\")\n",
    "df_init = pd.DataFrame({\n",
    "    \"word_symbols\": [\" \".join(s) for s in vocab.keys()],\n",
    "    \"count\": list(vocab.values())\n",
    "}).sort_values(by=[\"count\",\"word_symbols\"], ascending=[False, True]).reset_index(drop=True)\n",
    "display(df_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb6bf2",
   "metadata": {},
   "source": [
    "## 3) BPE Training — **One merge per run** (repeat this cell)\n",
    "Each time you run this cell:\n",
    "1. Count frequencies of adjacent symbol pairs across the current `vocab`\n",
    "2. Pick the **most frequent** pair\n",
    "3. **Merge** it in every word (replace the pair with a single new symbol)\n",
    "4. Append the merge to `merges` and print the updated state\n",
    "\n",
    "> **Tip:** Run this cell multiple times (5–10) to watch merges evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fadc91c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BPE Step 12 ===\n",
      "Top pairs:\n",
      "  (low est</w>) : 1\n",
      "  (n e) : 1\n",
      "  (e w) : 1\n",
      "  (w est</w>) : 1\n",
      "  (w i) : 1\n",
      "  (i d) : 1\n",
      "  (d est</w>) : 1\n",
      "Merging most frequent pair: (low est</w>) with freq = 1\n",
      "Updated merges: [('l', 'o'), ('lo', 'w'), ('l', 'o'), ('lo', 'w'), ('e', 's'), ('es', 't'), ('est', '</w>'), ('low', '</w>'), ('low', 'e'), ('lowe', 'r'), ('lower', '</w>'), ('low', 'est</w>')]\n",
      "Updated vocab:\n",
      "  ('low</w>',) x 1\n",
      "  ('lower</w>',) x 1\n",
      "  ('lowest</w>',) x 1\n",
      "  ('n', 'e', 'w', 'est</w>') x 1\n",
      "  ('w', 'i', 'd', 'est</w>') x 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_symbols</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low&lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lower&lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lowest&lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n e w est&lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w i d est&lt;/w&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word_symbols  count\n",
       "0        low</w>      1\n",
       "1      lower</w>      1\n",
       "2     lowest</w>      1\n",
       "3  n e w est</w>      1\n",
       "4  w i d est</w>      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep state across runs of this cell\n",
    "try:\n",
    "    merges\n",
    "    step\n",
    "except NameError:\n",
    "    merges = []\n",
    "    step = 0\n",
    "\n",
    "step += 1\n",
    "print(f\"\\n=== BPE Step {step} ===\")\n",
    "\n",
    "# 3.1 Count adjacent pairs\n",
    "pairs = Counter()\n",
    "for syms, freq in vocab.items():\n",
    "    for a, b in zip(syms, syms[1:]):\n",
    "        pairs[(a, b)] += freq\n",
    "\n",
    "top = pairs.most_common(10)\n",
    "print(\"Top pairs:\")\n",
    "for (a,b), f in top:\n",
    "    print(f\"  ({a} {b}) : {f}\")\n",
    "\n",
    "if not pairs:\n",
    "    print(\"No more pairs to merge.\")\n",
    "else:\n",
    "    # 3.2 Choose best pair\n",
    "    (pa, pb), best_freq = pairs.most_common(1)[0]\n",
    "    print(f\"Merging most frequent pair: ({pa} {pb}) with freq = {best_freq}\")\n",
    "\n",
    "    # 3.3 Merge across vocab (replace (pa,pb) with pa+pb)\n",
    "    merged_token = pa + pb\n",
    "    new_vocab = {}\n",
    "    for syms, freq in vocab.items():\n",
    "        new_syms = []\n",
    "        i = 0\n",
    "        while i < len(syms):\n",
    "            if i < len(syms)-1 and syms[i] == pa and syms[i+1] == pb:\n",
    "                new_syms.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_syms.append(syms[i])\n",
    "                i += 1\n",
    "        new_vocab[tuple(new_syms)] = new_vocab.get(tuple(new_syms), 0) + freq\n",
    "\n",
    "    vocab = new_vocab\n",
    "    merges.append((pa, pb))\n",
    "    print(\"Updated merges:\", merges)\n",
    "    print(\"Updated vocab:\")\n",
    "    for syms, cnt in vocab.items():\n",
    "        print(\" \", syms, \"x\", cnt)\n",
    "\n",
    "    # Show as a small table\n",
    "    df_now = pd.DataFrame({\n",
    "        \"word_symbols\": [\" \".join(s) for s in vocab.keys()],\n",
    "        \"count\": list(vocab.values())\n",
    "    })\n",
    "    display(df_now.sort_values(by=[\"count\",\"word_symbols\"], ascending=[False,True]).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a8c81",
   "metadata": {},
   "source": [
    "## 4) Use Learned Merges to Tokenize New Words (greedy, minimal code)\n",
    "We turn the `merges` list into a rank map (earlier merges have higher priority). Then for each new word:\n",
    "- Start with `list(chars) + ['</w>']`\n",
    "- Repeatedly merge the **best-ranked adjacent pair** present\n",
    "- Stop when no learned pair is applicable; strip `</w>` at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a65503e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lowest -> ['lowest</w>']\n",
      "     new -> ['n', 'e', 'w']\n",
      "   widen -> ['w', 'i', 'd', 'e', 'n']\n",
      "   lower -> ['lower</w>']\n",
      "    slow -> ['s', 'low</w>']\n",
      "\n",
      "Comparison on: lowest\n",
      "Word-level tokens (split by space): ['lowest']\n",
      "# tokens (word-level): 1\n",
      "Char-level tokens: ['l', 'o', 'w', 'e', 's', 't']\n",
      "# tokens (char-level): 6\n",
      "BPE tokens: ['lowest</w>']\n",
      "# tokens (BPE): 1\n"
     ]
    }
   ],
   "source": [
    "if len(merges) == 0:\n",
    "    print(\"You haven't learned any merges yet. Re-run the previous BPE step cell a few times.\")\n",
    "else:\n",
    "    # Build pair->rank dict: lower rank = earlier (higher priority)\n",
    "    ranks = {m:i for i, m in enumerate(merges)}\n",
    "\n",
    "    def tokenize_with_merges(word):\n",
    "        syms = list(word) + [\"</w>\"]\n",
    "        # repeatedly merge the best-ranked pair that exists\n",
    "        while True:\n",
    "            # list all adjacent pairs\n",
    "            cand = [((syms[i], syms[i+1]), i) for i in range(len(syms)-1)]\n",
    "            if not cand:\n",
    "                break\n",
    "            # find the best-ranked pair present\n",
    "            best = None\n",
    "            best_rank = 10**9\n",
    "            best_idx = -1\n",
    "            for (a,b), idx in cand:\n",
    "                r = ranks.get((a,b), 10**9)\n",
    "                if r < best_rank:\n",
    "                    best = (a,b); best_rank = r; best_idx = idx\n",
    "            if best is None or best_rank == 10**9:\n",
    "                break\n",
    "            a,b = best\n",
    "            syms = syms[:best_idx] + [a+b] + syms[best_idx+2:]\n",
    "        if syms and syms[-1] == \"</w>\": syms = syms[:-1]\n",
    "        return syms\n",
    "\n",
    "    for w in test_words:\n",
    "        toks = tokenize_with_merges(w)\n",
    "        print(f\"{w:>8} ->\", toks)\n",
    "\n",
    "    # Compare counts vs. word/char tokenization on one example\n",
    "    demo = \"lowest\"\n",
    "    print(\"\\nComparison on:\", demo)\n",
    "    print(\"Word-level tokens (split by space):\", demo.split())\n",
    "    print(\"# tokens (word-level):\", len(demo.split()))\n",
    "    print(\"Char-level tokens:\", list(demo))\n",
    "    print(\"# tokens (char-level):\", len(list(demo)))\n",
    "    print(\"BPE tokens:\", tokenize_with_merges(demo))\n",
    "    print(\"# tokens (BPE):\", len(tokenize_with_merges(demo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4151cc9-cacf-4102-a568-d705080febd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
