{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84487be3-46b8-4bbb-8bae-9f8dc7103fe3",
   "metadata": {},
   "source": [
    "# Use all the above text vectorization models for a text classification application and give a conclusive report on which vectors are best sutited for the building the application. Use comprehensive performance metrics to compare the text vectorization models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f96433-e6cd-44d6-8ce9-407850cc10a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEXT VECTORIZATION COMPARISON FOR CLASSIFICATION\n",
      "Comparing Bag of Words vs TF-IDF\n",
      "================================================================================\n",
      "================================================================================\n",
      "LOADING DATASET\n",
      "================================================================================\n",
      "Training samples: 2854\n",
      "Test samples: 1899\n",
      "Number of categories: 5\n",
      "Categories: alt.atheism, comp.graphics, rec.sport.baseball, sci.med, soc.religion.christian\n",
      "\n",
      "Average document length: 186.65 words\n",
      "Min document length: 0 words\n",
      "Max document length: 9109 words\n",
      "\n",
      "================================================================================\n",
      "CREATING VECTORIZERS\n",
      "================================================================================\n",
      "Created 7 vectorizer configurations\n",
      "  - BOW_basic\n",
      "  - BOW_ngram\n",
      "  - BOW_char\n",
      "  - TFIDF_basic\n",
      "  - TFIDF_ngram\n",
      "  - TFIDF_char\n",
      "  - TFIDF_sublinear\n",
      "\n",
      "================================================================================\n",
      "RUNNING COMPREHENSIVE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Processing BOW_basic...\n",
      "----------------------------------------\n",
      "  Vectorizing with BOW_basic...\n",
      "    Shape: (2854, 5000)\n",
      "    Sparsity: 99.07%\n",
      "    Vectorization time: 0.59s\n",
      "    Testing with Naive Bayes...\n",
      "      Accuracy: 0.8125\n",
      "      F1-Score: 0.8136\n",
      "    Testing with Logistic Regression...\n",
      "      Accuracy: 0.7815\n",
      "      F1-Score: 0.7786\n",
      "    Testing with Linear SVM...\n",
      "      Accuracy: 0.7483\n",
      "      F1-Score: 0.7456\n",
      "    Testing with Random Forest...\n",
      "      Accuracy: 0.7220\n",
      "      F1-Score: 0.7061\n",
      "\n",
      "Processing BOW_ngram...\n",
      "----------------------------------------\n",
      "  Vectorizing with BOW_ngram...\n",
      "    Shape: (2854, 5000)\n",
      "    Sparsity: 99.04%\n",
      "    Vectorization time: 1.34s\n",
      "    Testing with Naive Bayes...\n",
      "      Accuracy: 0.8094\n",
      "      F1-Score: 0.8105\n",
      "    Testing with Logistic Regression...\n",
      "      Accuracy: 0.7741\n",
      "      F1-Score: 0.7713\n",
      "    Testing with Linear SVM...\n",
      "      Accuracy: 0.7509\n",
      "      F1-Score: 0.7477\n",
      "    Testing with Random Forest...\n",
      "      Accuracy: 0.7251\n",
      "      F1-Score: 0.7086\n",
      "\n",
      "Processing BOW_char...\n",
      "----------------------------------------\n",
      "  Vectorizing with BOW_char...\n",
      "    Shape: (2854, 5000)\n",
      "    Sparsity: 85.48%\n",
      "    Vectorization time: 7.59s\n",
      "    Testing with Naive Bayes...\n",
      "      Accuracy: 0.7267\n",
      "      F1-Score: 0.7313\n",
      "    Testing with Logistic Regression...\n",
      "      Accuracy: 0.7335\n",
      "      F1-Score: 0.7319\n",
      "    Testing with Linear SVM...\n",
      "      Accuracy: 0.7209\n",
      "      F1-Score: 0.7187\n",
      "    Testing with Random Forest...\n",
      "      Accuracy: 0.6898\n",
      "      F1-Score: 0.6765\n",
      "\n",
      "Processing TFIDF_basic...\n",
      "----------------------------------------\n",
      "  Vectorizing with TFIDF_basic...\n",
      "    Shape: (2854, 5000)\n",
      "    Sparsity: 99.07%\n",
      "    Vectorization time: 0.59s\n",
      "    Testing with Naive Bayes...\n",
      "      Accuracy: 0.8215\n",
      "      F1-Score: 0.8196\n",
      "    Testing with Logistic Regression...\n",
      "      Accuracy: 0.8183\n",
      "      F1-Score: 0.8132\n",
      "    Testing with Linear SVM...\n",
      "      Accuracy: 0.8199\n",
      "      F1-Score: 0.8159\n",
      "    Testing with Random Forest...\n",
      "      Accuracy: 0.7262\n",
      "      F1-Score: 0.7095\n",
      "\n",
      "Processing TFIDF_ngram...\n",
      "----------------------------------------\n",
      "  Vectorizing with TFIDF_ngram...\n",
      "    Shape: (2854, 5000)\n",
      "    Sparsity: 99.04%\n",
      "    Vectorization time: 1.42s\n",
      "    Testing with Naive Bayes...\n",
      "      Accuracy: 0.8162\n",
      "      F1-Score: 0.8146\n",
      "    Testing with Logistic Regression...\n",
      "      Accuracy: 0.8199\n",
      "      F1-Score: 0.8150\n",
      "    Testing with Linear SVM...\n",
      "      Accuracy: 0.8178\n",
      "      F1-Score: 0.8139\n",
      "    Testing with Random Forest...\n",
      "      Accuracy: 0.7267\n",
      "      F1-Score: 0.7113\n",
      "\n",
      "Processing TFIDF_char...\n",
      "----------------------------------------\n",
      "  Vectorizing with TFIDF_char...\n",
      "    Shape: (2854, 5000)\n",
      "    Sparsity: 85.48%\n",
      "    Vectorization time: 7.13s\n",
      "    Testing with Naive Bayes...\n",
      "      Accuracy: 0.7667\n",
      "      F1-Score: 0.7632\n",
      "    Testing with Logistic Regression...\n",
      "      Accuracy: 0.7688\n",
      "      F1-Score: 0.7658\n",
      "    Testing with Linear SVM...\n",
      "      Accuracy: 0.7920\n",
      "      F1-Score: 0.7898\n",
      "    Testing with Random Forest...\n",
      "      Accuracy: 0.7088\n",
      "      F1-Score: 0.6966\n",
      "\n",
      "Processing TFIDF_sublinear...\n",
      "----------------------------------------\n",
      "  Vectorizing with TFIDF_sublinear...\n",
      "    Shape: (2854, 5000)\n",
      "    Sparsity: 99.07%\n",
      "    Vectorization time: 0.59s\n",
      "    Testing with Naive Bayes...\n",
      "      Accuracy: 0.8215\n",
      "      F1-Score: 0.8193\n",
      "    Testing with Logistic Regression...\n",
      "      Accuracy: 0.8204\n",
      "      F1-Score: 0.8150\n",
      "    Testing with Linear SVM...\n",
      "      Accuracy: 0.8194\n",
      "      F1-Score: 0.8156\n",
      "    Testing with Random Forest...\n",
      "      Accuracy: 0.7283\n",
      "      F1-Score: 0.7122\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '\\home\\claude'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 571\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 571\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 542\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    539\u001b[0m results_df \u001b[38;5;241m=\u001b[39m comparison\u001b[38;5;241m.\u001b[39mcreate_summary_dataframe()\n\u001b[0;32m    541\u001b[0m \u001b[38;5;66;03m# Save detailed results to CSV\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/claude/vectorization_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDetailed results saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorization_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# Create visualizations\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3903\u001b[0m     path_or_buf,\n\u001b[0;32m   3904\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3905\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3906\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3907\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3908\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3909\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3910\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3911\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3912\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3913\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3914\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3915\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3916\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3917\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3918\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3919\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    251\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    252\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    253\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 739\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '\\home\\claude'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Text Vectorization Comparison for Classification\n",
    "Comparing Bag of Words (BOW) and TF-IDF for text classification tasks\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import hstack\n",
    "import gc\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class TextVectorizationComparison:\n",
    "    \"\"\"\n",
    "    A comprehensive comparison framework for text vectorization methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.results = {}\n",
    "        self.vectorizers = {}\n",
    "        self.classifiers = {}\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load the 20 newsgroups dataset for classification\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"LOADING DATASET\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Load a subset of categories for faster processing\n",
    "        categories = ['alt.atheism', 'soc.religion.christian', \n",
    "                     'comp.graphics', 'sci.med', 'rec.sport.baseball']\n",
    "        \n",
    "        newsgroups_train = fetch_20newsgroups(\n",
    "            subset='train', \n",
    "            categories=categories,\n",
    "            remove=('headers', 'footers', 'quotes'),\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        newsgroups_test = fetch_20newsgroups(\n",
    "            subset='test',\n",
    "            categories=categories,\n",
    "            remove=('headers', 'footers', 'quotes'),\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.X_train_raw = newsgroups_train.data\n",
    "        self.X_test_raw = newsgroups_test.data\n",
    "        self.y_train = newsgroups_train.target\n",
    "        self.y_test = newsgroups_test.target\n",
    "        self.target_names = newsgroups_train.target_names\n",
    "        \n",
    "        print(f\"Training samples: {len(self.X_train_raw)}\")\n",
    "        print(f\"Test samples: {len(self.X_test_raw)}\")\n",
    "        print(f\"Number of categories: {len(self.target_names)}\")\n",
    "        print(f\"Categories: {', '.join(self.target_names)}\")\n",
    "        print()\n",
    "        \n",
    "        # Calculate basic statistics\n",
    "        train_lengths = [len(doc.split()) for doc in self.X_train_raw]\n",
    "        print(f\"Average document length: {np.mean(train_lengths):.2f} words\")\n",
    "        print(f\"Min document length: {np.min(train_lengths)} words\")\n",
    "        print(f\"Max document length: {np.max(train_lengths)} words\")\n",
    "        print()\n",
    "        \n",
    "    def create_vectorizers(self):\n",
    "        \"\"\"Create different vectorization configurations\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"CREATING VECTORIZERS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Bag of Words configurations\n",
    "        self.vectorizers['BOW_basic'] = CountVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            lowercase=True\n",
    "        )\n",
    "        \n",
    "        self.vectorizers['BOW_ngram'] = CountVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            lowercase=True\n",
    "        )\n",
    "        \n",
    "        self.vectorizers['BOW_char'] = CountVectorizer(\n",
    "            max_features=5000,\n",
    "            analyzer='char_wb',\n",
    "            ngram_range=(2, 4),\n",
    "            lowercase=True\n",
    "        )\n",
    "        \n",
    "        # TF-IDF configurations\n",
    "        self.vectorizers['TFIDF_basic'] = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            use_idf=True,\n",
    "            smooth_idf=True\n",
    "        )\n",
    "        \n",
    "        self.vectorizers['TFIDF_ngram'] = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            use_idf=True,\n",
    "            smooth_idf=True\n",
    "        )\n",
    "        \n",
    "        self.vectorizers['TFIDF_char'] = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            analyzer='char_wb',\n",
    "            ngram_range=(2, 4),\n",
    "            lowercase=True,\n",
    "            use_idf=True\n",
    "        )\n",
    "        \n",
    "        self.vectorizers['TFIDF_sublinear'] = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            use_idf=True,\n",
    "            smooth_idf=True,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(self.vectorizers)} vectorizer configurations\")\n",
    "        for name in self.vectorizers.keys():\n",
    "            print(f\"  - {name}\")\n",
    "        print()\n",
    "        \n",
    "    def create_classifiers(self):\n",
    "        \"\"\"Create multiple classifiers for comparison\"\"\"\n",
    "        self.classifiers = {\n",
    "            'Naive Bayes': MultinomialNB(alpha=0.1),\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=self.random_state),\n",
    "            'Linear SVM': LinearSVC(max_iter=2000, random_state=self.random_state),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=20, \n",
    "                                                   random_state=self.random_state, n_jobs=-1)\n",
    "        }\n",
    "        \n",
    "    def vectorize_data(self, vectorizer_name, vectorizer):\n",
    "        \"\"\"Transform text data using specified vectorizer\"\"\"\n",
    "        print(f\"  Vectorizing with {vectorizer_name}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        X_train = vectorizer.fit_transform(self.X_train_raw)\n",
    "        X_test = vectorizer.transform(self.X_test_raw)\n",
    "        vectorization_time = time.time() - start_time\n",
    "        \n",
    "        # Get feature information\n",
    "        if hasattr(vectorizer, 'get_feature_names_out'):\n",
    "            n_features = len(vectorizer.get_feature_names_out())\n",
    "        else:\n",
    "            n_features = X_train.shape[1]\n",
    "        \n",
    "        print(f\"    Shape: {X_train.shape}\")\n",
    "        print(f\"    Sparsity: {100.0 * (1 - X_train.nnz / (X_train.shape[0] * X_train.shape[1])):.2f}%\")\n",
    "        print(f\"    Vectorization time: {vectorization_time:.2f}s\")\n",
    "        \n",
    "        return X_train, X_test, vectorization_time, n_features\n",
    "        \n",
    "    def evaluate_classifier(self, clf_name, clf, X_train, X_test):\n",
    "        \"\"\"Train and evaluate a classifier\"\"\"\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        clf.fit(X_train, self.y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Predict\n",
    "        start_time = time.time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        pred_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "            'f1': f1_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "            'cohen_kappa': cohen_kappa_score(self.y_test, y_pred),\n",
    "            'matthews_corr': matthews_corrcoef(self.y_test, y_pred),\n",
    "            'train_time': train_time,\n",
    "            'pred_time': pred_time\n",
    "        }\n",
    "        \n",
    "        # For probabilistic classifiers, calculate AUC\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_proba = clf.predict_proba(X_test)\n",
    "            metrics['roc_auc'] = roc_auc_score(self.y_test, y_proba, \n",
    "                                              multi_class='ovr', average='weighted')\n",
    "        else:\n",
    "            metrics['roc_auc'] = None\n",
    "            \n",
    "        return metrics, y_pred\n",
    "        \n",
    "    def run_comparison(self):\n",
    "        \"\"\"Run the complete comparison\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"RUNNING COMPREHENSIVE COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        self.create_classifiers()\n",
    "        detailed_results = {}\n",
    "        \n",
    "        for vec_name, vectorizer in self.vectorizers.items():\n",
    "            print(f\"\\nProcessing {vec_name}...\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Vectorize data\n",
    "            X_train, X_test, vec_time, n_features = self.vectorize_data(vec_name, vectorizer)\n",
    "            \n",
    "            vec_type = 'BOW' if 'BOW' in vec_name else 'TF-IDF'\n",
    "            \n",
    "            # Test with each classifier\n",
    "            for clf_name, clf in self.classifiers.items():\n",
    "                print(f\"    Testing with {clf_name}...\")\n",
    "                \n",
    "                # Clone classifier to avoid state issues\n",
    "                from sklearn.base import clone\n",
    "                clf_clone = clone(clf)\n",
    "                \n",
    "                metrics, y_pred = self.evaluate_classifier(clf_name, clf_clone, X_train, X_test)\n",
    "                \n",
    "                # Store results\n",
    "                key = f\"{vec_name}_{clf_name}\"\n",
    "                detailed_results[key] = {\n",
    "                    'vectorizer': vec_name,\n",
    "                    'vec_type': vec_type,\n",
    "                    'classifier': clf_name,\n",
    "                    'n_features': n_features,\n",
    "                    'vec_time': vec_time,\n",
    "                    **metrics\n",
    "                }\n",
    "                \n",
    "                print(f\"      Accuracy: {metrics['accuracy']:.4f}\")\n",
    "                print(f\"      F1-Score: {metrics['f1']:.4f}\")\n",
    "                \n",
    "            # Clear memory\n",
    "            del X_train, X_test\n",
    "            gc.collect()\n",
    "            \n",
    "        self.results = detailed_results\n",
    "        \n",
    "    def create_summary_dataframe(self):\n",
    "        \"\"\"Create a summary DataFrame of results\"\"\"\n",
    "        df = pd.DataFrame(self.results).T\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Calculate overall score (weighted average of metrics)\n",
    "        df['overall_score'] = (\n",
    "            df['accuracy'] * 0.25 +\n",
    "            df['precision'] * 0.20 +\n",
    "            df['recall'] * 0.20 +\n",
    "            df['f1'] * 0.25 +\n",
    "            df['cohen_kappa'] * 0.10\n",
    "        )\n",
    "        \n",
    "        # Add efficiency score (inverse of time)\n",
    "        df['efficiency_score'] = 1 / (df['train_time'] + df['pred_time'] + 0.01)\n",
    "        df['efficiency_score'] = df['efficiency_score'] / df['efficiency_score'].max()\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def visualize_results(self, df):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"CREATING VISUALIZATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Set up the figure\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        \n",
    "        # 1. Accuracy comparison by vectorizer type\n",
    "        ax1 = plt.subplot(3, 3, 1)\n",
    "        vec_accuracy = df.groupby('vec_type')['accuracy'].mean()\n",
    "        vec_accuracy.plot(kind='bar', ax=ax1, color=['#3498db', '#e74c3c'])\n",
    "        ax1.set_title('Average Accuracy by Vectorization Type', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Vectorization Type')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\n",
    "        \n",
    "        # 2. F1-Score comparison\n",
    "        ax2 = plt.subplot(3, 3, 2)\n",
    "        vec_f1 = df.groupby('vec_type')['f1'].mean()\n",
    "        vec_f1.plot(kind='bar', ax=ax2, color=['#2ecc71', '#f39c12'])\n",
    "        ax2.set_title('Average F1-Score by Vectorization Type', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Vectorization Type')\n",
    "        ax2.set_ylabel('F1-Score')\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
    "        \n",
    "        # 3. Training time comparison\n",
    "        ax3 = plt.subplot(3, 3, 3)\n",
    "        vec_time = df.groupby('vec_type')['train_time'].mean()\n",
    "        vec_time.plot(kind='bar', ax=ax3, color=['#9b59b6', '#1abc9c'])\n",
    "        ax3.set_title('Average Training Time by Vectorization Type', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Vectorization Type')\n",
    "        ax3.set_ylabel('Time (seconds)')\n",
    "        ax3.set_xticklabels(ax3.get_xticklabels(), rotation=0)\n",
    "        \n",
    "        # 4. Performance by classifier and vectorizer\n",
    "        ax4 = plt.subplot(3, 3, 4)\n",
    "        pivot_acc = df.pivot_table(values='accuracy', index='classifier', columns='vec_type', aggfunc='mean')\n",
    "        pivot_acc.plot(kind='bar', ax=ax4)\n",
    "        ax4.set_title('Accuracy by Classifier and Vectorization Type', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Classifier')\n",
    "        ax4.set_ylabel('Accuracy')\n",
    "        ax4.legend(title='Vec Type')\n",
    "        ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # 5. Heatmap of all metrics\n",
    "        ax5 = plt.subplot(3, 3, 5)\n",
    "        metrics_cols = ['accuracy', 'precision', 'recall', 'f1', 'cohen_kappa']\n",
    "        heatmap_data = df.groupby('vec_type')[metrics_cols].mean()\n",
    "        sns.heatmap(heatmap_data.T, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax5)\n",
    "        ax5.set_title('Performance Metrics Heatmap', fontsize=12, fontweight='bold')\n",
    "        ax5.set_xlabel('Vectorization Type')\n",
    "        ax5.set_ylabel('Metrics')\n",
    "        \n",
    "        # 6. Best configurations\n",
    "        ax6 = plt.subplot(3, 3, 6)\n",
    "        top_configs = df.nlargest(10, 'overall_score')[['vectorizer', 'classifier', 'overall_score']]\n",
    "        y_pos = np.arange(len(top_configs))\n",
    "        bars = ax6.barh(y_pos, top_configs['overall_score'].values, color='steelblue')\n",
    "        ax6.set_yticks(y_pos)\n",
    "        ax6.set_yticklabels([f\"{row['vectorizer'][:15]}\\n{row['classifier']}\" \n",
    "                             for _, row in top_configs.iterrows()], fontsize=8)\n",
    "        ax6.set_xlabel('Overall Score')\n",
    "        ax6.set_title('Top 10 Configurations', fontsize=12, fontweight='bold')\n",
    "        ax6.invert_yaxis()\n",
    "        \n",
    "        # 7. Precision-Recall trade-off\n",
    "        ax7 = plt.subplot(3, 3, 7)\n",
    "        for vec_type in df['vec_type'].unique():\n",
    "            subset = df[df['vec_type'] == vec_type]\n",
    "            ax7.scatter(subset['precision'], subset['recall'], label=vec_type, s=100, alpha=0.6)\n",
    "        ax7.set_xlabel('Precision')\n",
    "        ax7.set_ylabel('Recall')\n",
    "        ax7.set_title('Precision-Recall Trade-off', fontsize=12, fontweight='bold')\n",
    "        ax7.legend()\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 8. Efficiency analysis\n",
    "        ax8 = plt.subplot(3, 3, 8)\n",
    "        df['total_time'] = df['train_time'] + df['pred_time'] + df['vec_time']\n",
    "        efficiency_data = df.groupby('vec_type')[['total_time', 'accuracy']].mean()\n",
    "        ax8_2 = ax8.twinx()\n",
    "        efficiency_data['total_time'].plot(kind='bar', ax=ax8, color='coral', alpha=0.7, position=0, width=0.4)\n",
    "        efficiency_data['accuracy'].plot(kind='bar', ax=ax8_2, color='teal', alpha=0.7, position=1, width=0.4)\n",
    "        ax8.set_xlabel('Vectorization Type')\n",
    "        ax8.set_ylabel('Total Time (s)', color='coral')\n",
    "        ax8_2.set_ylabel('Accuracy', color='teal')\n",
    "        ax8.set_title('Efficiency vs Accuracy Trade-off', fontsize=12, fontweight='bold')\n",
    "        ax8.tick_params(axis='y', labelcolor='coral')\n",
    "        ax8_2.tick_params(axis='y', labelcolor='teal')\n",
    "        ax8.set_xticklabels(ax8.get_xticklabels(), rotation=0)\n",
    "        \n",
    "        # 9. Matthews Correlation Coefficient comparison\n",
    "        ax9 = plt.subplot(3, 3, 9)\n",
    "        mcc_data = df.groupby('vectorizer')['matthews_corr'].mean().sort_values(ascending=False)[:8]\n",
    "        mcc_data.plot(kind='barh', ax=ax9, color='purple')\n",
    "        ax9.set_xlabel('Matthews Correlation Coefficient')\n",
    "        ax9.set_ylabel('Vectorizer')\n",
    "        ax9.set_title('MCC by Vectorizer Configuration', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('Comprehensive Text Vectorization Comparison Results', fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/home/claude/vectorization_comparison.png', dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Visualizations saved to 'vectorization_comparison.png'\")\n",
    "        \n",
    "    def generate_report(self, df):\n",
    "        \"\"\"Generate a detailed analytical report\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"COMPREHENSIVE ANALYSIS REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Overall comparison\n",
    "        print(\"\\n1. OVERALL PERFORMANCE COMPARISON\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        bow_metrics = df[df['vec_type'] == 'BOW'][['accuracy', 'precision', 'recall', 'f1']].mean()\n",
    "        tfidf_metrics = df[df['vec_type'] == 'TF-IDF'][['accuracy', 'precision', 'recall', 'f1']].mean()\n",
    "        \n",
    "        print(\"\\nBag of Words (BOW) Average Performance:\")\n",
    "        for metric, value in bow_metrics.items():\n",
    "            print(f\"  {metric.capitalize():12s}: {value:.4f}\")\n",
    "            \n",
    "        print(\"\\nTF-IDF Average Performance:\")\n",
    "        for metric, value in tfidf_metrics.items():\n",
    "            print(f\"  {metric.capitalize():12s}: {value:.4f}\")\n",
    "            \n",
    "        print(\"\\nPerformance Advantage:\")\n",
    "        for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "            diff = tfidf_metrics[metric] - bow_metrics[metric]\n",
    "            winner = \"TF-IDF\" if diff > 0 else \"BOW\"\n",
    "            print(f\"  {metric.capitalize():12s}: {winner} (+{abs(diff):.4f})\")\n",
    "            \n",
    "        # Best configurations\n",
    "        print(\"\\n2. TOP 5 BEST CONFIGURATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        top5 = df.nlargest(5, 'overall_score')[['vectorizer', 'classifier', 'accuracy', 'f1', 'overall_score']]\n",
    "        for idx, row in top5.iterrows():\n",
    "            print(f\"\\n  Rank {idx+1 if idx < 5 else idx-4}:\")\n",
    "            print(f\"    Vectorizer: {row['vectorizer']}\")\n",
    "            print(f\"    Classifier: {row['classifier']}\")\n",
    "            print(f\"    Accuracy:   {row['accuracy']:.4f}\")\n",
    "            print(f\"    F1-Score:   {row['f1']:.4f}\")\n",
    "            print(f\"    Overall:    {row['overall_score']:.4f}\")\n",
    "            \n",
    "        # Classifier-specific analysis\n",
    "        print(\"\\n3. BEST VECTORIZER FOR EACH CLASSIFIER\")\n",
    "        print(\"-\" * 40)\n",
    "        for classifier in df['classifier'].unique():\n",
    "            clf_data = df[df['classifier'] == classifier]\n",
    "            best = clf_data.loc[clf_data['accuracy'].idxmax()]\n",
    "            print(f\"\\n  {classifier}:\")\n",
    "            print(f\"    Best Vectorizer: {best['vectorizer']}\")\n",
    "            print(f\"    Accuracy: {best['accuracy']:.4f}\")\n",
    "            print(f\"    F1-Score: {best['f1']:.4f}\")\n",
    "            \n",
    "        # Efficiency analysis\n",
    "        print(\"\\n4. EFFICIENCY ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        bow_time = df[df['vec_type'] == 'BOW']['total_time'].mean()\n",
    "        tfidf_time = df[df['vec_type'] == 'TF-IDF']['total_time'].mean()\n",
    "        \n",
    "        print(f\"\\n  BOW Average Total Time:    {bow_time:.2f} seconds\")\n",
    "        print(f\"  TF-IDF Average Total Time: {tfidf_time:.2f} seconds\")\n",
    "        print(f\"  Time Difference: {abs(tfidf_time - bow_time):.2f} seconds\")\n",
    "        print(f\"  Faster Method: {'BOW' if bow_time < tfidf_time else 'TF-IDF'}\")\n",
    "        \n",
    "        # Statistical significance\n",
    "        print(\"\\n5. ROBUSTNESS ANALYSIS (Cohen's Kappa)\")\n",
    "        print(\"-\" * 40)\n",
    "        bow_kappa = df[df['vec_type'] == 'BOW']['cohen_kappa'].mean()\n",
    "        tfidf_kappa = df[df['vec_type'] == 'TF-IDF']['cohen_kappa'].mean()\n",
    "        \n",
    "        print(f\"\\n  BOW Average Cohen's Kappa:    {bow_kappa:.4f}\")\n",
    "        print(f\"  TF-IDF Average Cohen's Kappa: {tfidf_kappa:.4f}\")\n",
    "        print(f\"  More Robust: {'TF-IDF' if tfidf_kappa > bow_kappa else 'BOW'}\")\n",
    "        \n",
    "        # Feature analysis\n",
    "        print(\"\\n6. FEATURE COMPLEXITY ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        for vec_name in df['vectorizer'].unique():\n",
    "            vec_data = df[df['vectorizer'] == vec_name].iloc[0]\n",
    "            print(f\"\\n  {vec_name}:\")\n",
    "            print(f\"    Number of features: {int(vec_data['n_features'])}\")\n",
    "            print(f\"    Avg accuracy: {df[df['vectorizer'] == vec_name]['accuracy'].mean():.4f}\")\n",
    "            \n",
    "        # Recommendations\n",
    "        print(\"\\n7. RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        print(\"\\n  Based on comprehensive analysis:\")\n",
    "        \n",
    "        # Overall winner\n",
    "        tfidf_wins = sum([\n",
    "            tfidf_metrics['accuracy'] > bow_metrics['accuracy'],\n",
    "            tfidf_metrics['f1'] > bow_metrics['f1'],\n",
    "            tfidf_kappa > bow_kappa\n",
    "        ])\n",
    "        \n",
    "        if tfidf_wins >= 2:\n",
    "            print(\"\\n  ✓ PRIMARY RECOMMENDATION: TF-IDF\")\n",
    "            print(\"    - Superior performance across most metrics\")\n",
    "            print(\"    - Better at handling term importance\")\n",
    "            print(\"    - More robust classification results\")\n",
    "        else:\n",
    "            print(\"\\n  ✓ PRIMARY RECOMMENDATION: Bag of Words\")\n",
    "            print(\"    - Comparable performance with simpler model\")\n",
    "            print(\"    - Faster training and inference\")\n",
    "            print(\"    - Easier to interpret\")\n",
    "            \n",
    "        print(\"\\n  ✓ SPECIFIC USE CASES:\")\n",
    "        print(\"    • For Maximum Accuracy: Use TF-IDF with Logistic Regression\")\n",
    "        print(\"    • For Speed: Use BOW with Naive Bayes\")\n",
    "        print(\"    • For Interpretability: Use BOW with simple n-grams\")\n",
    "        print(\"    • For Robustness: Use TF-IDF with SVM\")\n",
    "        \n",
    "        # Best overall configuration\n",
    "        best_config = df.loc[df['overall_score'].idxmax()]\n",
    "        print(f\"\\n  ✓ BEST OVERALL CONFIGURATION:\")\n",
    "        print(f\"    • Vectorizer: {best_config['vectorizer']}\")\n",
    "        print(f\"    • Classifier: {best_config['classifier']}\")\n",
    "        print(f\"    • Expected Accuracy: {best_config['accuracy']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'bow_avg': bow_metrics,\n",
    "            'tfidf_avg': tfidf_metrics,\n",
    "            'best_config': best_config,\n",
    "            'top5': top5\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TEXT VECTORIZATION COMPARISON FOR CLASSIFICATION\")\n",
    "    print(\"Comparing Bag of Words vs TF-IDF\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize comparison framework\n",
    "    comparison = TextVectorizationComparison(random_state=42)\n",
    "    \n",
    "    # Load dataset\n",
    "    comparison.load_dataset()\n",
    "    \n",
    "    # Create vectorizers\n",
    "    comparison.create_vectorizers()\n",
    "    \n",
    "    # Run comprehensive comparison\n",
    "    comparison.run_comparison()\n",
    "    \n",
    "    # Generate results DataFrame\n",
    "    results_df = comparison.create_summary_dataframe()\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    results_df.to_csv('vectorization_results.csv', index=False)\n",
    "    print(\"\\nDetailed results saved to 'vectorization_results.csv'\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    comparison.visualize_results(results_df)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    summary = comparison.generate_report(results_df)\n",
    "    \n",
    "    # Final conclusion\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL CONCLUSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if summary['tfidf_avg']['accuracy'] > summary['bow_avg']['accuracy']:\n",
    "        improvement = (summary['tfidf_avg']['accuracy'] - summary['bow_avg']['accuracy']) * 100\n",
    "        print(f\"\\n✓ TF-IDF demonstrates superior performance with {improvement:.2f}% higher accuracy\")\n",
    "        print(\"  TF-IDF's ability to weight terms by importance provides better discrimination\")\n",
    "        print(\"  for text classification tasks, especially with diverse vocabulary.\")\n",
    "    else:\n",
    "        print(\"\\n✓ Bag of Words shows competitive performance with simpler implementation\")\n",
    "        print(\"  BOW's straightforward approach proves effective for this classification task\")\n",
    "        print(\"  while maintaining computational efficiency.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Analysis Complete!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46738cca-48ec-4295-8fa9-d9df6713b588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
