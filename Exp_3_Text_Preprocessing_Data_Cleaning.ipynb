{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEoLwTqK9uGS30qk213I1t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWpeUpNqbbPJ",
        "outputId": "4d3a4606-4377-4efa-df52-efbb2ff5da42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: This is an example sentence, showing off the text pre-processing pipeline with NLTK!\n",
            "\n",
            "After Tokenization: ['This', 'is', 'an', 'example', 'sentence', ',', 'showing', 'off', 'the', 'text', 'pre-processing', 'pipeline', 'with', 'NLTK', '!']\n",
            "Explanation: Tokenization breaks the text into individual words or tokens. This is the first step in understanding the structure of the text.\n",
            "\n",
            "After Lowercasing: ['this', 'is', 'an', 'example', 'sentence', ',', 'showing', 'off', 'the', 'text', 'pre-processing', 'pipeline', 'with', 'nltk', '!']\n",
            "Explanation: Lowercasing converts all characters to lowercase. This ensures that words like 'The' and 'the' are treated as the same word, reducing the size of the vocabulary.\n",
            "\n",
            "After Removing Punctuation: ['this', 'is', 'an', 'example', 'sentence', 'showing', 'off', 'the', 'text', 'pre-processing', 'pipeline', 'with', 'nltk']\n",
            "Explanation: Removing punctuation removes symbols that do not contribute to the meaning of the words. This helps to focus on the actual content and reduces noise.\n",
            "\n",
            "After Removing Stop Words: ['example', 'sentence', 'showing', 'text', 'pre-processing', 'pipeline', 'nltk']\n",
            "Explanation: Removing stop words eliminates common words that do not carry significant meaning (e.g., 'this', 'is', 'an'). This helps to reduce the dimensionality of the data and focus on more important terms.\n",
            "\n",
            "After Stemming: ['exampl', 'sentenc', 'show', 'text', 'pre-process', 'pipelin', 'nltk']\n",
            "Explanation: Stemming reduces words to their root form (e.g., 'showing' becomes 'show'). This helps to group words with similar meanings together, further reducing the vocabulary size and improving the efficiency of machine learning models.\n",
            "\n",
            "\n",
            "Importance for building vocabulary for machine learning:\n",
            "Each of these pre-processing steps helps to clean and normalize the text data. By reducing variations in words (due to casing, punctuation, or morphology) and removing irrelevant terms, we create a more focused and smaller set of unique words, which forms the vocabulary for machine learning models. A smaller and cleaner vocabulary leads to more efficient and effective model training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# nltk.download('punkt_tab') # Removing this as it seems to not be the correct resource\n",
        "# Re-downloading punkt to ensure English tokenizer is available\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Input text\n",
        "text = \"This is an example sentence, showing off the text pre-processing pipeline with NLTK!\"\n",
        "print(f\"Original text: {text}\\n\")\n",
        "\n",
        "# Step 1: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(f\"After Tokenization: {tokens}\")\n",
        "print(\"Explanation: Tokenization breaks the text into individual words or tokens. This is the first step in understanding the structure of the text.\\n\")\n",
        "\n",
        "# Step 2: Lowercasing\n",
        "tokens = [word.lower() for word in tokens]\n",
        "print(f\"After Lowercasing: {tokens}\")\n",
        "print(\"Explanation: Lowercasing converts all characters to lowercase. This ensures that words like 'The' and 'the' are treated as the same word, reducing the size of the vocabulary.\\n\")\n",
        "\n",
        "# Step 3: Removing Punctuation\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "print(f\"After Removing Punctuation: {tokens}\")\n",
        "print(\"Explanation: Removing punctuation removes symbols that do not contribute to the meaning of the words. This helps to focus on the actual content and reduces noise.\\n\")\n",
        "\n",
        "# Step 4: Removing Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word for word in tokens if word not in stop_words]\n",
        "print(f\"After Removing Stop Words: {tokens}\")\n",
        "print(\"Explanation: Removing stop words eliminates common words that do not carry significant meaning (e.g., 'this', 'is', 'an'). This helps to reduce the dimensionality of the data and focus on more important terms.\\n\")\n",
        "\n",
        "# Step 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [stemmer.stem(word) for word in tokens]\n",
        "print(f\"After Stemming: {tokens}\")\n",
        "print(\"Explanation: Stemming reduces words to their root form (e.g., 'showing' becomes 'show'). This helps to group words with similar meanings together, further reducing the vocabulary size and improving the efficiency of machine learning models.\\n\")\n",
        "\n",
        "print(\"\\nImportance for building vocabulary for machine learning:\")\n",
        "print(\"Each of these pre-processing steps helps to clean and normalize the text data. By reducing variations in words (due to casing, punctuation, or morphology) and removing irrelevant terms, we create a more focused and smaller set of unique words, which forms the vocabulary for machine learning models. A smaller and cleaner vocabulary leads to more efficient and effective model training.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from collections import Counter\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "# Removed the download for 'averaged_perceptron_tagger_eng' and 'punkt_tab'\n",
        "# as they are not needed for the standard English tokenization and POS tagging.\n",
        "\n",
        "\n",
        "def text_preprocessing_pipeline(text):\n",
        "    \"\"\"\n",
        "    Complete text preprocessing pipeline using NLTK\n",
        "    Shows each step and explains the changes made\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"TEXT PREPROCESSING PIPELINE FOR MACHINE LEARNING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Original text\n",
        "    print(f\"\\nüìù ORIGINAL TEXT:\")\n",
        "    print(f\"'{text}'\")\n",
        "    print(f\"Character count: {len(text)}\")\n",
        "    print(f\"Word count (rough): {len(text.split())}\")\n",
        "\n",
        "    # Step 1: Case normalization (Lowercasing)\n",
        "    print(f\"\\nüîΩ STEP 1: LOWERCASING\")\n",
        "    text_lower = text.lower()\n",
        "    print(f\"Output: '{text_lower}'\")\n",
        "    print(f\"Changes made: All uppercase letters converted to lowercase\")\n",
        "    print(f\"üéØ ML Importance: Ensures 'Apple', 'APPLE', and 'apple' are treated as the same word\")\n",
        "    print(f\"   This prevents the model from creating separate features for the same concept.\")\n",
        "\n",
        "    # Step 2: Expanding contractions (basic approach)\n",
        "    print(f\"\\nüìñ STEP 2: EXPANDING CONTRACTIONS\")\n",
        "    contractions = {\n",
        "        \"don't\": \"do not\", \"won't\": \"will not\", \"can't\": \"cannot\",\n",
        "        \"n't\": \" not\", \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
        "        \"'d\": \" would\", \"'m\": \" am\", \"it's\": \"it is\", \"that's\": \"that is\"\n",
        "    }\n",
        "    text_expanded = text_lower\n",
        "    for contraction, expansion in contractions.items():\n",
        "        text_expanded = text_expanded.replace(contraction, expansion)\n",
        "\n",
        "    print(f\"Output: '{text_expanded}'\")\n",
        "    print(f\"Changes made: Contractions expanded to full forms\")\n",
        "    print(f\"üéØ ML Importance: Standardizes language - 'don't' and 'do not' become the same feature\")\n",
        "\n",
        "    # Step 3: Remove special characters and digits (keeping spaces)\n",
        "    print(f\"\\nüßπ STEP 3: REMOVING SPECIAL CHARACTERS & DIGITS\")\n",
        "    text_clean = re.sub(r'[^a-zA-Z\\s]', '', text_expanded)\n",
        "    print(f\"Output: '{text_clean}'\")\n",
        "    print(f\"Changes made: Removed punctuation, numbers, and special characters\")\n",
        "    print(f\"üéØ ML Importance: Reduces noise and focuses on meaningful words\")\n",
        "    print(f\"   Prevents creating separate features for 'word' vs 'word!' vs 'word?'\")\n",
        "\n",
        "    # Step 4: Remove extra whitespaces\n",
        "    print(f\"\\n‚ö™ STEP 4: WHITESPACE NORMALIZATION\")\n",
        "    text_normalized = re.sub(r'\\s+', ' ', text_clean).strip()\n",
        "    print(f\"Output: '{text_normalized}'\")\n",
        "    print(f\"Changes made: Multiple spaces converted to single spaces, leading/trailing spaces removed\")\n",
        "    print(f\"üéØ ML Importance: Prevents tokenization errors and ensures consistent formatting\")\n",
        "\n",
        "    # Step 5: Tokenization\n",
        "    print(f\"\\nüî™ STEP 5: TOKENIZATION\")\n",
        "    tokens = word_tokenize(text_normalized)\n",
        "    print(f\"Output: {tokens}\")\n",
        "    print(f\"Changes made: Text split into individual words (tokens)\")\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    print(f\"üéØ ML Importance: Converts text into discrete units that algorithms can process\")\n",
        "    print(f\"   Each token becomes a potential feature in your model\")\n",
        "\n",
        "    # Step 6: Remove stopwords\n",
        "    print(f\"\\nüö´ STEP 6: STOPWORD REMOVAL\")\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stop = [token for token in tokens if token not in stop_words]\n",
        "    print(f\"Stopwords found and removed: {[token for token in tokens if token in stop_words]}\")\n",
        "    print(f\"Output: {tokens_no_stop}\")\n",
        "    print(f\"üéØ ML Importance: Removes high-frequency, low-information words\")\n",
        "    print(f\"   Reduces vocabulary size and focuses on meaningful content words\")\n",
        "    print(f\"   Token count reduced from {len(tokens)} to {len(tokens_no_stop)}\")\n",
        "\n",
        "    # Step 7: Part-of-Speech Tagging (for demonstration)\n",
        "    print(f\"\\nüè∑Ô∏è  STEP 7: PART-OF-SPEECH TAGGING\")\n",
        "    pos_tags = pos_tag(tokens_no_stop)\n",
        "    print(f\"Output: {pos_tags}\")\n",
        "    print(f\"Changes made: Each word tagged with its grammatical role\")\n",
        "    print(f\"üéØ ML Importance: Helps distinguish word meanings (e.g., 'run' as noun vs verb)\")\n",
        "    print(f\"   Can be used for advanced preprocessing like keeping only nouns/verbs\")\n",
        "\n",
        "    # Step 8: Stemming\n",
        "    print(f\"\\nüå± STEP 8: STEMMING\")\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens_no_stop]\n",
        "    print(f\"Output: {stemmed_tokens}\")\n",
        "    print(f\"Changes made: Words reduced to their root forms\")\n",
        "\n",
        "    # Show stemming examples\n",
        "    stemming_examples = {}\n",
        "    for original, stemmed in zip(tokens_no_stop, stemmed_tokens):\n",
        "        if original != stemmed:\n",
        "            stemming_examples[original] = stemmed\n",
        "\n",
        "    if stemming_examples:\n",
        "        print(f\"Stemming examples: {stemming_examples}\")\n",
        "\n",
        "    print(f\"üéØ ML Importance: Groups related words together (running, runs, ran ‚Üí run)\")\n",
        "    print(f\"   Reduces vocabulary size and improves generalization\")\n",
        "\n",
        "    # Step 9: Lemmatization\n",
        "    print(f\"\\nüéØ STEP 9: LEMMATIZATION\")\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_no_stop]\n",
        "    print(f\"Output: {lemmatized_tokens}\")\n",
        "    print(f\"Changes made: Words converted to their dictionary base forms\")\n",
        "\n",
        "    # Show lemmatization examples\n",
        "    lemma_examples = {}\n",
        "    for original, lemmatized in zip(tokens_no_stop, lemmatized_tokens):\n",
        "        if original != lemmatized:\n",
        "            lemma_examples[original] = lemmatized\n",
        "\n",
        "    if lemma_examples:\n",
        "        print(f\"Lemmatization examples: {lemma_examples}\")\n",
        "\n",
        "    print(f\"üéØ ML Importance: More accurate than stemming - preserves word meaning\")\n",
        "    print(f\"   'better' becomes 'good', not 'bett' (as stemming would do)\")\n",
        "\n",
        "    # Final vocabulary analysis\n",
        "    print(f\"\\nüìä FINAL VOCABULARY ANALYSIS\")\n",
        "    print(f\"Original text length: {len(text)} characters\")\n",
        "    print(f\"Final processed tokens: {lemmatized_tokens}\")\n",
        "    print(f\"Unique tokens in final vocabulary: {len(set(lemmatized_tokens))}\")\n",
        "    print(f\"Total tokens: {len(lemmatized_tokens)}\")\n",
        "\n",
        "    # Show vocabulary reduction\n",
        "    original_vocab = set(text.lower().split())\n",
        "    final_vocab = set(lemmatized_tokens)\n",
        "    print(f\"\\nVocabulary size reduction:\")\n",
        "    print(f\"  Before preprocessing: {len(original_vocab)} unique words\")\n",
        "    print(f\"  After preprocessing: {len(final_vocab)} unique words\")\n",
        "    print(f\"  Reduction: {len(original_vocab) - len(final_vocab)} words removed\")\n",
        "\n",
        "    print(f\"\\nüöÄ READY FOR MACHINE LEARNING!\")\n",
        "    print(f\"Clean tokens: {lemmatized_tokens}\")\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Example usage with a realistic text sample\n",
        "sample_text = \"I'm absolutely LOVING the new iPhone's camera!!! It's amazing and I can't believe how great the photos are. The developers have really improved the technology significantly.\"\n",
        "\n",
        "# Run the preprocessing pipeline\n",
        "final_tokens = text_preprocessing_pipeline(sample_text)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"WHY TEXT PREPROCESSING MATTERS FOR MACHINE LEARNING\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "üéØ VOCABULARY CONSISTENCY:\n",
        "   - Without preprocessing: ['iPhone', 'iphone', 'IPHONE'] = 3 different features\n",
        "   - With preprocessing: ['iphone'] = 1 consistent feature\n",
        "\n",
        "üéØ NOISE REDUCTION:\n",
        "   - Removes irrelevant punctuation, numbers, and special characters\n",
        "   - Focuses model attention on meaningful content\n",
        "\n",
        "üéØ DIMENSIONALITY REDUCTION:\n",
        "   - Stemming/Lemmatization: ['running', 'runs', 'ran'] ‚Üí ['run']\n",
        "   - Stopword removal: Eliminates 'the', 'is', 'and', etc.\n",
        "   - Reduces feature space size ‚Üí faster training, less overfitting\n",
        "\n",
        "üéØ IMPROVED GENERALIZATION:\n",
        "   - Normalized text helps model recognize patterns across different writing styles\n",
        "   - Model learns from clean, consistent data representation\n",
        "\n",
        "üéØ COMPUTATIONAL EFFICIENCY:\n",
        "   - Smaller vocabulary = fewer dimensions = faster processing\n",
        "   - Less memory usage during training and inference\n",
        "\n",
        "The final clean tokens become your MODEL'S VOCABULARY - the foundation for all\n",
        "machine learning tasks like classification, sentiment analysis, or text generation!\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "L4RHIZyucI3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Union, Optional\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from dataclasses import dataclass, asdict\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class PreprocessingMetrics:\n",
        "    \"\"\"Container for preprocessing performance metrics\"\"\"\n",
        "    # Timing metrics\n",
        "    total_processing_time: float\n",
        "    avg_processing_time_per_doc: float\n",
        "    preprocessing_speed_tokens_per_sec: float\n",
        "\n",
        "    # Volume metrics\n",
        "    total_documents: int\n",
        "    total_tokens_before: int\n",
        "    total_tokens_after: int\n",
        "    token_reduction_rate: float\n",
        "\n",
        "    # Vocabulary metrics\n",
        "    unique_tokens_before: int\n",
        "    unique_tokens_after: int\n",
        "    vocabulary_reduction_rate: float\n",
        "\n",
        "    # Quality metrics\n",
        "    avg_tokens_per_document: float\n",
        "    empty_documents_after_processing: int\n",
        "    most_common_tokens: List[tuple]\n",
        "\n",
        "    # Linguistic metrics\n",
        "    named_entities_found: int\n",
        "    pos_tag_distribution: Dict[str, int]\n",
        "    sentence_count: int\n",
        "    avg_sentence_length: float\n",
        "\n",
        "    # Memory metrics\n",
        "    memory_usage_mb: float\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return asdict(self)\n",
        "\n",
        "class ProductionTextPreprocessor:\n",
        "    \"\"\"\n",
        "    Production-ready text preprocessing pipeline using spaCy\n",
        "    Used by industry models like Claude, ChatGPT, and Grok\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name: str = \"en_core_web_sm\",\n",
        "                 custom_stopwords: Optional[List[str]] = None,\n",
        "                 keep_pos: Optional[List[str]] = None,\n",
        "                 min_token_length: int = 2,\n",
        "                 max_token_length: int = 20,\n",
        "                 remove_punct: bool = True,\n",
        "                 remove_spaces: bool = True,\n",
        "                 remove_numbers: bool = False,\n",
        "                 lemmatize: bool = True,\n",
        "                 lowercase: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the preprocessing pipeline\n",
        "\n",
        "        Args:\n",
        "            model_name: spaCy model to use\n",
        "            custom_stopwords: Additional stopwords to remove\n",
        "            keep_pos: POS tags to keep (e.g., ['NOUN', 'VERB', 'ADJ'])\n",
        "            min_token_length: Minimum token length to keep\n",
        "            max_token_length: Maximum token length to keep\n",
        "            remove_punct: Remove punctuation\n",
        "            remove_spaces: Remove spaces\n",
        "            remove_numbers: Remove numeric tokens\n",
        "            lemmatize: Apply lemmatization\n",
        "            lowercase: Convert to lowercase\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.nlp = spacy.load(model_name)\n",
        "        except OSError:\n",
        "            logger.error(f\"Model {model_name} not found. Please install with: python -m spacy download {model_name}\")\n",
        "            raise\n",
        "\n",
        "        # Configure pipeline settings\n",
        "        self.custom_stopwords = set(custom_stopwords) if custom_stopwords else set()\n",
        "        self.keep_pos = set(keep_pos) if keep_pos else None\n",
        "        self.min_token_length = min_token_length\n",
        "        self.max_token_length = max_token_length\n",
        "        self.remove_punct = remove_punct\n",
        "        self.remove_spaces = remove_spaces\n",
        "        self.remove_numbers = remove_numbers\n",
        "        self.lemmatize = lemmatize\n",
        "        self.lowercase = lowercase\n",
        "\n",
        "        # Performance tracking\n",
        "        self.processing_stats = defaultdict(list)\n",
        "\n",
        "        logger.info(f\"Initialized TextPreprocessor with model: {model_name}\")\n",
        "\n",
        "    def preprocess_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Preprocess a single text document\n",
        "\n",
        "        Args:\n",
        "            text: Input text string\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing processed text and metadata\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return {\n",
        "                'original_text': text,\n",
        "                'processed_text': '',\n",
        "                'tokens': [],\n",
        "                'metadata': {\n",
        "                    'original_length': len(text) if text else 0,\n",
        "                    'processed_length': 0,\n",
        "                    'token_count': 0,\n",
        "                    'processing_time': 0,\n",
        "                    'entities': [],\n",
        "                    'pos_tags': {},\n",
        "                    'sentences': 0\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # Process with spaCy\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract tokens based on configuration\n",
        "        processed_tokens = []\n",
        "        pos_counts = Counter()\n",
        "        entities = []\n",
        "\n",
        "        for token in doc:\n",
        "            # Skip tokens based on configuration\n",
        "            if self.remove_punct and token.is_punct:\n",
        "                continue\n",
        "            if self.remove_spaces and token.is_space:\n",
        "                continue\n",
        "            if self.remove_numbers and token.like_num:\n",
        "                continue\n",
        "            if token.is_stop or token.text.lower() in self.custom_stopwords:\n",
        "                continue\n",
        "            if len(token.text) < self.min_token_length or len(token.text) > self.max_token_length:\n",
        "                continue\n",
        "            if self.keep_pos and token.pos_ not in self.keep_pos:\n",
        "                continue\n",
        "\n",
        "            # Process token\n",
        "            if self.lemmatize:\n",
        "                processed_token = token.lemma_\n",
        "            else:\n",
        "                processed_token = token.text\n",
        "\n",
        "            if self.lowercase:\n",
        "                processed_token = processed_token.lower()\n",
        "\n",
        "            # Additional cleaning\n",
        "            processed_token = re.sub(r'[^\\w\\s]', '', processed_token)\n",
        "\n",
        "            if processed_token.strip():\n",
        "                processed_tokens.append(processed_token)\n",
        "                pos_counts[token.pos_] += 1\n",
        "\n",
        "        # Extract named entities\n",
        "        for ent in doc.ents:\n",
        "            entities.append({\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'start': ent.start_char,\n",
        "                'end': ent.end_char\n",
        "            })\n",
        "\n",
        "        # Create processed text\n",
        "        processed_text = ' '.join(processed_tokens)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            'original_text': text,\n",
        "            'processed_text': processed_text,\n",
        "            'tokens': processed_tokens,\n",
        "            'metadata': {\n",
        "                'original_length': len(text),\n",
        "                'processed_length': len(processed_text),\n",
        "                'token_count': len(processed_tokens),\n",
        "                'processing_time': processing_time,\n",
        "                'entities': entities,\n",
        "                'pos_tags': dict(pos_counts),\n",
        "                'sentences': len(list(doc.sents))\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def process_file(self,\n",
        "                    input_path: Union[str, Path],\n",
        "                    text_column: str = 'text',\n",
        "                    id_column: Optional[str] = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Process text from various file formats\n",
        "\n",
        "        Args:\n",
        "            input_path: Path to input file (.txt, .csv, .json, .jsonl)\n",
        "            text_column: Column name containing text (for structured files)\n",
        "            id_column: Column name for document IDs\n",
        "\n",
        "        Returns:\n",
        "            List of processed documents\n",
        "        \"\"\"\n",
        "        input_path = Path(input_path)\n",
        "\n",
        "        if not input_path.exists():\n",
        "            raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
        "\n",
        "        logger.info(f\"Processing file: {input_path}\")\n",
        "\n",
        "        # Read file based on extension\n",
        "        if input_path.suffix.lower() == '.txt':\n",
        "            with open(input_path, 'r', encoding='utf-8') as f:\n",
        "                texts = [{'text': f.read(), 'id': input_path.stem}]\n",
        "\n",
        "        elif input_path.suffix.lower() == '.csv':\n",
        "            df = pd.read_csv(input_path)\n",
        "            texts = []\n",
        "            for idx, row in df.iterrows():\n",
        "                doc_id = row[id_column] if id_column and id_column in df.columns else f\"doc_{idx}\"\n",
        "                texts.append({'text': str(row[text_column]), 'id': doc_id})\n",
        "\n",
        "        elif input_path.suffix.lower() == '.json':\n",
        "            with open(input_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            if isinstance(data, list):\n",
        "                texts = []\n",
        "                for idx, item in enumerate(data):\n",
        "                    if isinstance(item, dict):\n",
        "                        doc_id = item.get(id_column, f\"doc_{idx}\") if id_column else f\"doc_{idx}\"\n",
        "                        texts.append({'text': str(item.get(text_column, '')), 'id': doc_id})\n",
        "                    else:\n",
        "                        texts.append({'text': str(item), 'id': f\"doc_{idx}\"})\n",
        "            else:\n",
        "                texts = [{'text': str(data), 'id': 'single_doc'}]\n",
        "\n",
        "        elif input_path.suffix.lower() == '.jsonl':\n",
        "            texts = []\n",
        "            with open(input_path, 'r', encoding='utf-8') as f:\n",
        "                for idx, line in enumerate(f):\n",
        "                    item = json.loads(line.strip())\n",
        "                    doc_id = item.get(id_column, f\"doc_{idx}\") if id_column else f\"doc_{idx}\"\n",
        "                    texts.append({'text': str(item.get(text_column, '')), 'id': doc_id})\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {input_path.suffix}\")\n",
        "\n",
        "        # Process all texts\n",
        "        processed_docs = []\n",
        "        for doc in texts:\n",
        "            result = self.preprocess_text(doc['text'])\n",
        "            result['document_id'] = doc['id']\n",
        "            processed_docs.append(result)\n",
        "\n",
        "        return processed_docs\n",
        "\n",
        "    def save_results(self,\n",
        "                    processed_docs: List[Dict[str, Any]],\n",
        "                    output_path: Union[str, Path],\n",
        "                    format_type: str = 'json',\n",
        "                    include_metadata: bool = True) -> None:\n",
        "        \"\"\"\n",
        "        Save processed results to file\n",
        "\n",
        "        Args:\n",
        "            processed_docs: List of processed documents\n",
        "            output_path: Output file path\n",
        "            format_type: Output format ('json', 'csv', 'jsonl')\n",
        "            include_metadata: Whether to include processing metadata\n",
        "        \"\"\"\n",
        "        output_path = Path(output_path)\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if format_type.lower() == 'json':\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(processed_docs, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        elif format_type.lower() == 'csv':\n",
        "            # Flatten for CSV format\n",
        "            csv_data = []\n",
        "            for doc in processed_docs:\n",
        "                row = {\n",
        "                    'document_id': doc.get('document_id', ''),\n",
        "                    'original_text': doc['original_text'],\n",
        "                    'processed_text': doc['processed_text'],\n",
        "                    'tokens': '|'.join(doc['tokens']),  # Join tokens with delimiter\n",
        "                }\n",
        "\n",
        "                if include_metadata:\n",
        "                    row.update({\n",
        "                        'original_length': doc['metadata']['original_length'],\n",
        "                        'processed_length': doc['metadata']['processed_length'],\n",
        "                        'token_count': doc['metadata']['token_count'],\n",
        "                        'processing_time': doc['metadata']['processing_time'],\n",
        "                        'entity_count': len(doc['metadata']['entities']),\n",
        "                        'sentence_count': doc['metadata']['sentences']\n",
        "                    })\n",
        "\n",
        "                csv_data.append(row)\n",
        "\n",
        "            pd.DataFrame(csv_data).to_csv(output_path, index=False)\n",
        "\n",
        "        elif format_type.lower() == 'jsonl':\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                for doc in processed_docs:\n",
        "                    f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported output format: {format_type}\")\n",
        "\n",
        "        logger.info(f\"Results saved to: {output_path}\")\n",
        "\n",
        "    def compute_performance_metrics(self, processed_docs: List[Dict[str, Any]]) -> PreprocessingMetrics:\n",
        "        \"\"\"\n",
        "        Compute comprehensive performance metrics\n",
        "\n",
        "        Args:\n",
        "            processed_docs: List of processed documents\n",
        "\n",
        "        Returns:\n",
        "            PreprocessingMetrics object with all performance metrics\n",
        "        \"\"\"\n",
        "        if not processed_docs:\n",
        "            raise ValueError(\"No processed documents provided\")\n",
        "\n",
        "        # Extract metrics from processed documents\n",
        "        total_docs = len(processed_docs)\n",
        "        processing_times = [doc['metadata']['processing_time'] for doc in processed_docs]\n",
        "        original_lengths = [doc['metadata']['original_length'] for doc in processed_docs]\n",
        "        processed_lengths = [doc['metadata']['processed_length'] for doc in processed_docs]\n",
        "        token_counts_before = [len(doc['original_text'].split()) for doc in processed_docs]\n",
        "        token_counts_after = [doc['metadata']['token_count'] for doc in processed_docs]\n",
        "\n",
        "        # Collect all tokens for vocabulary analysis\n",
        "        all_original_tokens = []\n",
        "        all_processed_tokens = []\n",
        "        all_entities = []\n",
        "        all_pos_tags = Counter()\n",
        "        sentence_counts = []\n",
        "\n",
        "        for doc in processed_docs:\n",
        "            all_original_tokens.extend(doc['original_text'].split())\n",
        "            all_processed_tokens.extend(doc['tokens'])\n",
        "            all_entities.extend(doc['metadata']['entities'])\n",
        "            all_pos_tags.update(doc['metadata']['pos_tags'])\n",
        "            sentence_counts.append(doc['metadata']['sentences'])\n",
        "\n",
        "        # Calculate metrics\n",
        "        total_processing_time = sum(processing_times)\n",
        "        avg_processing_time = total_processing_time / total_docs\n",
        "        total_tokens_before = sum(token_counts_before)\n",
        "        total_tokens_after = sum(token_counts_after)\n",
        "\n",
        "        # Performance metrics\n",
        "        preprocessing_speed = total_tokens_after / total_processing_time if total_processing_time > 0 else 0\n",
        "        token_reduction_rate = (total_tokens_before - total_tokens_after) / total_tokens_before if total_tokens_before > 0 else 0\n",
        "\n",
        "        # Vocabulary metrics\n",
        "        unique_tokens_before = len(set(all_original_tokens))\n",
        "        unique_tokens_after = len(set(all_processed_tokens))\n",
        "        vocab_reduction_rate = (unique_tokens_before - unique_tokens_after) / unique_tokens_before if unique_tokens_before > 0 else 0\n",
        "\n",
        "        # Quality metrics\n",
        "        avg_tokens_per_doc = total_tokens_after / total_docs\n",
        "        empty_docs = sum(1 for doc in processed_docs if not doc['processed_text'].strip())\n",
        "        most_common = Counter(all_processed_tokens).most_common(10)\n",
        "\n",
        "        # Linguistic metrics\n",
        "        total_sentences = sum(sentence_counts)\n",
        "        avg_sentence_length = total_tokens_after / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "        # Memory estimation (rough)\n",
        "        memory_usage = sum(len(doc['processed_text']) for doc in processed_docs) / (1024 * 1024)  # MB\n",
        "\n",
        "        return PreprocessingMetrics(\n",
        "            total_processing_time=total_processing_time,\n",
        "            avg_processing_time_per_doc=avg_processing_time,\n",
        "            preprocessing_speed_tokens_per_sec=preprocessing_speed,\n",
        "            total_documents=total_docs,\n",
        "            total_tokens_before=total_tokens_before,\n",
        "            total_tokens_after=total_tokens_after,\n",
        "            token_reduction_rate=token_reduction_rate,\n",
        "            unique_tokens_before=unique_tokens_before,\n",
        "            unique_tokens_after=unique_tokens_after,\n",
        "            vocabulary_reduction_rate=vocab_reduction_rate,\n",
        "            avg_tokens_per_document=avg_tokens_per_doc,\n",
        "            empty_documents_after_processing=empty_docs,\n",
        "            most_common_tokens=most_common,\n",
        "            named_entities_found=len(all_entities),\n",
        "            pos_tag_distribution=dict(all_pos_tags),\n",
        "            sentence_count=total_sentences,\n",
        "            avg_sentence_length=avg_sentence_length,\n",
        "            memory_usage_mb=memory_usage\n",
        "        )\n",
        "\n",
        "    def generate_performance_report(self, metrics: PreprocessingMetrics, output_path: Optional[str] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a comprehensive performance report\n",
        "\n",
        "        Args:\n",
        "            metrics: PreprocessingMetrics object\n",
        "            output_path: Optional path to save the report\n",
        "\n",
        "        Returns:\n",
        "            Report as string\n",
        "        \"\"\"\n",
        "        report = f\"\"\"\n",
        "{'='*80}\n",
        "TEXT PREPROCESSING PERFORMANCE REPORT\n",
        "{'='*80}\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "üìä PROCESSING PERFORMANCE\n",
        "‚îú‚îÄ Total Documents Processed: {metrics.total_documents:,}\n",
        "‚îú‚îÄ Total Processing Time: {metrics.total_processing_time:.2f} seconds\n",
        "‚îú‚îÄ Average Time per Document: {metrics.avg_processing_time_per_doc:.4f} seconds\n",
        "‚îî‚îÄ Processing Speed: {metrics.preprocessing_speed_tokens_per_sec:.0f} tokens/second\n",
        "\n",
        "üéØ TOKEN ANALYSIS\n",
        "‚îú‚îÄ Total Tokens Before: {metrics.total_tokens_before:,}\n",
        "‚îú‚îÄ Total Tokens After: {metrics.total_tokens_after:,}\n",
        "‚îú‚îÄ Token Reduction Rate: {metrics.token_reduction_rate:.1%}\n",
        "‚îî‚îÄ Average Tokens per Document: {metrics.avg_tokens_per_document:.1f}\n",
        "\n",
        "üìö VOCABULARY ANALYSIS\n",
        "‚îú‚îÄ Unique Tokens Before: {metrics.unique_tokens_before:,}\n",
        "‚îú‚îÄ Unique Tokens After: {metrics.unique_tokens_after:,}\n",
        "‚îú‚îÄ Vocabulary Reduction Rate: {metrics.vocabulary_reduction_rate:.1%}\n",
        "‚îî‚îÄ Empty Documents After Processing: {metrics.empty_documents_after_processing}\n",
        "\n",
        "üè∑Ô∏è  LINGUISTIC FEATURES\n",
        "‚îú‚îÄ Named Entities Found: {metrics.named_entities_found:,}\n",
        "‚îú‚îÄ Total Sentences: {metrics.sentence_count:,}\n",
        "‚îú‚îÄ Average Sentence Length: {metrics.avg_sentence_length:.1f} tokens\n",
        "‚îî‚îÄ POS Tag Distribution:\n",
        "\"\"\"\n",
        "\n",
        "        # Add POS tag distribution\n",
        "        for pos_tag, count in sorted(metrics.pos_tag_distribution.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            report += f\"   ‚îú‚îÄ {pos_tag}: {count:,} ({count/sum(metrics.pos_tag_distribution.values()):.1%})\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "üî§ MOST COMMON TOKENS\n",
        "\"\"\"\n",
        "        for token, count in metrics.most_common_tokens[:10]:\n",
        "            report += f\"‚îú‚îÄ '{token}': {count:,} occurrences\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "üíæ RESOURCE USAGE\n",
        "‚îú‚îÄ Estimated Memory Usage: {metrics.memory_usage_mb:.2f} MB\n",
        "‚îî‚îÄ Processing Efficiency: {metrics.total_tokens_after / (metrics.total_processing_time or 1):.0f} tokens/sec\n",
        "\n",
        "üéØ QUALITY INDICATORS\n",
        "‚îú‚îÄ Token Retention Rate: {(1 - metrics.token_reduction_rate):.1%}\n",
        "‚îú‚îÄ Vocabulary Retention Rate: {(1 - metrics.vocabulary_reduction_rate):.1%}\n",
        "‚îú‚îÄ Document Success Rate: {((metrics.total_documents - metrics.empty_documents_after_processing) / metrics.total_documents):.1%}\n",
        "‚îî‚îÄ Average Document Reduction: {(metrics.token_reduction_rate):.1%}\n",
        "\n",
        "üöÄ READINESS FOR MACHINE LEARNING\n",
        "‚úì Standardized vocabulary\n",
        "‚úì Noise removed\n",
        "‚úì Consistent tokenization\n",
        "‚úì Linguistic features extracted\n",
        "‚úì Structured output format\n",
        "\n",
        "{'='*80}\n",
        "\"\"\"\n",
        "\n",
        "        if output_path:\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(report)\n",
        "            logger.info(f\"Performance report saved to: {output_path}\")\n",
        "\n",
        "        return report\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage of the production preprocessing pipeline\"\"\"\n",
        "\n",
        "    # Initialize preprocessor with production settings\n",
        "    preprocessor = ProductionTextPreprocessor(\n",
        "        model_name=\"en_core_web_sm\",  # Download with: python -m spacy download en_core_web_sm\n",
        "        keep_pos=['NOUN', 'VERB', 'ADJ', 'ADV'],  # Keep content words\n",
        "        min_token_length=2,\n",
        "        lemmatize=True,\n",
        "        lowercase=True,\n",
        "        remove_numbers=False\n",
        "    )\n",
        "\n",
        "    # Example: Create sample data\n",
        "    sample_data = [\n",
        "        \"I'm absolutely LOVING the new iPhone's camera!!! It's amazing and I can't believe how great the photos are.\",\n",
        "        \"The developers have really improved the technology significantly. Machine learning models like GPT-4 and Claude are revolutionizing NLP.\",\n",
        "        \"Data preprocessing is crucial for building robust machine learning pipelines in production environments.\",\n",
        "        \"Natural Language Processing involves tokenization, lemmatization, and feature extraction for downstream tasks.\"\n",
        "    ]\n",
        "\n",
        "    # Save sample data to file\n",
        "    sample_df = pd.DataFrame(sample_data, columns=['text'])\n",
        "    sample_df['id'] = [f'doc_{i}' for i in range(len(sample_data))]\n",
        "    sample_df.to_csv('sample_input.csv', index=False)\n",
        "\n",
        "    # Process file\n",
        "    processed_docs = preprocessor.process_file('sample_input.csv', text_column='text', id_column='id')\n",
        "\n",
        "    # Save results in multiple formats\n",
        "    preprocessor.save_results(processed_docs, 'processed_output.json', format_type='json')\n",
        "    preprocessor.save_results(processed_docs, 'processed_output.csv', format_type='csv')\n",
        "\n",
        "    # Compute performance metrics\n",
        "    metrics = preprocessor.compute_performance_metrics(processed_docs)\n",
        "\n",
        "    # Generate and display performance report\n",
        "    report = preprocessor.generate_performance_report(metrics, 'performance_report.txt')\n",
        "    print(report)\n",
        "\n",
        "    # Save metrics as JSON for programmatic access\n",
        "    with open('preprocessing_metrics.json', 'w') as f:\n",
        "        json.dump(metrics.to_dict(), f, indent=2)\n",
        "\n",
        "    print(\"‚úÖ Processing complete! Check the output files:\")\n",
        "    print(\"   - processed_output.json (structured results)\")\n",
        "    print(\"   - processed_output.csv (tabular format)\")\n",
        "    print(\"   - performance_report.txt (detailed metrics)\")\n",
        "    print(\"   - preprocessing_metrics.json (metrics data)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8GbPpdp81bv",
        "outputId": "9f2f0fe5-216d-473b-e904-896373a782a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TEXT PREPROCESSING PERFORMANCE REPORT\n",
            "================================================================================\n",
            "Generated: 2025-08-03 12:41:39\n",
            "\n",
            "üìä PROCESSING PERFORMANCE\n",
            "‚îú‚îÄ Total Documents Processed: 4\n",
            "‚îú‚îÄ Total Processing Time: 0.05 seconds\n",
            "‚îú‚îÄ Average Time per Document: 0.0136 seconds\n",
            "‚îî‚îÄ Processing Speed: 624 tokens/second\n",
            "\n",
            "üéØ TOKEN ANALYSIS\n",
            "‚îú‚îÄ Total Tokens Before: 61\n",
            "‚îú‚îÄ Total Tokens After: 34\n",
            "‚îú‚îÄ Token Reduction Rate: 44.3%\n",
            "‚îî‚îÄ Average Tokens per Document: 8.5\n",
            "\n",
            "üìö VOCABULARY ANALYSIS\n",
            "‚îú‚îÄ Unique Tokens Before: 55\n",
            "‚îú‚îÄ Unique Tokens After: 33\n",
            "‚îú‚îÄ Vocabulary Reduction Rate: 40.0%\n",
            "‚îî‚îÄ Empty Documents After Processing: 0\n",
            "\n",
            "üè∑Ô∏è  LINGUISTIC FEATURES\n",
            "‚îú‚îÄ Named Entities Found: 4\n",
            "‚îú‚îÄ Total Sentences: 7\n",
            "‚îú‚îÄ Average Sentence Length: 4.9 tokens\n",
            "‚îî‚îÄ POS Tag Distribution:\n",
            "   ‚îú‚îÄ NOUN: 17 (50.0%)\n",
            "   ‚îú‚îÄ VERB: 9 (26.5%)\n",
            "   ‚îú‚îÄ ADJ: 6 (17.6%)\n",
            "   ‚îú‚îÄ ADV: 2 (5.9%)\n",
            "\n",
            "üî§ MOST COMMON TOKENS\n",
            "‚îú‚îÄ 'machine': 2 occurrences\n",
            "‚îú‚îÄ 'absolutely': 1 occurrences\n",
            "‚îú‚îÄ 'love': 1 occurrences\n",
            "‚îú‚îÄ 'new': 1 occurrences\n",
            "‚îú‚îÄ 'camera': 1 occurrences\n",
            "‚îú‚îÄ 'amazing': 1 occurrences\n",
            "‚îú‚îÄ 'believe': 1 occurrences\n",
            "‚îú‚îÄ 'great': 1 occurrences\n",
            "‚îú‚îÄ 'photo': 1 occurrences\n",
            "‚îú‚îÄ 'developer': 1 occurrences\n",
            "\n",
            "üíæ RESOURCE USAGE\n",
            "‚îú‚îÄ Estimated Memory Usage: 0.00 MB\n",
            "‚îî‚îÄ Processing Efficiency: 624 tokens/sec\n",
            "\n",
            "üéØ QUALITY INDICATORS\n",
            "‚îú‚îÄ Token Retention Rate: 55.7%\n",
            "‚îú‚îÄ Vocabulary Retention Rate: 60.0%\n",
            "‚îú‚îÄ Document Success Rate: 100.0%\n",
            "‚îî‚îÄ Average Document Reduction: 44.3%\n",
            "\n",
            "üöÄ READINESS FOR MACHINE LEARNING\n",
            "‚úì Standardized vocabulary\n",
            "‚úì Noise removed\n",
            "‚úì Consistent tokenization\n",
            "‚úì Linguistic features extracted\n",
            "‚úì Structured output format\n",
            "\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Processing complete! Check the output files:\n",
            "   - processed_output.json (structured results)\n",
            "   - processed_output.csv (tabular format)\n",
            "   - performance_report.txt (detailed metrics)\n",
            "   - preprocessing_metrics.json (metrics data)\n"
          ]
        }
      ]
    }
  ]
}